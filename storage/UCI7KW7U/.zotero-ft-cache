๭ਲഠ౵׈֥ ሰ඀ીႵҐႨህႵग़޼
؊đଢ଼ॖၛᄝ಩ၩഡСഈđႨሱ࠭༟
֥ߋᛍফఖ 1 ބ %'ᄇఖࣉྛᄇb
֌ଢ଼ܓઙ׈֥ ሰ඀ࣇ܂ଢ଼۱ದ൐Ⴈđ
ໃࣜ൱ಃđ҂֤ࣉྛԮѬb
໡ૌ჻ၩཌྷྐᆀऎႵᆃဢ֥ਅᆩބ
त༁đა໡ૌ܋๝Ќ޹ᆩ്Ӂಃb
ೂ ܓݔ ઙᆀႵ౓ಃྛູđ໡ૌॖିؓ
ݼ ᅫ ھ оܱ ൌീЇও֌҂ཋႿ ޼ Ⴈ ھ
֩ົಃծീđѩॖିሔ࣮مੰᄳ಩b
හਁᇇ๦्ก


图灵程序设计丛书
人民邮电出版社
北京
基于 Python 的理论与实现
[日]斋藤康毅 著
陆宇杰 译
深度学习入门
Beijing・Boston・Farnham・Sebastopol・Tokyo
O’Reilly Japan, Inc. 授权人民邮电出版社出版
Deep Learning from Scratch


内容提要
本书是深度学习真正意义上的入门书,深入浅出地剖析了深度学习的原理和相关技术。 书中使用 Python 3,尽量不依赖外部库或工具,带领读者从零创建一个经典的深度学习网 络,使读者在此过程中逐步理解深度学习。书中不仅介绍了深度学习和神经网络的概念、 特征等基础知识,对误差反向传播法、卷积神经网络等也有深入讲解,此外还介绍了学 习相关的实用技巧,自动驾驶、图像生成、强化学习等方面的应用,以及为什么加深层 可以提高识别精度等“为什么”的问题。 本书适合深度学习初学者阅读,也可作为高校教材使用。
◆ 著 [日]斋藤康毅 译 陆宇杰 责任编辑 杜晓静 执行编辑 刘香娣 责任印制 周昇亮 ◆ 人民邮电出版社出版发行 北京市丰台区成寿寺路 11 号 邮编 100164 电子邮件 315@ptpress.com.cn
网址 http://www.ptpress.com.cn
北京 印刷 ◆ 开本:880×1230 1/32
印张:9.625
字数:300 千字 2018 年 7 月第 1 版
印数:1 - 4 000 册 2018 年 7 月北京第 1 次印刷
著作权合同登记号 图字:01-2017-0526 号
定价:59.00 元
读者服务热线:(010)51095186转 600 印装质量热线:(010)81055316
反盗版热线:(010)81055315
广告经营许可证:京东工商广登字 20170147 号
深度学习入门 : 基于Python的理论与实现 / (日) 斋藤康毅著 ; 陆宇杰译. -- 北京 : 人民邮电出版社, 2018.7 (图灵程序设计丛书) ISBN 978-7-115-48558-8
I. 1深... II. 1斋... 2陆... III. 1软件工具-程 序设计 IV. 1TP311.561
中国版本图书馆CIP数据核字(2018)第112509号
图书在版编目(CIP)数据


版权声明
Copyright © 2016 Koki Saitoh, O’Reilly Japan, Inc.
Posts and Telecommunications Press, 2018.
Authorized translation of the Japanese edition of “Deep Learning from
Scratch” © 2016 O’ Reilly Japan, Inc. This translation is published and sold by
permission of O’ Reilly Japan, Inc., the owner of all rights to publish and sell the
same.
日文原版由 O’Reilly Japan, Inc. 出版,2016。
简体中文版由人民邮电出版社出版,2018。日文原版的翻译得到 O’Reilly Japan, Inc. 的授权。此简体中文版的出版和销售得到出版权和销售权的所有 者——O’Reilly Japan, Inc. 的许可。
版权所有,未得书面许可,本书的任何部分和全部不得以任何形式重制。


O’Reilly Media 通过图书、杂志、在线服务、调查研究和会议等方式传播创新知识。
自 1978 年开始,O’Reilly 一直都是前沿发展的见证者和推动者。超级极客们正在开
创着未来,而我们关注真正重要的技术趋势——通过放大那些“细微的信号”来刺激
社会对新科技的应用。作为技术社区中活跃的参与者,O’Reilly 的发展充满了对创新
的倡导、创造和发扬光大。
O’Reilly 为软件开发人员带来革命性的“动物书”;创建第一个商业网站(GNN);组
织了影响深远的开放源代码峰会,以至于开源软件运动以此命名;创立了 Make 杂志,
从而成为 DIY 革命的主要先锋;公司一如既往地通过多种形式缔结信息与人的纽带。
O’Reilly 的会议和峰会集聚了众多超级极客和高瞻远瞩的商业领袖,共同描绘出开创
新产业的革命性思想。作为技术人士获取信息的选择,O’Reilly 现在还将先锋专家的
知识传递给普通的计算机用户。无论是通过书籍出版、在线服务或者面授课程,每一
项 O’Reilly 的产品都反映了公司不可动摇的理念——信息是激发创新的力量。
业界评论
“O’Reilly Radar 博客有口皆碑。”
——Wired
“O’Reilly 凭借一系列(真希望当初我也想到了)非凡想法建立了数百万美元的业务。”
——Business 2.0
“O’Reilly Conference 是聚集关键思想领袖的绝对典范。”
——CRN
“一本 O’Reilly 的书就代表一个有用、有前途、需要学习的主题。”
——Irish Times
“Tim 是位特立独行的商人,他不光放眼于最长远、最广阔的视野,并且切实地按照
Yogi Berra 的建议去做了:‘如果你在路上遇到岔路口,走小路(岔路)。’回顾过去,
Tim 似乎每一次都选择了小路,而且有几次都是一闪即逝的机会,尽管大路也不错。”
——Linux Journal
O’Reilly Media, Inc.介绍


目录
译者序······················································· xiii
前言························································· xv
第 1 章 Python 入门· ··········································· 1
1.1 Python 是什么· ········································· 1
1.2 Python 的安装· ········································· 2
1.2.1 Python 版本· ····································· 2
1.2.2 使用的外部库· ···································· 2
1.2.3 Anaconda 发行版· ································· 3
1.3 Python 解释器· ········································· 4
1.3.1 算术计算········································· 4
1.3.2 数据类型········································· 5
1.3.3 变量············································· 5
1.3.4 列表············································· 6
1.3.5 字典············································· 7
1.3.6 布尔型··········································· 7
1.3.7 if 语句· ·········································· 8
1.3.8 for 语句·········································· 8
1.3.9 函数············································· 9
1.4 Python 脚本文件· ······································· 9


目录
vi
1.4.1 保存为文件······································· 9
1.4.2 类· ············································ 10
1.5 NumPy· ·············································· 11
1.5.1 导入 NumPy· ···································· 11
1.5.2 生成 NumPy 数组· ································ 12
1.5.3 NumPy 的算术运算······························· 12
1.5.4 NumPy 的 N 维数组· ······························ 13
1.5.5 广播············································ 14
1.5.6 访问元素········································ 15
1.6 Matplotlib············································· 16
1.6.1 绘制简单图形· ··································· 16
1.6.2 pyplot 的功能· ··································· 17
1.6.3 显示图像········································ 18
1.7 小结·················································· 19
第 2 章 感知机················································ 21
2.1 感知机是什么· ········································· 21
2.2 简单逻辑电路· ········································· 23
2.2.1 与门············································ 23
2.2.2 与非门和或门· ··································· 23
2.3 感知机的实现· ········································· 25
2.3.1 简单的实现······································ 25
2.3.2 导入权重和偏置· ································· 26
2.3.3 使用权重和偏置的实现· ··························· 26
2.4 感知机的局限性· ······································· 28
2.4.1 异或门·········································· 28
2.4.2 线性和非线性· ··································· 30
2.5 多层感知机············································ 31
2.5.1 已有门电路的组合· ······························· 31


目录 vii
2.5.2 异或门的实现· ··································· 33
2.6 从与非门到计算机· ····································· 35
2.7 小结·················································· 36
第 3 章 神经网络·············································· 37
3.1 从感知机到神经网络· ··································· 37
3.1.1 神经网络的例子· ································· 37
3.1.2 复习感知机······································ 38
3.1.3 激活函数登场· ··································· 40
3.2 激活函数·············································· 42
3.2.1 sigmoid 函数· ···································· 42
3.2.2 阶跃函数的实现· ································· 43
3.2.3 阶跃函数的图形· ································· 44
3.2.4 sigmoid 函数的实现· ······························ 45
3.2.5 sigmoid 函数和阶跃函数的比较······················ 46
3.2.6 非线性函数······································ 48
3.2.7 ReLU 函数· ····································· 49
3.3 多维数组的运算· ······································· 50
3.3.1 多维数组········································ 50
3.3.2 矩阵乘法········································ 51
3.3.3 神经网络的内积· ································· 55
3.4 3 层神经网络的实现· ···································· 56
3.4.1 符号确认········································ 57
3.4.2 各层间信号传递的实现· ··························· 58
3.4.3 代码实现小结· ··································· 62
3.5 输出层的设计· ········································· 63
3.5.1 恒等函数和 softmax 函数· ·························· 64
3.5.2 实现 softmax 函数时的注意事项· ···················· 66
3.5.3 softmax 函数的特征· ······························ 67


目录
viii
3.5.4 输出层的神经元数量· ····························· 68
3.6 手写数字识别· ········································· 69
3.6.1 MNIST 数据集· ·································· 70
3.6.2 神经网络的推理处理· ····························· 73
3.6.3 批处理·········································· 75
3.7 小结·················································· 79
第 4 章 神经网络的学习· ······································· 81
4.1 从数据中学习· ········································· 81
4.1.1 数据驱动········································ 82
4.1.2 训练数据和测试数据· ····························· 84
4.2 损失函数·············································· 85
4.2.1 均方误差········································ 85
4.2.2 交叉熵误差······································ 87
4.2.3 mini-batch 学习· ································· 88
4.2.4 mini-batch 版交叉熵误差的实现· ···················· 91
4.2.5 为何要设定损失函数· ····························· 92
4.3 数值微分·············································· 94
4.3.1 导数············································ 94
4.3.2 数值微分的例子· ································· 96
4.3.3 偏导数·········································· 98
4.4 梯度··················································100
4.4.1 梯度法··········································102
4.4.2 神经网络的梯度· ·································106
4.5 学习算法的实现· ·······································109
4.5.1 2 层神经网络的类·································110
4.5.2 mini-batch 的实现· ·······························114
4.5.3 基于测试数据的评价· ·····························116
4.6 小结··················································118


目录 ix
第 5 章 误差反向传播法· ·······································121
5.1 计算图················································121
5.1.1 用计算图求解· ···································122
5.1.2 局部计算········································124
5.1.3 为何用计算图解题· ·······························125
5.2 链式法则··············································126
5.2.1 计算图的反向传播· ·······························127
5.2.2 什么是链式法则· ·································127
5.2.3 链式法则和计算图· ·······························129
5.3 反向传播··············································130
5.3.1 加法节点的反向传播· ·····························130
5.3.2 乘法节点的反向传播· ·····························132
5.3.3 苹果的例子······································133
5.4 简单层的实现· ·········································135
5.4.1 乘法层的实现· ···································135
5.4.2 加法层的实现· ···································137
5.5 激活函数层的实现· ·····································139
5.5.1 ReLU 层· ·······································139
5.5.2 Sigmoid 层·······································141
5.6 Affine/Softmax 层的实现·································144
5.6.1 Affine 层· ·······································144
5.6.2 批版本的 Affine 层· ·······························148
5.6.3 Softmax-with-Loss 层· ····························150
5.7 误差反向传播法的实现· ·································154
5.7.1 神经网络学习的全貌图· ···························154
5.7.2 对应误差反向传播法的神经网络的实现· ··············155
5.7.3 误差反向传播法的梯度确认·························158
5.7.4 使用误差反向传播法的学习·························159
5.8 小结··················································161


目录
x
第 6 章 与学习相关的技巧· ·····································163
6.1 参数的更新············································163
6.1.1 探险家的故事· ···································164
6.1.2 SGD· ··········································164
6.1.3 SGD 的缺点· ····································166
6.1.4 Momentum······································168
6.1.5 AdaGrad········································170
6.1.6 Adam· ·········································172
6.1.7 使用哪种更新方法呢· ·····························174
6.1.8 基于 MNIST 数据集的更新方法的比较················175
6.2 权重的初始值· ·········································176
6.2.1 可以将权重初始值设为 0 吗· ························176
6.2.2 隐藏层的激活值的分布· ···························177
6.2.3 ReLU 的权重初始值·······························181
6.2.4 基于 MNIST 数据集的权重初始值的比较· ·············183
6.3 Batch Normalization· ···································184
6.3.1 Batch Normalization 的算法· ·······················184
6.3.2 Batch Normalization 的评估· ·······················186
6.4 正则化················································188
6.4.1 过拟合··········································189
6.4.2 权值衰减········································191
6.4.3 Dropout· ·······································192
6.5 超参数的验证· ·········································195
6.5.1 验证数据········································195
6.5.2 超参数的最优化· ·································196
6.5.3 超参数最优化的实现· ·····························198
6.6 小结··················································200


目录 xi
第 7 章 卷积神经网络· ·········································201
7.1 整体结构··············································201
7.2 卷积层················································202
7.2.1 全连接层存在的问题· ·····························203
7.2.2 卷积运算········································203
7.2.3 填充············································206
7.2.4 步幅············································207
7.2.5 3 维数据的卷积运算· ······························209
7.2.6 结合方块思考· ···································211
7.2.7 批处理··········································213
7.3 池化层················································214
7.4 卷积层和池化层的实现· ·································216
7.4.1 4 维数组· ·······································216
7.4.2 基于 im2col 的展开· ·······························217
7.4.3 卷积层的实现· ···································219
7.4.4 池化层的实现· ···································222
7.5 CNN 的实现· ··········································224
7.6 CNN 的可视化· ········································228
7.6.1 第 1 层权重的可视化·······························228
7.6.2 基于分层结构的信息提取· ·························230
7.7 具有代表性的 CNN·····································231
7.7.1 LeNet· ·········································231
7.7.2 AlexNet·········································232
7.8 小结··················································233
第 8 章 深度学习··············································235
8.1 加深网络··············································235
8.1.1 向更深的网络出发· ·······························235
8.1.2 进一步提高识别精度· ·····························238


目录
xii
8.1.3 加深层的动机· ···································240
8.2 深度学习的小历史· ·····································242
8.2.1 ImageNet· ······································243
8.2.2 VGG· ··········································244
8.2.3 GoogLeNet· ·····································245
8.2.4 ResNet· ········································246
8.3 深度学习的高速化· ·····································248
8.3.1 需要努力解决的问题· ·····························248
8.3.2 基于 GPU 的高速化· ······························249
8.3.3 分布式学习······································250
8.3.4 运算精度的位数缩减· ·····························252
8.4 深度学习的应用案例· ···································253
8.4.1 物体检测········································253
8.4.2 图像分割········································255
8.4.3 图像标题的生成· ·································256
8.5 深度学习的未来· ·······································258
8.5.1 图像风格变换· ···································258
8.5.2 图像的生成······································259
8.5.3 自动驾驶········································261
8.5.4 Deep Q-Network(强化学习)· ·······················262
8.6 小结··················································264
附录 A Softmax-with-Loss 层的计算图· ···························267
A.1 正向传播· ············································268
A.2 反向传播· ············································270
A.3 小结· ················································277
参考文献· ····················································279


译者序
深度学习的浪潮已经汹涌澎湃了一段时间了,市面上相关的图书也已经
出版了很多。其中,既有知名学者伊恩·古德费洛(Ian Goodfellow)等人撰
写的系统介绍深度学习基本理论的《深度学习》,也有各种介绍深度学习框
架的使用方法的入门书。你可能会问,现在再出一本关于深度学习的书,是
不是“为时已晚”?其实并非如此,因为本书考察深度学习的角度非常独特,
它的出版可以说是“千呼万唤始出来”。
本书最大的特点是“剖解”了深度学习的底层技术。正如美国物理学家
理 查 德·费 曼(Richard Phillips Feynman)所 说:“What I cannot create, I
do not understand.”只有创造一个东西,才算真正弄懂了一个问题。本书就
是教你如何创建深度学习模型的一本书。并且,本书不使用任何现有的深度
学习框架,尽可能仅使用最基本的数学知识和 Python 库,从零讲解深度学
习核心问题的数学原理,从零创建一个经典的深度学习网络。
本书的日文版曾一度占据了东京大学校内书店(本乡校区)理工类图书
的畅销书榜首。各类读者阅读本书,均可有所受益。对于非 AI 方向的技术
人员,本书将大大降低入门深度学习的门槛;对于在校的大学生、研究生,
本书不失为学习深度学习的一本好教材;即便是对于在工作中已经熟练使用
框架开发各类深度学习模型的读者,也可以从本书中获得新的体会。
本书从开始翻译到出版,前前后后历时一年之久。译者翻译时力求忠于
原文,表达简练。为了保证翻译质量,每翻译完一章后,译者都会放置一段


译者序
xiv
时间,再重新检查一遍。图灵公司的专业编辑们又进一步对译稿进行了全面
细致的校对,提出了许多宝贵意见,在此表示感谢。但是,由于译者才疏学浅,
书中难免存在一些错误或疏漏,恳请读者批评指正,以便我们在重印时改正。
最后,希望本书的出版能为国内的 AI 技术社区添砖加瓦!
陆宇杰
2018 年 2 月 上海


前言
科幻电影般的世界已经变成了现实—人工智能战胜过日本将棋、国际
象棋的冠军,最近甚至又打败了围棋冠军;智能手机不仅可以理解人们说的话,
还能在视频通话中进行实时的“机器翻译”;配备了摄像头的“自动防撞的车”
保护着人们的生命安全,自动驾驶技术的实用化也为期不远。环顾我们的四
周,原来被认为只有人类才能做到的事情,现在人工智能都能毫无差错地完
成,甚至试图超越人类。因为人工智能的发展,我们所处的世界正在逐渐变
成一个崭新的世界。
在这个发展速度惊人的世界背后,深度学习技术在发挥着重要作用。对
于深度学习,世界各地的研究人员不吝褒奖之辞,称赞其为革新性技术,甚
至有人认为它是几十年才有一次的突破。实际上,深度学习这个词经常出现
在报纸和杂志中,备受关注,就连一般大众也都有所耳闻。
本书就是一本以深度学习为主题的书,目的是让读者尽可能深入地理解
深度学习的技术。因此,本书提出了“从零开始”这个概念。
本书的特点是通过实现深度学习的过程,来逼近深度学习的本质。通过
实现深度学习的程序,尽可能无遗漏地介绍深度学习相关的技术。另外,本
书还提供了实际可运行的程序,供读者自己进行各种各样的实验。
为了实现深度学习,我们需要经历很多考验,花费很长时间,但是相应
地也能学到和发现很多东西。而且,实现深度学习的过程是一个有趣的、令


前言
xvi
人兴奋的过程。希望读者通过这一过程可以熟悉深度学习中使用的技术,并
能从中感受到快乐。
目前,深度学习活跃在世界上各个地方。在几乎人手一部的智能手机中、
开启自动驾驶的汽车中、为 Web 服务提供动力的服务器中,深度学习都在
发挥着作用。此时此刻,就在很多人没有注意到的地方,深度学习正在默默
地发挥着其功能。今后,深度学习势必将更加活跃。为了让读者理解深度学
习的相关技术,感受到深度学习的魅力,笔者写下了本书。
本书的理念
本书是一本讲解深度学习的书,将从最基础的内容开始讲起,逐一介绍
理解深度学习所需的知识。书中尽可能用平实的语言来介绍深度学习的概念、
特征、工作原理等内容。不过,本书并不是只介绍技术的概要,而是旨在让
读者更深入地理解深度学习。这是本书的特色之一。
那么,怎么才能更深入地理解深度学习呢?在笔者看来,最好的办法就
是亲自实现。从零开始编写可实际运行的程序,一边看源代码,一边思考。
笔者坚信,这种做法对正确理解深度学习(以及那些看上去很高级的技术)
是很重要的。这里用了“从零开始”一词,表示我们将尽可能地不依赖外部
的现成品(库、工具等)。也就是说,本书的目标是,尽量不使用内容不明的
黑盒,而是从自己能理解的最基础的知识出发,一步一步地实现最先进的深
度学习技术。并通过这一实现过程,使读者加深对深度学习的理解。
如果把本书比作一本关于汽车的书,那么本书并不会教你怎么开车,其
着眼点不是汽车的驾驶方法,而是要让读者理解汽车的原理。为了让读者理
解汽车的结构,必须打开汽车的引擎盖,把零件一个一个地拿在手里观察,
并尝试操作它们。之后,用尽可能简单的形式提取汽车的本质,并组装汽车
模型。本书的目标是,通过制造汽车模型的过程,让读者感受到自己可以实
际制造出汽车,并在这一过程中熟悉汽车相关的技术。
为了实现深度学习,本书使用了 Python 这一编程语言。Python 非常受
欢迎,初学者也能轻松使用。Python 尤其适合用来制作样品(原型),使用


前言 xvii
Python 可以立刻尝试突然想到的东西,一边观察结果,一边进行各种各样
的实验。本书将在讲解深度学习理论的同时,使用 Python 实现程序,进行
各种实验。
在光看数学式和理论说明无法理解的情况下,可以尝试阅读源代码
并运行,很多时候思路都会变得清晰起来。对数学式感到困惑时,
就阅读源代码来理解技术的流程,这样的事情相信很多人都经历过。
本书通过实际实现(落实到代码)来理解深度学习,是一本强调“工程”
的书。书中会出现很多数学式,但同时也会有很多程序员视角的源代码。
本书面向的读者
本书旨在让读者通过实际动手操作来深入理解深度学习。为了明确本书
的读者对象,这里将本书涉及的内容列举如下。
• 使用 Python,尽可能少地使用外部库,从零开始实现深度学习的程序。
• 为了让 Python 的初学者也能理解,介绍 Python 的使用方法。
• 提供实际可运行的 Python 源代码,同时提供可以让读者亲自实验的
学习环境。
• 从简单的机器学习问题开始,最终实现一个能高精度地识别图像的系统。
• 以简明易懂的方式讲解深度学习和神经网络的理论。
• 对于误差反向传播法、卷积运算等乍一看很复杂的技术,使读者能够
在实现层面上理解。
• 介绍一些学习深度学习时有用的实践技巧,如确定学习率的方法、权
重的初始值等。
• 介绍最近流行的 Batch Normalization、Dropout、Adam 等,并进行
实现。
• 讨论为什么深度学习表现优异、为什么加深层能提高识别精度、为什
么隐藏层很重要等问题。
• 介绍自动驾驶、图像生成、强化学习等深度学习的应用案例。


前言
xviii
本书不面向的读者
明确本书不适合什么样的读者也很重要。为此,这里将本书不会涉及的
内容列举如下。
• 不介绍深度学习相关的最新研究进展。
• 不介绍 Caffe、TensorFlow、Chainer 等深度学习框架的使用方法。
• 不介绍深度学习的详细理论,特别是神经网络相关的详细理论。
• 不详细介绍用于提高识别精度的参数调优相关的内容。
• 不会为了实现深度学习的高速化而进行 GPU 相关的实现。
• 本书以图像识别为主题,不涉及自然语言处理或者语音识别的例子。
综上,本书不涉及最新研究和理论细节。但是,读完本书之后,读者
应该有能力进一步去阅读最新的论文或者神经网络相关的理论方面的技
术书。
本书以图像识别为主题,主要学习使用深度学习进行图像识别时
所需的技术。自然语言处理或者语音识别等不是本书的讨论对象。
本书的阅读方法
学习新知识时,只听别人讲解的话,有时会无法理解,或者会立刻忘记。
正如“不闻不若闻之,闻之不若见之,见之不若知之,知之不若行之 ”A ,在
学习新东西时,没有什么比实践更重要了。本书在介绍某个主题时,都细心
地准备了一个可以实践的场所——能够作为程序运行的源代码。
本书会提供 Python 源代码,读者可以自己动手实际运行这些源代码。
在阅读源代码的同时,可以尝试去实现一些自己想到的东西,以确保真正
A 出自荀子《儒效篇》。


前言 xix
理解了。另外,读者也可以使用本书的源代码,尝试进行各种实验,反复
试错。
本书将沿着“理论说明”和“Python 实现”两个路线前进。因此,建议
读者准备好编程环境。本书可以使用 Windows、Mac、Linux 中的任何一个
系统。关于 Python 的安装和使用方法将在第 1 章介绍。另外,本书中用到
的程序可以从以下网址下载。
http://www.ituring.com.cn/book/1921
让我们开始吧
通过前面的介绍,希望读者了解本书大概要讲的内容,产生继续阅读的
兴趣。
最近出现了很多深度学习相关的库,任何人都可以方便地使用。实际上,
使用这些库的话,可以轻松地运行深度学习的程序。那么,为什么我们还要
特意花时间从零开始实现深度学习呢?一个理由就是,在制作东西的过程中
可以学到很多。
在制作东西的过程中,会进行各种各样的实验,有时也会卡住,抱着脑
袋想为什么会这样。这种费时的工作对深刻理解技术而言是宝贵的财富。像
这样认真花费时间获得的知识在使用现有的库、阅读最新的文章、创建原创
的系统时都大有用处。而且最重要的是,制作本身就是一件快乐的事情。(还
需要快乐以外的其他什么理由吗?)
既然一切都准备好了,下面就让我们踏上实现深度学习的旅途吧!
表述规则
本书在表述上采用如下规则。
粗体字(Bold)
用来表示新引入的术语、强调的要点以及关键短语。


前言
xx
等宽字(Constant Width)
用来表示下面这些信息:程序代码、命令、序列、组成元素、语句选项、
分支、变量、属性、键值、函数、类型、类、命名空间、方法、模块、属性、
参数、值、对象、事件、事件处理器、XML 标签、HTML 标签、宏、文件
的内容、来自命令行的输出等。若在其他地方引用了以上这些内容(如变量、
函数、关键字等),也会使用该格式标记。
等宽粗体字(Constant Width Bold)
用来表示用户输入的命令或文本信息。在强调代码的作用时也会使用该
格式标记。
等宽斜体字(Constant Width Italic)
用来表示必须根据用户环境替换的字符串。
用来表示提示、启发以及某些值得深究的内容的补充信息。
表示程序库中存在的 bug 或时常会发生的问题等警告信息,引
起读者对该处内容的注意。
读者意见与咨询
虽然笔者已经尽最大努力对本书的内容进行了验证与确认,但仍不免在
某些地方出现错误或者容易引起误解的表达等,给读者的理解带来困扰。如
果读者遇到这些问题,请及时告知,我们在本书重印时会将其改正,在此先
表示不胜感激。与此同时,也希望读者能够为本书将来的修订提出中肯的建
议。本书编辑部的联系方式如下。


前言 xxi
株式会社 O’Reilly Japan
电子邮件 japan@oreilly.co.jp
本书的主页地址如下。
http://www.ituring.com.cn/book/1583
http://www.oreilly.co.jp/books/9784873117584(日语)
https://github.com/oreilly-japan/deep-learning-from-scratch
关于 O’Reilly 的其他信息,可以访问下面的 O’Reilly 主页查看。
http://www.oreilly.com/(英语)
http://www.oreilly.co.jp/(日语)
致谢
首先,笔者要感谢推动了深度学习相关技术(机器学习、计算机科学等)
发展的研究人员和工程师。本书的完成离不开他们的研究工作。其次,笔者
还要感谢在图书或网站上公开有用信息的各位同仁。其中,斯坦福大学的
CS231n [5] 公开课慷慨提供了很多有用的技术和信息,笔者从中学到了很多东西。
在本书执笔过程中,曾受到下列人士的帮助:teamLab 公司的加藤哲朗、
喜多慎弥、飞永由夏、中野皓太、中村将达、林辉大、山本辽;Top Studio
公司的武藤健志、增子萌;Flickfit 公司的野村宪司;得克萨斯大学奥斯汀
分校 JSPS 海外特别研究员丹野秀崇。他们阅读了本书原稿,提出了很多宝
贵的建议,在此深表谢意。另外,需要说明的是,本书中存在的不足或错误
均是笔者的责任。
最后,还要感谢 O’Reilly Japan 的宮川直树,在从本书的构想到完成的
大约一年半的时间里,宫川先生一直支持着笔者。非常感谢!
2016 年 9 月 1 日
斋藤康毅




第1章
Python 入门
Python 这一编程语言已经问世 20 多年了,在这期间,Python 不仅完成
了自身的进化,还获得了大量的用户。现在,Python 作为最具人气的编程语言,
受到了许多人的喜爱。
接下来我们将使用 Python 实现深度学习系统。不过在这之前,本章将简
单地介绍一下 Python,看一下它的使用方法。已经掌握了 Python、NumPy、
Matplotlib 等知识的读者,可以跳过本章,直接阅读后面的章节。
1.1 Python 是什么
Python 是一个简单、易读、易记的编程语言,而且是开源的,可以免
费地自由使用。Python 可以用类似英语的语法编写程序,编译起来也不费
力,因此我们可以很轻松地使用 Python。特别是对首次接触编程的人士来说,
Python 是最合适不过的语言。事实上,很多高校和大专院校的计算机课程
均采用 Python 作为入门语言。
此外,使用 Python 不仅可以写出可读性高的代码,还可以写出性能高(处
理速度快)的代码。在需要处理大规模数据或者要求快速响应的情况下,使
用 Python 可以稳妥地完成。因此,Python 不仅受到初学者的喜爱,同时也
受到专业人士的喜爱。实际上,Google、Microsoft、Facebook 等战斗在 IT
行业最前沿的企业也经常使用 Python。


第 1 章 Python 入门
2
再者,在科学领域,特别是在机器学习、数据科学领域,Python 也被
大 量 使 用。Python 除 了 高 性 能 之 外,凭 借 着 NumPy、SciPy 等 优 秀 的 数
值计算、统计分析库,在数据科学领域占有不可动摇的地位。深度学习的
框架中也有很多使用 Python 的场景,比如 Caffe、TensorFlow、Chainer、
Theano 等著名的深度学习框架都提供了 Python 接口。因此,学习 Python
对使用深度学习框架大有益处。
综上,Python 是最适合数据科学领域的编程语言。而且,Python 具有
受众广的优秀品质,从初学者到专业人士都在使用。因此,为了完成本书的
从零开始实现深度学习的目标,Python 可以说是最合适的工具。
1.2 Python 的安装
下面,我们首先将 Python 安装到当前环境(电脑)上。这里说明一下安
装时需要注意的一些地方。
1.2.1 Python 版本
Python 有 Python 2.x 和 Python 3.x 两个版本。如果我们调查一下目前
Python 的使用情况,会发现除了最新的版本 3.x 以外,旧的版本 2.x 仍在被
大量使用。因此,在安装 Python 时,需要慎重选择安装 Python 的哪个版
本。这是因为两个版本之间没有兼容性(严格地讲,是没有“向后兼容性”),
也就是说,会发生用 Python 3.x 写的代码不能被 Python 2.x 执行的情况。
本 书 中 使 用 Python 3.x,只 安 装 了 Python 2.x 的 读 者 建 议 另 外 安 装 一 下
Python 3.x。
1.2.2 使用的外部库
本书的目标是从零开始实现深度学习。因此,除了 NumPy 库和 Matplotlib
库之外,我们极力避免使用外部库。之所以使用这两个库,是因为它们可以
有效地促进深度学习的实现。


1.2 Python 的安装 3
NumPy 是用于数值计算的库,提供了很多高级的数学算法和便利的数
组(矩阵)操作方法。本书中将使用这些便利的方法来有效地促进深度学习
的实现。
Matplotlib 是用来画图的库。使用 Matplotlib 能将实验结果可视化,并
在视觉上确认深度学习运行期间的数据。
本书将使用下列编程语言和库。
• Python 3.x(2016 年 8 月时的最新版本是 3.5)
• NumPy
• Matplotlib
下面将为需要安装 Python 的读者介绍一下 Python 的安装方法。已经安
装了 Python 的读者,请跳过这一部分内容。
1.2.3 Anaconda 发行版
Python 的安装方法有很多种,本书推荐使用 Anaconda 这个发行版。发
行版集成了必要的库,使用户可以一次性完成安装。Anaconda 是一个侧重
于数据分析的发行版,前面说的 NumPy、Matplotlib 等有助于数据分析的
库都包含在其中 A。
如前所述,本书将使用 Python 3.x 版本,因此 Anaconda 发行版也要安
装 3.x 的版本。请读者从官方网站下载与自己的操作系统相应的发行版,然
后安装。
A Anaconda 作为一个针对数据分析的发行版,包含了许多有用的库,而本书中实际上只会使用其中的 NumPy 库和 Matplotlib 库。因此,如果想保持轻量级的开发环境,单独安装这两个库也是可以的。 ——译者注


第 1 章 Python 入门
4
1.3 Python 解释器
完成 Python 的安装后,要先确认一下 Python 的版本。打开终端(Windows
中的命令行窗口),输入 python --version 命令,该命令会输出已经安装的
Python 的版本信息。
$ python --version
Python 3.4.1 :: Anaconda 2.1.0 (x86_64)
如上所示,显示了 Python 3.4.1(根据实际安装的版本,版本号可能不同),
说明已正确安装了 Python 3.x。接着输入 python,启动 Python 解释器。
$ python
Python 3.4.1 |Anaconda 2.1.0 (x86_64)| (default, Sep 10 2014, 17:24:09) [GCC 4.2.1 (Apple Inc. build 5577)] on darwin Type "help", "copyright", "credits" or "license" for more information. >>>
Python 解释器也被称为“对话模式”,用户能够以和 Python 对话的方式
进行编程。比如,当用户询问“1 + 2 等于几?”的时候,Python 解释器会回
答“3”,所谓对话模式,就是指这样的交互。现在,我们实际输入一下看看。
>>> 1 + 2 3
Python 解释器可以像这样进行对话式(交互式)的编程。下面,我们使
用这个对话模式,来看几个简单的 Python 编程的例子。
1.3.1 算术计算
加法或乘法等算术计算,可按如下方式进行。
>>> 1 - 2 -1 >>> 4 * 5 20


1.3 Python 解释器 5
>>> 7 / 5 1.4 >>> 3 ** 2 9
* 表 示 乘 法,/ 表 示 除 法,** 表 示 乘 方(3**2 是 3 的 2 次 方)。另 外,在
Python 2.x 中,整数除以整数的结果是整数,比如,7 ÷ 5 的结果是 1。但在
Python 3.x 中,整数除以整数的结果是小数(浮点数)。
1.3.2 数据类型
编程中有数据类型(data type)这一概念。数据类型表示数据的性质,
有整数、小数、字符串等类型。Python 中的 type() 函数可以用来查看数据
类型。
>>> type(10) <class 'int'> >>> type(2.718) <class 'float'> >>> type("hello") <class 'str'>
根据上面的结果可知,10 是 int 类型(整型),2.718 是 float 类型(浮点型),
"hello" 是 str(字符串)类型。另外,“类型”和“类”这两个词有时用作相同
的意思。这里,对于输出结果 <class 'int'>,可以将其解释成“10 是 int 类(类
型)”。
1.3.3 变量
可以使用 x 或 y 等字母定义变量(variable)。此外,可以使用变量进行计算,
也可以对变量赋值。
>>> x = 10 # 初始化 >>> print(x) # 输出 x 10
>>> x = 100 # 赋值 >>> print(x) 100


第 1 章 Python 入门
6
>>> y = 3.14 >>> x * y 314.0
>>> type(x * y) <class 'float'>
Python 是属于“动态类型语言”的编程语言,所谓动态,是指变量的类
型是根据情况自动决定的。在上面的例子中,用户并没有明确指出“x 的类
型是 int(整型)”,是 Python 根据 x 被初始化为 10,从而判断出 x 的类型为
int 的。此外,我们也可以看到,整数和小数相乘的结果是小数(数据类型的
自动转换)。另外,“#”是注释的意思,它后面的文字会被 Python 忽略。
1.3.4 列表
除了单一的数值,还可以用列表(数组)汇总数据。
>>> a = [1, 2, 3, 4, 5] # 生成列表 >>> print(a) # 输出列表的内容 [1, 2, 3, 4, 5]
>>> len(a) # 获取列表的长度 5
>>> a[0] # 访问第一个元素的值 1
>>> a[4] 5
>>> a[4] = 99 # 赋值 >>> print(a)
[1, 2, 3, 4, 99]
元素的访问是通过 a[0] 这样的方式进行的。[] 中的数字称为索引(下标),
索引从 0 开始(索引 0 对应第一个元素)。此外,Python 的列表提供了切片
(slicing)这一便捷的标记法。使用切片不仅可以访问某个值,还可以访问列
表的子列表(部分列表)。
>>> print(a)
[1, 2, 3, 4, 99]
>>> a[0:2] # 获取索引为 0 到 2(不包括 2 !)的元素 [1, 2]
>>> a[1:] # 获取从索引为 1 的元素到最后一个元素 [2, 3, 4, 99]


1.3 Python 解释器 7
>>> a[:3] # 获取从第一个元素到索引为 3(不包括 3 !)的元素 [1, 2, 3]
>>> a[:-1] # 获取从第一个元素到最后一个元素的前一个元素之间的元素 [1, 2, 3, 4]
>>> a[:-2] # 获取从第一个元素到最后一个元素的前二个元素之间的元素 [1, 2, 3]
进行列表的切片时,需要写成 a[0:2] 这样的形式。a[0:2] 用于取出从索
引为 0 的元素到索引为 2 的元素的前一个元素之间的元素。另外,索引 −1 对
应最后一个元素,−2 对应最后一个元素的前一个元素。
1.3.5 字典
列表根据索引,按照 0, 1, 2, . . . 的顺序存储值,而字典则以键值对的形
式存储数据。字典就像《新华字典》那样,将单词和它的含义对应着存储起来。
>>> me = {'height':180} # 生成字典 >>> me['height'] # 访问元素 180
>>> me['weight'] = 70 # 添加新元素 >>> print(me)
{'height': 180, 'weight': 70}
1.3.6 布尔型
Python 中有 bool 型。bool 型取 True 或 False 中的一个值。针对 bool 型的
运算符包括 and、or 和 not(针对数值的运算符有 +、-、*、/ 等,根据不同的
数据类型使用不同的运算符)。
>>> hungry = True # 饿了? >>> sleepy = False # 困了? >>> type(hungry) <class 'bool'> >>> not hungry False
>>> hungry and sleepy # 饿并且困 False
>>> hungry or sleepy # 饿或者困 True


第 1 章 Python 入门
8
1.3.7 if 语句
根据不同的条件选择不同的处理分支时可以使用 if/else 语句。
>>> hungry = True >>> if hungry: ... print("I'm hungry") ... I'm hungry
>>> hungry = False >>> if hungry: ... print("I'm hungry") # 使用空白字符进行缩进 ... else: ... print("I'm not hungry") ... print("I'm sleepy") ... I'm not hungry I'm sleepy
Python中的空白字符具有重要的意义。上面的 if 语句中,if hungry: 下面
的语句开头有4个空白字符。它是缩进的意思,表示当前面的条件(if hungry)
成立时,此处的代码会被执行。这个缩进也可以用 tab 表示,Python 中推荐
使用空白字符。
Python 使用空白字符表示缩进。一般而言,每缩进一次,使用 4
个空白字符。
1.3.8 for 语句
进行循环处理时可以使用 for 语句。
>>> for i in [1, 2, 3]: ... print(i) ... 1 2 3
这是输出列表 [1, 2, 3] 中的元素的例子。使用 for ��� in ��� :语句结构,


1.4 Python 脚本文件 9
可以按顺序访问列表等数据集合中的各个元素。
1.3.9 函数
可以将一连串的处理定义成函数(function)。
>>> def hello(): ... print("Hello World!") ...
>>> hello() Hello World!
此外,函数可以取参数。
>>> def hello(object): ... print("Hello " + object + "!") ...
>>> hello("cat") Hello cat!
另外,字符串的拼接可以使用 +。
关闭 Python 解释器时,Linux 或 Mac OS X 的情况下输入 Ctrl-D(按住
Ctrl,再按 D 键);Windows 的情况下输入 Ctrl-Z,然后按 Enter 键。
1.4 Python 脚本文件
到目前为止,我们看到的都是基于 Python 解释器的例子。Python 解释
器能够以对话模式执行程序,非常便于进行简单的实验。但是,想进行一
连串的处理时,因为每次都需要输入程序,所以不太方便。这时,可以将
Python 程序保存为文件,然后(集中地)运行这个文件。下面,我们来看一
个 Python 脚本文件的例子。
1.4.1 保存为文件
打开文本编辑器,新建一个 hungry.py 的文件。hungry.py 只包含下面一
行语句。


第 1 章 Python 入门
10
print("I'm hungry!")
接着,打开终端(Windows 中的命令行窗口),移至 hungry.py 所在的位置。
然 后,将 hungry.py 文 件 名 作 为 参 数,运 行 python 命 令。这 里 假 设 hungry.
py 在 ~/deep-learning-from-scratch/ch01 目 录 下(在 本 书 提 供 的 源 代 码 中,
hungry.py 文件位于 ch01 目录下)。
$ cd ~/deep-learning-from-scratch/ch01 # 移动目录 $ python hungry.py I'm hungry!
这样,使用 python hungry.py 命令就可以执行这个 Python 程序了。
1.4.2 类
前面我们了解了 int 和 str 等数据类型(通过 type() 函数可以查看对象的
类型)。这些数据类型是“内置”的数据类型,是 Python 中一开始就有的数
据类型。现在,我们来定义新的类。如果用户自己定义类的话,就可以自己
创建数据类型。此外,也可以定义原创的方法(类的函数)和属性。
Python 中使用 class 关键字来定义类,类要遵循下述格式(模板)。
class 类名: def __init__(self, 参数 , ...): # 构造函数 ... def 方法名 1(self, 参数 , ...): # 方法 1 ... def 方法名 2(self, 参数 , ...): # 方法 2 ...
这里有一个特殊的 __init__ 方法,这是进行初始化的方法,也称为构造
函数(constructor), 只在生成类的实例时被调用一次。此外,在方法的第一
个参数中明确地写入表示自身(自身的实例)的 self 是 Python 的一个特点(学
过其他编程语言的人可能会觉得这种写 self 的方式有一点奇怪)。
下面我们通过一个简单的例子来创建一个类。这里将下面的程序保存为
man.py。


1.5 NumPy 11
class Man: def __init__(self, name): self.name = name print("Initialized!")
def hello(self): print("Hello " + self.name + "!")
def goodbye(self): print("Good-bye " + self.name + "!")
m = Man("David") m.hello() m.goodbye()
从终端运行 man.py。
$ python man.py Initialized! Hello David! Good-bye David!
这里我们定义了一个新类 Man。上面的例子中,类 Man 生成了实例(对象)m。
类 Man 的构造函数(初始化方法)会接收参数 name,然后用这个参数初始
化实例变量 self.name。实例变量是存储在各个实例中的变量。Python 中可
以像 self.name 这样,通过在 self 后面添加属性名来生成或访问实例变量。
1.5 NumPy
在深度学习的实现中,经常出现数组和矩阵的计算。NumPy 的数组类
(numpy.array)中提供了很多便捷的方法,在实现深度学习时,我们将使用这
些方法。本节我们来简单介绍一下后面会用到的 NumPy。
1.5.1 导入 NumPy
NumPy 是外部库。这里所说的“外部”是指不包含在标准版 Python 中。
因此,我们首先要导入 NumPy 库。


第 1 章 Python 入门
12
>>> import numpy as np
Python 中 使 用 import 语 句 来 导 入 库。这 里 的 import numpy as np,直
译的话就是“将 numpy 作为 np 导入”的意思。通过写成这样的形式,之后
NumPy 相关的方法均可通过 np 来调用。
1.5.2 生成 NumPy 数组
要生成 NumPy 数组,需要使用 np.array() 方法。np.array() 接收 Python
列表作为参数,生成 NumPy 数组(numpy.ndarray)。
>>> x = np.array([1.0, 2.0, 3.0]) >>> print(x) [ 1. 2. 3.] >>> type(x)
<class 'numpy.ndarray'>
1.5.3 NumPy 的算术运算
下面是 NumPy 数组的算术运算的例子。
>>> x = np.array([1.0, 2.0, 3.0]) >>> y = np.array([2.0, 4.0, 6.0]) >>> x + y # 对应元素的加法 array([ 3., 6., 9.]) >>> x - y array([ -1., -2., -3.]) >>> x * y # element-wise product array([ 2., 8., 18.]) >>> x / y array([ 0.5, 0.5, 0.5])
这里需要注意的是,数组 x 和数组 y 的元素个数是相同的(两者均是元素
个数为 3 的一维数组)。当 x 和 y 的元素个数相同时,可以对各个元素进行算
术运算。如果元素个数不同,程序就会报错,所以元素个数保持一致非常重要。
另外,“对应元素的”的英文是 element-wise,比如“对应元素的乘法”就是
element-wise product。
NumPy 数组不仅可以进行 element-wise 运算,也可以和单一的数值(标量)


1.5 NumPy 13
组合起来进行运算。此时,需要在 NumPy 数组的各个元素和标量之间进行运算。
这个功能也被称为广播(详见后文)。
>>> x = np.array([1.0, 2.0, 3.0]) >>> x / 2.0
array([ 0.5, 1. , 1.5])
1.5.4 NumPy 的 N 维数组
NumPy 不仅可以生成一维数组(排成一列的数组),也可以生成多维数组。
比如,可以生成如下的二维数组(矩阵)。
>>> A = np.array([[1, 2], [3, 4]]) >>> print(A) [[1 2] [3 4]]
>>> A.shape (2, 2)
>>> A.dtype
dtype('int64')
这里生成了一个 2 × 2 的矩阵 A。另外,矩阵 A 的形状可以通过 shape 查看,
矩阵元素的数据类型可以通过 dtype 查看。下面,我们来看一下矩阵的算术运算。
>>> B = np.array([[3, 0],[0, 6]]) >>> A + B array([[ 4, 2], [ 3, 10]]) >>> A * B array([[ 3, 0], [ 0, 24]])
和数组的算术运算一样,矩阵的算术运算也可以在相同形状的矩阵间以
对应元素的方式进行。并且,也可以通过标量(单一数值)对矩阵进行算术运算。
这也是基于广播的功能。
>>> print(A) [[1 2] [3 4]]


第 1 章 Python 入门
14
>>> A * 10 array([[ 10, 20], [ 30, 40]])
NumPy 数组(np.array)可以生成 N 维数组,即可以生成一维数组、
二维数组、三维数组等任意维数的数组。数学上将一维数组称为向量,
将二维数组称为矩阵。另外,可以将一般化之后的向量或矩阵等统
称为张量(tensor)。本书基本上将二维数组称为“矩阵”,将三维数
组及三维以上的数组称为“张量”或“多维数组”。
1.5.5 广播
NumPy 中,形状不同的数组之间也可以进行运算。之前的例子中,在
2×2 的矩阵 A 和标量 10 之间进行了乘法运算。在这个过程中,如图 1-1 所示,
标量 10 被扩展成了 2 × 2 的形状,然后再与矩阵 A 进行乘法运算。这个巧妙
的功能称为广播(broadcast)。
图 1-1 广播的例子:标量 10 被当作 2 × 2 的矩阵
1* =
2
34
10 10
10 10
1* =
2
34
10 10 20
30 40
我们通过下面这个运算再来看一个广播的例子。
>>> A = np.array([[1, 2], [3, 4]]) >>> B = np.array([10, 20]) >>> A * B array([[ 10, 40], [ 30, 80]])
在这个运算中,如图 1-2 所示,一维数组 B 被“巧妙地”变成了和二位数
组 A 相同的形状,然后再以对应元素的方式进行运算。
综上,因为 NumPy 有广播功能,所以不同形状的数组之间也可以顺利
地进行运算。


1.5 NumPy 15
图 1-2 广播的例子 2
1* =
2
34
10 20
10 20
1* =
2
34
10 20 10 40
30 80
1.5.6 访问元素
元素的索引从 0 开始。对各个元素的访问可按如下方式进行。
>>> X = np.array([[51, 55], [14, 19], [0, 4]]) >>> print(X) [[51 55] [14 19] [ 0 4]] >>> X[0] # 第 0 行 array([51, 55]) >>> X[0][1] # (0,1) 的元素 55
也可以使用 for 语句访问各个元素。
>>> for row in X: ... print(row) ... [51 55] [14 19] [0 4]
除了前面介绍的索引操作,NumPy 还可以使用数组访问各个元素。
>>> X = X.flatten() # 将 X 转换为一维数组 >>> print(X)
[51 55 14 19 0 4]
>>> X[np.array([0, 2, 4])] # 获取索引为 0、2、4 的元素 array([51, 14, 0])
运用这个标记法,可以获取满足一定条件的元素。例如,要从 X 中抽出
大于 15 的元素,可以写成如下形式。


第 1 章 Python 入门
16
>>> X > 15 array([ True, True, False, True, False, False], dtype=bool) >>> X[X>15]
array([51, 55, 19])
对 NumPy 数组使用不等号运算符等(上例中是 X > 15), 结果会得到一个
布尔型的数组。上例中就是使用这个布尔型数组取出了数组的各个元素(取
出 True 对应的元素)。
Python 等动态类型语言一般比 C 和 C++ 等静态类型语言(编译型语言)
运算速度慢。实际上,如果是运算量大的处理对象,用 C/C++ 写程
序更好。为此,当 Python 中追求性能时,人们会用 C/C++ 来实现
处理的内容。Python 则承担“中间人”的角色,负责调用那些用 C/
C++ 写的程序。NumPy 中,主要的处理也都是通过 C 或 C++ 实现的。
因此,我们可以在不损失性能的情况下,使用 Python 便利的语法。
1.6 Matplotlib
在深度学习的实验中,图形的绘制和数据的可视化非常重要。Matplotlib
是用于绘制图形的库,使用 Matplotlib 可以轻松地绘制图形和实现数据的可
视化。这里,我们来介绍一下图形的绘制方法和图像的显示方法。
1.6.1 绘制简单图形
可以使用 matplotlib 的 pyplot 模块绘制图形。话不多说,我们来看一个
绘制 sin 函数曲线的例子。
import numpy as np import matplotlib.pyplot as plt
# 生成数据
x = np.arange(0, 6, 0.1) # 以 0.1 为单位,生成 0 到 6 的数据 y = np.sin(x)
# 绘制图形


1.6 Matplotlib 17
plt.plot(x, y) plt.show()
这里使用 NumPy 的 arange 方法生成了 [0, 0.1, 0.2, ���, 5.8, 5.9]的
数据,将其设为 x。对 x 的各个元素,应用 NumPy 的 sin 函数 np.sin(),将 x、
y 的数据传给 plt.plot 方法,然后绘制图形。最后,通过 plt.show() 显示图形。
运行上述代码后,就会显示图 1-3 所示的图形。
图 1-3 sin 函数的图形
1.0
0.5
0 1 2 34 5 6
0.0
−0.5
−1.0
1.6.2 pyplot 的功能
在刚才的 sin 函数的图形中,我们尝试追加 cos 函数的图形,并尝试使用
pyplot 的添加标题和 x 轴标签名等其他功能。
import numpy as np import matplotlib.pyplot as plt
# 生成数据
x = np.arange(0, 6, 0.1) # 以 0.1 为单位,生成 0 到 6 的数据 y1 = np.sin(x)


第 1 章 Python 入门
18
y2 = np.cos(x)
# 绘制图形
plt.plot(x, y1, label="sin") plt.plot(x, y2, linestyle = "--", label="cos") # 用虚线绘制 plt.xlabel("x") # x 轴标签 plt.ylabel("y") # y 轴标签 plt.title('sin & cos') # 标题 plt.legend() plt.show()
结果如图 1-4 所示,我们看到图的标题、轴的标签名都被标出来了。
图 1-4 sin 函数和 cos 函数的图形
1.0 sin & cos
0.5
0.0
−0.5
−1.0 0 1 3
x
y
2 456
cos
sin
1.6.3 显示图像
pyplot 中 还 提 供 了 用 于 显 示 图 像 的 方 法 imshow()。另 外,可 以 使 用
matplotlib.image 模块的 imread() 方法读入图像。下面我们来看一个例子。
import matplotlib.pyplot as plt from matplotlib.image import imread


1.7 小结 19
img = imread('lena.png') # 读入图像(设定合适的路径!) plt.imshow(img)
plt.show()
运行上述代码后,会显示图 1-5 所示的图像。
图 1-5 显示图像
0
50
100
150
200
250 0 50 100 150 200 250
这里,我们假定图像 lena.png 在当前目录下。读者根据自己的环境,可
能需要变更文件名或文件路径。另外,本书提供的源代码中,在 dataset 目
录下有样本图像 lena.png。比如,在通过 Python 解释器从 ch01 目录运行上
述代码的情况下,将图像的路径 'lena.png' 改为 '../dataset/lena.png',即
可正确运行。
1.7 小结
本章重点介绍了实现深度学习(神经网络)所需的编程知识,以为学习
深度学习做好准备。从下一章开始,我们将通过使用 Python 实际运行代码,
逐步了解深度学习。


第 1 章 Python 入门
20
本章只介绍了关于 Python 的最低限度的知识,想进一步了解 Python 的
读者,可以参考下面这些图书。首先推荐《Python 语言及其应用》[1] 一书。
这是一本详细介绍从 Python 编程的基础到应用的实践性的入门书。关于
NumPy,《利用 Python 进行数据分析》[2] 一书中进行了简单易懂的总结。此
外,“Scipy Lecture Notes”[3] 这个网站上也有以科学计算为主题的 NumPy
和 Matplotlib 的详细介绍,有兴趣的读者可以参考。
下面,我们来总结一下本章所学的内容,如下所示。
本章所学的内容
• Python 是一种简单易记的编程语言。
• Python 是开源的,可以自由使用。
• 本书中将使用 Python 3.x 实现深度学习。
• 本书中将使用 NumPy 和 Matplotlib 这两种外部库。
• Python 有“解释器”和“脚本文件”两种运行模式。
• Python 能够将一系列处理集成为函数或类等模块。
• NumPy 中有很多用于操作多维数组的便捷方法。


第2章
感知机
本章将介绍感知机 A(perceptron)这一算法。感知机是由美国学者 Frank
Rosenblatt 在 1957 年提出来的。为何我们现在还要学习这一很久以前就有
的算法呢?因为感知机也是作为神经网络(深度学习)的起源的算法。因此,
学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。
本章我们将简单介绍一下感知机,并用感知机解决一些简单的问题。希
望读者通过这个过程能熟悉感知机。
2.1 感知机是什么
感知机接收多个输入信号,输出一个信号。这里所说的“信号”可以想
象成电流或河流那样具备“流动性”的东西。像电流流过导线,向前方输送
电子一样,感知机的信号也会形成流,向前方输送信息。但是,和实际的电
流不同的是,感知机的信号只有“流 / 不流”(1/0)两种取值。在本书中,0
对应“不传递信号”,1 对应“传递信号”。
图 2-1 是一个接收两个输入信号的感知机的例子。x1、x2 是输入信号,
y 是 输 出 信 号,w1、w2 是 权 重(w 是 weight 的 首 字 母)。图 中 的 ○ 称 为“神
经元”或者“节点”。输入信号被送往神经元时,会被分别乘以固定的权重
A 严格地讲,本章中所说的感知机应该称为“人工神经元”或“朴素感知机”,但是因为很多基本的处 理都是共通的,所以这里就简单地称为“感知机”。


第 2 章 感知机
22
(w1x1、w2x2)。神经元会计算传送过来的信号的总和,只有当这个总和超过
了某个界限值时,才会输出 1。这也称为“神经元被激活”。这里将这个界
限值称为阈值,用符号 θ 表示。
图 2-1 有两个输入的感知机
x1
x2
w1
w2
y
感知机的运行原理只有这些!把上述内容用数学式来表示,就是式(2.1)。
(2.1)
感知机的多个输入信号都有各自固有的权重,这些权重发挥着控制各个
信号的重要性的作用。也就是说,权重越大,对应该权重的信号的重要性就
越高。
权重相当于电流里所说的电阻。电阻是决定电流流动难度的参数,
电阻越低,通过的电流就越大。而感知机的权重则是值越大,通过
的信号就越大。不管是电阻还是权重,在控制信号流动难度(或者流
动容易度)这一点上的作用都是一样的。


2.2 简单逻辑电路 23
2.2 简单逻辑电路
2.2.1 与门
现在让我们考虑用感知机来解决简单的问题。这里首先以逻辑电路为题
材来思考一下与门(AND gate)。与门是有两个输入和一个输出的门电路。图 2-2
这种输入信号和输出信号的对应表称为“真值表”。如图 2-2 所示,与门仅在
两个输入均为 1 时输出 1,其他时候则输出 0。
x1 x2 y
1
1
11 1
0
0
00
0
00
图 2-2 与门的真值表
下面考虑用感知机来表示这个与门。需要做的就是确定能满足图 2-2 的
真值表的 w1、w2、θ 的值。那么,设定什么样的值才能制作出满足图 2-2 的
条件的感知机呢?
实际上,满足图 2-2 的条件的参数的选择方法有无数多个。比如,当
(w1, w2, θ) = (0.5, 0.5, 0.7) 时,可 以 满 足 图 2-2 的 条 件。此 外,当 (w1, w2, θ)
为 (0.5, 0.5, 0.8) 或者 (1.0, 1.0, 1.0) 时,同样也满足与门的条件。设定这样的
参数后,仅当 x1 和 x2 同时为 1 时,信号的加权总和才会超过给定的阈值 θ。
2.2.2 与非门和或门
接着,我们再来考虑一下与非门(NAND gate)。NAND 是 Not AND 的


第 2 章 感知机
24
意思,与非门就是颠倒了与门的输出。用真值表表示的话,如图 2-3 所示,
仅当 x1 和 x2 同时为 1 时输出 0,其他时候则输出 1。那么与非门的参数又可
以是什么样的组合呢?
图 2-3 与非门的真值表
1
11
11
1
001
0
0
0
x1 x2 y
要表示与非门,可以用 (w1, w2, θ) = (−0.5, −0.5, −0.7) 这样的组合(其
他的组合也是无限存在的)。实际上,只要把实现与门的参数值的符号取反,
就可以实现与非门。
接下来看一下图 2-4 所示的或门。或门是“只要有一个输入信号是 1,输
出就为 1”的逻辑电路。那么我们来思考一下,应该为这个或门设定什么样
的参数呢?
图 2-4 或门的真值表
x1 x2 y
11
11
111
0
0
0
00


2.3 感知机的实现 25
这里决定感知机参数的并不是计算机,而是我们人。我们看着真值
表这种“训练数据”,人工考虑(想到)了参数的值。而机器学习的课
题就是将这个决定参数值的工作交由计算机自动进行。学习是确定
合适的参数的过程,而人要做的是思考感知机的构造(模型),并把
训练数据交给计算机。
如上所示,我们已经知道使用感知机可以表示与门、与非门、或门的逻
辑电路。这里重要的一点是:与门、与非门、或门的感知机构造是一样的。
实际上,3 个门电路只有参数的值(权重和阈值)不同。也就是说,相同构造
的感知机,只需通过适当地调整参数的值,就可以像“变色龙演员”表演不
同的角色一样,变身为与门、与非门、或门。
2.3 感知机的实现
2.3.1 简单的实现
现在,我们用 Python 来实现刚才的逻辑电路。这里,先定义一个接收
参数 x1 和 x2 的 AND 函数。
def AND(x1, x2): w1, w2, theta = 0.5, 0.5, 0.7 tmp = x1*w1 + x2*w2 if tmp <= theta: return 0 elif tmp > theta: return 1
在函数内初始化参数 w1、w2、theta,当输入的加权总和超过阈值时返回 1,
否则返回 0。我们来确认一下输出结果是否如图 2-2 所示。
AND(0, 0) # 输出 0 AND(1, 0) # 输出 0 AND(0, 1) # 输出 0 AND(1, 1) # 输出 1
果然和我们预想的输出一样!这样我们就实现了与门。按照同样的步骤,


第 2 章 感知机
26
也可以实现与非门和或门,不过让我们来对它们的实现稍作修改。
2.3.2 导入权重和偏置
刚才的与门的实现比较直接、容易理解,但是考虑到以后的事情,我们
将其修改为另外一种实现形式。在此之前,首先把式(2.1)的 θ 换成 −b,于
是就可以用式(2.2)来表示感知机的行为。
(2.2)
式(2.1)和式(2.2)虽然有一个符号不同,但表达的内容是完全相同的。
此处,b 称为偏置,w1 和 w2 称为权重。如式(2.2)所示,感知机会计算输入
信号和权重的乘积,然后加上偏置,如果这个值大于 0 则输出 1,否则输出 0。
下面,我们使用 NumPy,按式(2.2)的方式实现感知机。在这个过程中,我
们用 Python 的解释器逐一确认结果。
>>> import numpy as np >>> x = np.array([0, 1]) # 输入 >>> w = np.array([0.5, 0.5]) # 权重 >>> b = -0.7 # 偏置 >>> w*x array([ 0. , 0.5]) >>> np.sum(w*x) 0.5
>>> np.sum(w*x) + b
-0.19999999999999996 # 大约为 -0.2(由浮点小数造成的运算误差)
如上例所示,在 NumPy 数组的乘法运算中,当两个数组的元素个数相同时,
各个元素分别相乘,因此 w*x 的结果就是它们的各个元素分别相乘([0, 1] *
[0.5, 0.5] => [0, 0.5])。之后,np.sum(w*x) 再计算相乘后的各个元素的总和。
最后再把偏置加到这个加权总和上,就完成了式(2.2)的计算。
2.3.3 使用权重和偏置的实现
使用权重和偏置,可以像下面这样实现与门。


2.3 感知机的实现 27
def AND(x1, x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.7 tmp = np.sum(w*x) + b if tmp <= 0: return 0 else: return 1
这里把 − θ 命名为偏置 b,但是请注意,偏置和权重 w1、w2 的作用是不
一样的。具体地说,w1 和 w2 是控制输入信号的重要性的参数,而偏置是调
整神经元被激活的容易程度(输出信号为 1 的程度)的参数。比如,若 b 为
−0.1,则只要输入信号的加权总和超过 0.1,神经元就会被激活。但是如果 b
为 −20.0,则输入信号的加权总和必须超过 20.0,神经元才会被激活。像这样,
偏置的值决定了神经元被激活的容易程度。另外,这里我们将 w1 和 w2 称为权重,
将 b 称为偏置,但是根据上下文,有时也会将 b、w1、w2 这些参数统称为权重。
偏置这个术语,有“穿木屐”A 的效果,即在没有任何输入时(输入为
0 时),给输出穿上多高的木屐(加上多大的值)的意思。实际上,在
式 (2.2) 的 b + w1x1 + w2x2 的计算中,当输入 x1 和 x2 为 0 时,只输出
偏置的值。
A
接着,我们继续实现与非门和或门。
def NAND(x1, x2): x = np.array([x1, x2])
w = np.array([-0.5, -0.5]) # 仅权重和偏置与 AND 不同! b = 0.7
tmp = np.sum(w*x) + b if tmp <= 0: return 0 else: return 1
def OR(x1, x2):
A 因为木屐的底比较厚,穿上它后,整个人也会显得更高。——译者注


第 2 章 感知机
28
x = np.array([x1, x2])
w = np.array([0.5, 0.5]) # 仅权重和偏置与 AND 不同! b = -0.2
tmp = np.sum(w*x) + b if tmp <= 0: return 0 else: return 1
我们在 2.2 节介绍过,与门、与非门、或门是具有相同构造的感知机,
区别只在于权重参数的值。因此,在与非门和或门的实现中,仅设置权重和
偏置的值这一点和与门的实现不同。
2.4 感知机的局限性
到这里我们已经知道,使用感知机可以实现与门、与非门、或门三种逻
辑电路。现在我们来考虑一下异或门(XOR gate)。
2.4.1 异或门
异或门也被称为逻辑异或电路。如图 2-5 所示,仅当 x1 或 x2 中的一方为
1 时,才会输出 1(“异或”是拒绝其他的意思)。那么,要用感知机实现这个
异或门的话,应该设定什么样的权重参数呢?
x1 x2 y
11
11
11
0
0
0
0
00
图 2-5 异或门的真值表


2.4 感知机的局限性 29
实际上,用前面介绍的感知机是无法实现这个异或门的。为什么用感知
机可以实现与门、或门,却无法实现异或门呢?下面我们尝试通过画图来思
考其中的原因。
首 先,我 们 试 着 将 或 门 的 动 作 形 象 化。或 门 的 情 况 下,当 权 重 参 数
(b, w1, w2) = (−0.5, 1.0, 1.0) 时,可满足图 2-4 的真值表条件。此时,感知机
可用下面的式(2.3)表示。
(2.3)
式(2.3)表示的感知机会生成由直线 −0.5 + x1 + x2 = 0 分割开的两个空
间。其中一个空间输出 1,另一个空间输出 0,如图 2-6 所示。
x2
x1 01
1
图 2-6 感知机的可视化:灰色区域是感知机输出 0 的区域,这个区域与或门的性质一致
或门在 (x1, x2) = (0, 0) 时输出 0,在 (x1, x2) 为 (0, 1)、(1, 0)、(1, 1) 时输
出 1。图 2-6 中,○表示 0,△表示 1。如果想制作或门,需要用直线将图 2-6


第 2 章 感知机
30
中的○和△分开。实际上,刚才的那条直线就将这 4 个点正确地分开了。
那么,换成异或门的话会如何呢?能否像或门那样,用一条直线作出分
割图 2-7 中的○和△的空间呢?
x2
x1 01
1
图 2-7 ○和△表示异或门的输出。可否通过一条直线作出分割○和△的空间呢?
想要用一条直线将图 2-7 中的○和△分开,无论如何都做不到。事实上,
用一条直线是无法将○和△分开的。
2.4.2 线性和非线性
图 2-7 中的○和△无法用一条直线分开,但是如果将“直线”这个限制条
件去掉,就可以实现了。比如,我们可以像图 2-8 那样,作出分开○和△的空间。
感知机的局限性就在于它只能表示由一条直线分割的空间。图2-8 这样弯
曲的曲线无法用感知机表示。另外,由图 2-8 这样的曲线分割而成的空间称为
非线性空间,由直线分割而成的空间称为线性空间。线性、非线性这两个术
语在机器学习领域很常见,可以将其想象成图 2-6和图2-8所示的直线和曲线。


2.5 多层感知机 31
x2
x1
0
1
1
图 2-8 使用曲线可以分开○和△
2.5 多层感知机
感知机不能表示异或门让人深感遗憾,但也无需悲观。实际上,感知机
的绝妙之处在于它可以“叠加层”(通过叠加层来表示异或门是本节的要点)。
这里,我们暂且不考虑叠加层具体是指什么,先从其他视角来思考一下异或
门的问题。
2.5.1 已有门电路的组合
异或门的制作方法有很多,其中之一就是组合我们前面做好的与门、与
非门、或门进行配置。这里,与门、与非门、或门用图 2-9 中的符号表示。另外,
图 2-9 中与非门前端的○表示反转输出的意思。
那么,请思考一下,要实现异或门的话,需要如何配置与门、与非门和
或门呢?这里给大家一个提示,用与门、与非门、或门代替图 2-10 中的各个
“?”,就可以实现异或门。


第 2 章 感知机
32
AND NAND OR
图 2-9 与门、与非门、或门的符号
图 2-10 将与门、与非门、或门代入到“?”中,就可以实现异或门!
x2
x1
y
2.4 节讲到的感知机的局限性,严格地讲,应该是“单层感知机无法
表示异或门”或者“单层感知机无法分离非线性空间”。接下来,我
们将看到通过组合感知机(叠加层)就可以实现异或门。
异或门可以通过图 2-11 所示的配置来实现。这里,x1 和 x2 表示输入信号,
y 表示输出信号。x1 和 x2 是与非门和或门的输入,而与非门和或门的输出则
是与门的输入。
图 2-11 通过组合与门、与非门、或门实现异或门
x1 s1
x2
s2
y
现在,我们来确认一下图 2-11 的配置是否真正实现了异或门。这里,把
s1 作为与非门的输出,把 s2 作为或门的输出,填入真值表中。结果如图 2-12
所示,观察 x1、x2、y,可以发现确实符合异或门的输出。


2.5 多层感知机 33
x1
0
1
0
1
0
0
1
1
1
1
1
0
0
1
1
1
0
1
1
0
x2 s1 s2 y
图 2-12 异或门的真值表
2.5.2 异或门的实现
下面我们试着用 Python 来实现图 2-11 所示的异或门。使用之前定义的
AND 函数、NAND 函数、OR 函数,可以像下面这样(轻松地)实现。
def XOR(x1, x2): s1 = NAND(x1, x2) s2 = OR(x1, x2) y = AND(s1, s2) return y
这个 XOR 函数会输出预期的结果。
XOR(0, 0) # 输出 0 XOR(1, 0) # 输出 1 XOR(0, 1) # 输出 1 XOR(1, 1) # 输出 0
这样,异或门的实现就完成了。下面我们试着用感知机的表示方法(明
确地显示神经元)来表示这个异或门,结果如图 2-13 所示。
如图 2-13 所示,异或门是一种多层结构的神经网络。这里,将最左边的
一列称为第 0 层,中间的一列称为第 1 层,最右边的一列称为第 2 层。
图 2-13 所示的感知机与前面介绍的与门、或门的感知机(图 2-1)形状不
同。实际上,与门、或门是单层感知机,而异或门是 2 层感知机。叠加了多
层的感知机也称为多层感知机(multi-layered perceptron)。


第 2 章 感知机
34
图 2-13 用感知机表示异或门
x1 s1
x2 s2
y
第0层 第1层 第2层
图 2-13 中的感知机总共由 3 层构成,但是因为拥有权重的层实质
上只有 2 层(第 0 层和第 1 层之间,第 1 层和第 2 层之间),所以称
为“2 层感知机”。不过,有的文献认为图 2-13 的感知机是由 3 层
构成的,因而将其称为“3 层感知机”。
在图 2-13 所示的 2 层感知机中,先在第 0 层和第 1 层的神经元之间进行
信号的传送和接收,然后在第 1 层和第 2 层之间进行信号的传送和接收,具
体如下所示。
1. 第 0 层的两个神经元接收输入信号,并将信号发送至第 1 层的神经元。
2. 第 1 层的神经元将信号发送至第 2 层的神经元,第 2 层的神经元输出 y。
这种 2 层感知机的运行过程可以比作流水线的组装作业。第 1 段(第 1 层)
的工人对传送过来的零件进行加工,完成后再传送给第 2 段(第 2 层)的工人。
第 2 层的工人对第 1 层的工人传过来的零件进行加工,完成这个零件后出货
(输出)。
像这样,在异或门的感知机中,工人之间不断进行零件的传送。通过这
样的结构(2 层结构),感知机得以实现异或门。这可以解释为“单层感知机
无法表示的东西,通过增加一层就可以解决”。也就是说,通过叠加层(加深
层),感知机能进行更加灵活的表示。


2.6 从与非门到计算机 35
2.6 从与非门到计算机
多层感知机可以实现比之前见到的电路更复杂的电路。比如,进行加法
运算的加法器也可以用感知机实现。此外,将二进制转换为十进制的编码器、
满足某些条件就输出 1 的电路(用于等价检验的电路)等也可以用感知机表示。
实际上,使用感知机甚至可以表示计算机!
计算机是处理信息的机器。向计算机中输入一些信息后,它会按照某种
既定的方法进行处理,然后输出结果。所谓“按照某种既定的方法进行处理”
是指,计算机和感知机一样,也有输入和输出,会按照某个既定的规则进行
计算。
人们一般会认为计算机内部进行的处理非常复杂,而令人惊讶的是,实
际上只需要通过与非门的组合,就能再现计算机进行的处理。这一令人吃惊
的事实说明了什么呢?说明使用感知机也可以表示计算机。前面也介绍了,
与非门可以使用感知机实现。也就是说,如果通过组合与非门可以实现计算
机的话,那么通过组合感知机也可以表示计算机(感知机的组合可以通过叠
加了多层的单层感知机来表示)。
说到仅通过与非门的组合就能实现计算机,大家也许一下子很难相信。
建议有兴趣的读者看一下《计算机系统要素:从零开始构建现代计
算机》。这本书以深入理解计算机为主题,论述了通过 NAND 构建可
运行俄罗斯方块的计算机的过程。此书能让读者真实体会到,通过
简单的 NAND 元件就可以实现计算机这样复杂的系统。
综上,多层感知机能够进行复杂的表示,甚至可以构建计算机。那么,
什么构造的感知机才能表示计算机呢?层级多深才可以构建计算机呢?
理论上可以说 2 层感知机就能构建计算机。这是因为,已有研究证明,
2 层感知机(严格地说是激活函数使用了非线性的 sigmoid 函数的感知机,具
体请参照下一章)可以表示任意函数。但是,使用 2 层感知机的构造,通过


第 2 章 感知机
36
设定合适的权重来构建计算机是一件非常累人的事情。实际上,在用与非门
等低层的元件构建计算机的情况下,分阶段地制作所需的零件(模块)会比
较自然,即先实现与门和或门,然后实现半加器和全加器,接着实现算数逻
辑单元(ALU),然后实现 CPU。因此,通过感知机表示计算机时,使用叠
加了多层的构造来实现是比较自然的流程。
本书中不会实际来实现计算机,但是希望读者能够记住,感知机通过叠
加层能够进行非线性的表示,理论上还可以表示计算机进行的处理。
2.7 小结
本章我们学习了感知机。感知机是一种非常简单的算法,大家应该很快
就能理解它的构造。感知机是下一章要学习的神经网络的基础,因此本章的
内容非常重要。
本章所学的内容
• 感知机是具有输入和输出的算法。给定一个输入后,将输出一个既
定的值。
• 感知机将权重和偏置设定为参数。
• 使用感知机可以表示与门和或门等逻辑电路。
• 异或门无法通过单层感知机来表示。
• 使用 2 层感知机可以表示异或门。
• 单层感知机只能表示线性空间,而多层感知机可以表示非线性空间。
• 多层感知机(在理论上)可以表示计算机。


第3章
神经网络
上一章我们学习了感知机。关于感知机,既有好消息,也有坏消息。好
消息是,即便对于复杂的函数,感知机也隐含着能够表示它的可能性。上一
章已经介绍过,即便是计算机进行的复杂处理,感知机(理论上)也可以将
其表示出来。坏消息是,设定权重的工作,即确定合适的、能符合预期的输
入与输出的权重,现在还是由人工进行的。上一章中,我们结合与门、或门
的真值表人工决定了合适的权重。
神经网络的出现就是为了解决刚才的坏消息。具体地讲,神经网络的一
个重要性质是它可以自动地从数据中学习到合适的权重参数。本章中,我们
会先介绍神经网络的概要,然后重点关注神经网络进行识别时的处理。在下
一章中,我们将了解如何从数据中学习权重参数。
3.1 从感知机到神经网络
神经网络和上一章介绍的感知机有很多共同点。这里,我们主要以两者
的差异为中心,来介绍神经网络的结构。
3.1.1 神经网络的例子
用图来表示神经网络的话,如图 3-1 所示。我们把最左边的一列称为
输入层,最右边的一列称为输出层,中间的一列称为中间层。中间层有时


第 3 章 神经网络
38
也称为隐藏层。“隐藏”一词的意思是,隐藏层的神经元(和输入层、输出
层不同)肉眼看不见。另外,本书中把输入层到输出层依次称为第 0 层、第
1 层、第 2 层(层号之所以从 0 开始,是为了方便后面基于 Python 进行实现)。
图 3-1 中,第 0 层对应输入层,第 1 层对应中间层,第 2 层对应输出层。
输入层 输出层
中间层
图 3-1 神经网络的例子
图 3-1 中的网络一共由 3 层神经元构成,但实质上只有 2 层神经
元有权重,因此将其称为“2 层网络”。请注意,有的书也会根据
构成网络的层数,把图 3-1 的网络称为“3 层网络”。本书将根据
实质上拥有权重的层数(输入层、隐藏层、输出层的总数减去 1
后的数量)来表示网络的名称。
只看图 3-1 的话,神经网络的形状类似上一章的感知机。实际上,就神
经元的连接方式而言,与上一章的感知机并没有任何差异。那么,神经网络
中信号是如何传递的呢?
3.1.2 复习感知机
在观察神经网络中信号的传递方法之前,我们先复习一下感知机。现在


3.1 从感知机到神经网络 39
来思考一下图 3-2 中的网络结构。
x1 w1
x2
w2
y
图 3-2 复习感知机
图 3-2 中的感知机接收 x1 和 x2 两个输入信号,输出 y。如果用数学式来
表示图 3-2 中的感知机,则如式(3.1)所示。
(3.1)
b 是被称为偏置的参数,用于控制神经元被激活的容易程度;而 w1 和 w2
是表示各个信号的权重的参数,用于控制各个信号的重要性。
顺便提一下,在图 3-2 的网络中,偏置 b 并没有被画出来。如果要明确
地表示出 b,可以像图 3-3 那样做。图 3-3 中添加了权重为 b 的输入信号 1。这
个感知机将 x1、x2、1 三个信号作为神经元的输入,将其和各自的权重相乘后,
传送至下一个神经元。在下一个神经元中,计算这些加权信号的总和。如果
这个总和超过 0,则输出 1,否则输出 0。另外,由于偏置的输入信号一直是 1,
所以为了区别于其他神经元,我们在图中把这个神经元整个涂成灰色。
现在将式(3.1)改写成更加简洁的形式。为了简化式(3.1),我们用一个
函数来表示这种分情况的动作(超过 0 则输出 1,否则输出 0)。引入新函数
h(x),将式(3.1)改写成下面的式(3.2)和式(3.3)。
y = h(b + w1x1 + w2x2) (3.2)


第 3 章 神经网络
40
x1 w1
x2
w2
y
1
b
图 3-3 明确表示出偏置
(3.3)
式(3.2)中,输入信号的总和会被函数 h(x) 转换,转换后的值就是输出 y。
然后,式(3.3)所表示的函数 h(x),在输入超过 0 时返回 1,否则返回 0。因此,
式(3.1)和式(3.2)、式(3.3)做的是相同的事情。
3.1.3 激活函数登场
刚才登场的 h(x)函数会将输入信号的总和转换为输出信号,这种函数
一般称为激活函数(activation function)。如“激活”一词所示,激活函数的
作用在于决定如何来激活输入信号的总和。
现在来进一步改写式(3.2)。式(3.2)分两个阶段进行处理,先计算输入
信号的加权总和,然后用激活函数转换这一总和。因此,如果将式(3.2)写
得详细一点,则可以分成下面两个式子。
a = b + w1x1 + w2x2 (3.4)
y = h(a) (3.5)
首先,式(3.4)计算加权输入信号和偏置的总和,记为 a。然后,式(3.5)
用 h() 函数将 a 转换为输出 y。


3.1 从感知机到神经网络 41
之前的神经元都是用一个○表示的,如果要在图中明确表示出式(3.4)
和式(3.5),则可以像图 3-4 这样做。
x1 w1
x2
w2
y
1
b
a
图 3-4 明确显示激活函数的计算过程
h()
如图 3-4 所示,表示神经元的○中明确显示了激活函数的计算过程,即
信号的加权总和为节点 a,然后节点 a 被激活函数 h() 转换成节点 y。本书中,“神
经元”和“节点”两个术语的含义相同。这里,我们称 a 和 y 为“节点”,其实
它和之前所说的“神经元”含义相同。
通常如图 3-5 的左图所示,神经元用一个○表示。本书中,在可以明确
神经网络的动作的情况下,将在图中明确显示激活函数的计算过程,如图 3-5
的右图所示。
图 3-5 左图是一般的神经元的图,右图是在神经元内部明确显示激活函数的计算过程的图(a 表示输入信号的总和,h() 表示激活函数,y 表示输出)
y
h() a


第 3 章 神经网络
42
下面,我们将仔细介绍激活函数。激活函数是连接感知机和神经网络的
桥梁。A
本书在使用“感知机”一词时,没有严格统一它所指的算法。一
般而言,“朴素感知机”是指单层网络,指的是激活函数使用了阶
跃函数 A 的模型。“多层感知机”是指神经网络,即使用 sigmoid
函数(后述)等平滑的激活函数的多层网络。
3.2 激活函数
式(3.3)表示的激活函数以阈值为界,一旦输入超过阈值,就切换输出。
这样的函数称为“阶跃函数”。因此,可以说感知机中使用了阶跃函数作为
激活函数。也就是说,在激活函数的众多候选函数中,感知机使用了阶跃函数。
那么,如果感知机使用其他函数作为激活函数的话会怎么样呢?实际上,如
果将激活函数从阶跃函数换成其他函数,就可以进入神经网络的世界了。下
面我们就来介绍一下神经网络使用的激活函数。
3.2.1 sigmoid 函数
神经网络中经常使用的一个激活函数就是式(3.6)表示的 sigmoid 函数
(sigmoid function)。
(3.6)
式(3.6)中的 exp(−x) 表示 e−x 的意思。e 是纳皮尔常数 2.7182 . . .。式(3.6)
表示的 sigmoid 函数看上去有些复杂,但它也仅仅是个函数而已。而函数就是
给定某个输入后,会返回某个输出的转换器。比如,向 sigmoid 函数输入 1.0 或 2.0
后,就会有某个值被输出,类似 h(1.0) = 0.731 . . .、h(2.0) = 0.880 . . . 这样。
神经网络中用 sigmoid 函数作为激活函数,进行信号的转换,转换后的
A 阶跃函数是指一旦输入超过阈值,就切换输出的函数。


3.2 激活函数 43
信号被传送给下一个神经元。实际上,上一章介绍的感知机和接下来要介绍
的神经网络的主要区别就在于这个激活函数。其他方面,比如神经元的多层
连接的构造、信号的传递方法等,基本上和感知机是一样的。下面,让我们
通过和阶跃函数的比较来详细学习作为激活函数的 sigmoid 函数。
3.2.2 阶跃函数的实现
这里我们试着用 Python 画出阶跃函数的图(从视觉上确认函数的形状对
理解函数而言很重要)。阶跃函数如式(3.3)所示,当输入超过 0 时,输出 1,
否则输出 0。可以像下面这样简单地实现阶跃函数。
def step_function(x): if x > 0: return 1 else: return 0
这个实现简单、易于理解,但是参数 x 只能接受实数(浮点数)。也就是
说,允许形如 step_function(3.0) 的调用,但不允许参数取 NumPy 数组,例
如 step_function(np.array([1.0, 2.0]))。为了便于后面的操作,我们把它修
改为支持 NumPy 数组的实现。为此,可以考虑下述实现。
def step_function(x): y=x>0 return y.astype(np.int)
上述函数的内容只有两行。由于使用了 NumPy 中的“技巧”,可能会有
点难理解。下面我们通过 Python 解释器的例子来看一下这里用了什么技巧。
下面这个例子中准备了 NumPy 数组 x,并对这个 NumPy 数组进行了不等号
运算。
>>> import numpy as np >>> x = np.array([-1.0, 1.0, 2.0]) >>> x array([-1., 1., 2.]) >>> y = x > 0


第 3 章 神经网络
44
>>> y array([False, True, True], dtype=bool)
对 NumPy 数组进行不等号运算后,数组的各个元素都会进行不等号运算,
生成一个布尔型数组。这里,数组 x 中大于 0 的元素被转换为 True,小于等
于 0 的元素被转换为 False,从而生成一个新的数组 y。
数组 y 是一个布尔型数组,但是我们想要的阶跃函数是会输出 int 型的 0
或 1 的函数。因此,需要把数组 y 的元素类型从布尔型转换为 int 型。
>>> y = y.astype(np.int) >>> y array([0, 1, 1])
如上所示,可以用 astype() 方法转换 NumPy 数组的类型。astype() 方
法通过参数指定期望的类型,这个例子中是 np.int 型。Python 中将布尔型
转换为 int 型后,True 会转换为 1,False 会转换为 0。以上就是阶跃函数的
实现中所用到的 NumPy 的“技巧”。
3.2.3 阶跃函数的图形
下面我们就用图来表示上面定义的阶跃函数,为此需要使用 matplotlib 库。
import numpy as np import matplotlib.pylab as plt
def step_function(x): return np.array(x > 0, dtype=np.int)
x = np.arange(-5.0, 5.0, 0.1) y = step_function(x) plt.plot(x, y) plt.ylim(-0.1, 1.1) # 指定 y 轴的范围 plt.show()
np.arange(-5.0, 5.0, 0.1) 在 −5.0 到 5.0 的范围内,以 0.1 为单位,生成
NumPy 数组([-5.0, -4.9, ���, 4.9])。step_function() 以该 NumPy 数组为
参数,对数组的各个元素执行阶跃函数运算,并以数组形式返回运算结果。
对数组 x、y 进行绘图,结果如图 3-6 所示。


3.2 激活函数 45
图 3-6 阶跃函数的图形
1.0
0.8
0.6
0.4
0.2
0.0
−6 −4 −2 0 2 4 6
如图 3-6 所示,阶跃函数以 0 为界,输出从 0 切换为 1(或者从 1 切换为 0)。
它的值呈阶梯式变化,所以称为阶跃函数。
3.2.4 sigmoid 函数的实现
下面,我们来实现 sigmoid 函数。用 Python 可以像下面这样写出式(3.6)
表示的 sigmoid 函数。
def sigmoid(x): return 1 / (1 + np.exp(-x))
这里,np.exp(-x) 对应 exp(−x)。这个实现没有什么特别难的地方,但
是要注意参数 x 为 NumPy 数组时,结果也能被正确计算。实际上,如果在
这个 sigmoid 函数中输入一个 NumPy 数组,则结果如下所示。
>>> x = np.array([-1.0, 1.0, 2.0]) >>> sigmoid(x)
array([ 0.26894142, 0.73105858, 0.88079708])


第 3 章 神经网络
46
之所以 sigmoid 函数的实现能支持 NumPy 数组,秘密就在于 NumPy 的
广播功能(1.5.5 节)。根据 NumPy 的广播功能,如果在标量和 NumPy 数组
之间进行运算,则标量会和 NumPy 数组的各个元素进行运算。这里来看一
个具体的例子。
>>> t = np.array([1.0, 2.0, 3.0]) >>> 1.0 + t
array([ 2., 3., 4.]) >>> 1.0 / t
array([ 1. , 0.5 , 0.33333333])
在这个例子中,标量(例子中是 1.0)和 NumPy 数组之间进行了数值运
算(+、/ 等)。结果,标量和 NumPy 数组的各个元素进行了运算,运算结
果以 NumPy 数组的形式被输出。刚才的 sigmoid 函数的实现也是如此,因
为 np.exp(-x) 会生成 NumPy 数组,所以 1 / (1 + np.exp(-x)) 的运算将会在
NumPy 数组的各个元素间进行。
下面我们把 sigmoid 函数画在图上。画图的代码和刚才的阶跃函数的代
码几乎是一样的,唯一不同的地方是把输出 y 的函数换成了 sigmoid 函数。
x = np.arange(-5.0, 5.0, 0.1) y = sigmoid(x) plt.plot(x, y) plt.ylim(-0.1, 1.1) # 指定 y 轴的范围 plt.show()
运行上面的代码,可以得到图 3-7。
3.2.5 sigmoid 函数和阶跃函数的比较
现在我们来比较一下 sigmoid 函数和阶跃函数,如图 3-8 所示。两者的
不同点在哪里呢?又有哪些共同点呢?我们通过观察图 3-8 来思考一下。
观察图 3-8,首先注意到的是“平滑性”的不同。sigmoid 函数是一条平
滑的曲线,输出随着输入发生连续性的变化。而阶跃函数以 0 为界,输出发
生急剧性的变化。sigmoid 函数的平滑性对神经网络的学习具有重要意义。


3.2 激活函数 47
图 3-7 sigmoid 函数的图形
1.0
0.8
0.6
0.4
0.2
0.0
−6 −4 −2 0 2 4 6
图 3-8 阶跃函数与 sigmoid 函数(虚线是阶跃函数)
1.0
0.8
0.6
0.4
0.2
0.0
−6 −4 −2 0 2 4 6


第 3 章 神经网络
48
另一个不同点是,相对于阶跃函数只能返回 0 或 1,sigmoid 函数可以返
回 0.731 . . .、0.880 . . . 等实数(这一点和刚才的平滑性有关)。也就是说,感
知机中神经元之间流动的是 0 或 1 的二元信号,而神经网络中流动的是连续
的实数值信号。
如果把这两个函数与水联系起来,则阶跃函数可以比作“竹筒敲石”A,
sigmoid 函数可以比作“水车”。阶跃函数就像竹筒敲石一样,只做是否传送
水(0 或 1)两个动作,而 sigmoid 函数就像水车一样,根据流过来的水量相应
地调整传送出去的水量。
接着说一下阶跃函数和 sigmoid 函数的共同性质。阶跃函数和 sigmoid
函数虽然在平滑性上有差异,但是如果从宏观视角看图 3-8,可以发现它们
具有相似的形状。实际上,两者的结构均是“输入小时,输出接近 0(为 0);
随着输入增大,输出向 1 靠近(变成 1)”。也就是说,当输入信号为重要信息时,
阶跃函数和 sigmoid 函数都会输出较大的值;当输入信号为不重要的信息时,
两者都输出较小的值。还有一个共同点是,不管输入信号有多小,或者有多
大,输出信号的值都在 0 到 1 之间。
3.2.6 非线性函数
阶跃函数和 sigmoid 函数还有其他共同点,就是两者均为非线性函数。
sigmoid 函数是一条曲线,阶跃函数是一条像阶梯一样的折线,两者都属于
非线性的函数。
在介绍激活函数时,经常会看到“非线性函数”和“线性函数”等术语。
函数本来是输入某个值后会返回一个值的转换器。向这个转换器输
入某个值后,输出值是输入值的常数倍的函数称为线性函数(用数学
式表示为 h(x) = cx。c 为常数)。因此,线性函数是一条笔直的直线。
而非线性函数,顾名思义,指的是不像线性函数那样呈现出一条直
线的函数。
A 竹筒敲石是日本的一种庭院设施。支点架起竹筒,一端下方置石,另一端切口上翘。在切口上滴水, 水积多后该端下垂,水流出,另一端翘起,之后又因重力而落下,击石发出响声。——译者注


3.2 激活函数 49
神经网络的激活函数必须使用非线性函数。换句话说,激活函数不能使
用线性函数。为什么不能使用线性函数呢?因为使用线性函数的话,加深神
经网络的层数就没有意义了。
线性函数的问题在于,不管如何加深层数,总是存在与之等效的“无
隐 藏 层 的 神 经 网 络”。为 了 具 体 地(稍 微 直 观 地)理 解 这 一 点,我 们 来 思
考 下 面 这 个 简 单 的 例 子。这 里 我 们 考 虑 把 线 性 函 数 h(x) = cx 作 为 激 活
函 数,把 y(x) = h(h(h(x))) 的 运 算 对 应 3 层 神 经 网 络 A。这 个 运 算 会 进 行
y(x) = c × c × c × x 的乘法运算,但是同样的处理可以由 y(x) = ax(注意,
a = c 3)这一次乘法运算(即没有隐藏层的神经网络)来表示。如本例所示,
使用线性函数时,无法发挥多层网络带来的优势。因此,为了发挥叠加层所
带来的优势,激活函数必须使用非线性函数。
3.2.7 ReLU 函数
到目前为止,我们介绍了作为激活函数的阶跃函数和 sigmoid 函数。在
神经网络发展的历史上,sigmoid 函数很早就开始被使用了,而最近则主要
使用 ReLU(Rectified Linear Unit)函数。
ReLU 函数在输入大于 0 时,直接输出该值;在输入小于等于 0 时,输
出 0(图 3-9)。
ReLU 函数可以表示为下面的式 (3.7)。
(3.7)
如 图 3-9 和 式(3.7)所 示,ReLU 函 数 是 一 个 非 常 简 单 的 函 数。因 此,
ReLU 函数的实现也很简单,可以写成如下形式。
def relu(x): return np.maximum(0, x)
A 该对应只是一个近似,实际的神经网络运算比这个例子要复杂,但不影响后面的结论成立。 ——译者注


第 3 章 神经网络
50
图 3-9 ReLU 函数
5
4
3
2
1
−1
0
−6 −4 −2 0 2 4 6
这里使用了 NumPy 的 maximum 函数。maximum 函数会从输入的数值中选
择较大的那个值进行输出。
本章剩余部分的内容仍将使用 sigmoid 函数作为激活函数,但在本书的
后半部分,则将主要使用 ReLU 函数。
3.3 多维数组的运算
如果掌握了 NumPy 多维数组的运算,就可以高效地实现神经网络。因此,
本节将介绍 NumPy 多维数组的运算,然后再进行神经网络的实现。
3.3.1 多维数组
简单地讲,多维数组就是“数字的集合”,数字排成一列的集合、排成
长方形的集合、排成三维状或者(更加一般化的)N 维状的集合都称为多维数
组。下面我们就用 NumPy 来生成多维数组,先从前面介绍过的一维数组开始。


3.3 多维数组的运算 51
>>> import numpy as np >>> A = np.array([1, 2, 3, 4]) >>> print(A) [1 2 3 4]
>>> np.ndim(A) 1
>>> A.shape (4,)
>>> A.shape[0] 4
如上所示,数组的维数可以通过 np.dim() 函数获得。此外,数组的形状
可以通过实例变量 shape 获得。在上面的例子中,A 是一维数组,由 4 个元素
构成。注意,这里的 A.shape 的结果是个元组(tuple)。这是因为一维数组的
情况下也要返回和多维数组的情况下一致的结果。例如,二维数组时返回的
是元组 (4,3),三维数组时返回的是元组 (4,3,2),因此一维数组时也同样以
元组的形式返回结果。下面我们来生成一个二维数组。
>>> B = np.array([[1,2], [3,4], [5,6]]) >>> print(B) [[1 2] [3 4] [5 6]]
>>> np.ndim(B) 2
>>> B.shape (3, 2)
这里生成了一个 3 × 2 的数组 B。3 × 2 的数组表示第一个维度有 3 个元素,
第二个维度有 2 个元素。另外,第一个维度对应第 0 维,第二个维度对应第
1 维(Python 的索引从 0 开始)。二维数组也称为矩阵(matrix)。如图 3-10 所示,
数组的横向排列称为行(row),纵向排列称为列(column)。
3.3.2 矩阵乘法
下面,我们来介绍矩阵(二维数组)的乘积。比如 2 × 2 的矩阵,其乘积
可以像图 3-11 这样进行计算(按图中顺序进行计算是规定好了的)。


第 3 章 神经网络
52
12
34
6
列
行
5
图 3-10 横向排列称为行,纵向排列称为列
图 3-11 矩阵的乘积的计算方法
12
34
A
=
B
56
78
19 22
43 50
1×5+2×7
3×5+4×7
如本例所示,矩阵的乘积是通过左边矩阵的行(横向)和右边矩阵的列(纵
向)以对应元素的方式相乘后再求和而得到的。并且,运算的结果保存为新
的多维数组的元素。比如,A 的第 1 行和 B 的第 1 列的乘积结果是新数组的
第 1 行第 1 列的元素,A 的第 2 行和 B 的第 1 列的结果是新数组的第 2 行第 1
列的元素。另外,在本书的数学标记中,矩阵将用黑斜体表示(比如,矩阵
A),以区别于单个元素的标量(比如,a 或 b)。这个运算在 Python 中可以用
如下代码实现。
>>> A = np.array([[1,2], [3,4]])


3.3 多维数组的运算 53
>>> A.shape (2, 2)
>>> B = np.array([[5,6], [7,8]]) >>> B.shape (2, 2)
>>> np.dot(A, B) array([[19, 22], [43, 50]])
这 里,A 和 B 都 是 2 × 2 的 矩 阵,它 们 的 乘 积 可 以 通 过 NumPy 的
np.dot() 函数计算(乘积也称为点积)。np.dot() 接收两个 NumPy 数组作为参
数,并返回数组的乘积。这里要注意的是,np.dot(A, B) 和 np.dot(B, A) 的
值可能不一样。和一般的运算(+ 或 * 等)不同,矩阵的乘积运算中,操作数(A、
B)的顺序不同,结果也会不同。
这里介绍的是计算 2 × 2 形状的矩阵的乘积的例子,其他形状的矩阵的
乘积也可以用相同的方法来计算。比如,2 × 3 的矩阵和 3 × 2 的矩阵的乘积
可按如下形式用 Python 来实现。
>>> A = np.array([[1,2,3], [4,5,6]]) >>> A.shape (2, 3)
>>> B = np.array([[1,2], [3,4], [5,6]]) >>> B.shape (3, 2)
>>> np.dot(A, B) array([[22, 28], [49, 64]])
2 × 3 的矩阵 A 和 3 × 2 的矩阵 B 的乘积可按以上方式实现。这里需要
注意的是矩阵的形状(shape)。具体地讲,矩阵 A 的第 1 维的元素个数(列数)
必须和矩阵 B 的第 0 维的元素个数(行数)相等。在上面的例子中,矩阵 A
的形状是 2 × 3,矩阵 B 的形状是 3 × 2,矩阵 A 的第 1 维的元素个数(3)和
矩阵 B 的第 0 维的元素个数(3)相等。如果这两个值不相等,则无法计算矩
阵的乘积。比如,如果用 Python 计算 2 × 3 的矩阵 A 和 2 × 2 的矩阵 C 的乘
积,则会输出如下错误。
>>> C = np.array([[1,2], [3,4]]) >>> C.shape


第 3 章 神经网络
54
(2, 2)
>>> A.shape (2, 3)
>>> np.dot(A, C)
Traceback (most recent call last): File "<stdin>", line 1, in <module> ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)
这个错误的意思是,矩阵 A 的第 1 维和矩阵 C 的第 0 维的元素个数不一
致(维度的索引从 0 开始)。也就是说,在多维数组的乘积运算中,必须使两
个矩阵中的对应维度的元素个数一致,这一点很重要。我们通过图 3-12 再来
确认一下。
图 3-12 在矩阵的乘积运算中,对应维度的元素个数要保持一致
A B =C
3×2 2×4 3×4
形状:
保持一致
图 3-12 中,3 × 2 的矩阵 A 和 2 × 4 的矩阵 B 的乘积运算生成了 3 × 4 的
矩阵 C。如图所示,矩阵 A 和矩阵 B 的对应维度的元素个数必须保持一致。
此外,还有一点很重要,就是运算结果的矩阵 C 的形状是由矩阵 A 的行数
和矩阵 B 的列数构成的。
另外,当 A 是二维矩阵、B 是一维数组时,如图 3-13 所示,对应维度
的元素个数要保持一致的原则依然成立。
可按如下方式用 Python 实现图 3-13 的例子。
>>> A = np.array([[1,2], [3, 4], [5,6]]) >>> A.shape (3, 2)
>>> B = np.array([7,8]) >>> B.shape (2,) >>> np.dot(A, B) array([23, 53, 83])


3.3 多维数组的运算 55
图 3-13 A 是二维矩阵、B 是一维数组时,也要保持对应维度的元素个数一致
3×2 2 3
形状:
保持一致
A B =C
3.3.3 神经网络的内积
下面我们使用 NumPy 矩阵来实现神经网络。这里我们以图 3-14 中的简
单神经网络为对象。这个神经网络省略了偏置和激活函数,只有权重。
X W =Y
2 2×3 3
1
2
3
5
4
6
(1 3 5)
246
一致
x1
x2
y1
y2
y3
图 3-14 通过矩阵的乘积进行神经网络的运算
实现该神经网络时,要注意 X、W、Y 的形状,特别是 X 和 W 的对应
维度的元素个数是否一致,这一点很重要。
>>> X = np.array([1, 2]) >>> X.shape (2,)
>>> W = np.array([[1, 3, 5], [2, 4, 6]]) >>> print(W) [[1 3 5] [2 4 6]] >>> W.shape


第 3 章 神经网络
56
(2, 3)
>>> Y = np.dot(X, W) >>> print(Y) [ 5 11 17]
如上所示,使用 np.dot(多维数组的点积),可以一次性计算出 Y 的结果。
这意味着,即便 Y 的元素个数为 100 或 1000,也可以通过一次运算就计算出
结果!如果不使用 np.dot,就必须单独计算 Y 的每一个元素(或者说必须使
用 for 语句),非常麻烦。因此,通过矩阵的乘积一次性完成计算的技巧,在
实现的层面上可以说是非常重要的。
3.4 3 层神经网络的实现
现在我们来进行神经网络的实现。这里我们以图 3-15 的 3 层神经网络为
对象,实现从输入到输出的(前向)处理。在代码实现方面,使用上一节介
绍的 NumPy 多维数组。巧妙地使用 NumPy 数组,可以用很少的代码完成
神经网络的前向处理。
图 3-15 3 层神经网络:输入层(第 0 层)有 2 个神经元,第 1 个隐藏层(第 1 层)有 3 个神经元, 第 2 个隐藏层(第 2 层)有 2 个神经元,输出层(第 3 层)有 2 个神经元
x1
x2
y1
y2


3.4 3 层神经网络的实现 57
3.4.1 符号确认
在介绍神经网络中的处理之前,我们先导入 、 等符号。这些符
号可能看上去有些复杂,不过因为只在本节使用,稍微读一下就跳过去也问
题不大。
本节的重点是神经网络的运算可以作为矩阵运算打包进行。因为
神经网络各层的运算是通过矩阵的乘法运算打包进行的(从宏观
视角来考虑),所以即便忘了(未记忆)具体的符号规则,也不影
响理解后面的内容。
我们先从定义符号开始。请看图 3-16。图 3-16 中只突出显示了从输入层
神经元 x2 到后一层的神经元 的权重。
如图 3-16 所示,权重和隐藏层的神经元的右上角有一个“(1)”,它表示
权重和神经元的层号(即第 1 层的权重、第 1 层的神经元)。此外,权重的右
下角有两个数字,它们是后一层的神经元和前一层的神经元的索引号。比如,
表示前一层的第 2 个神经元 x2 到后一层的第 1 个神经元 的权重。权
重右下角按照“后一层的索引号、前一层的索引号”的顺序排列。
w12
(1)
第1层的权重
后一层的第1个神经元
前一层的第2个神经元
x1
x2
图 3-16 权重的符号


第 3 章 神经网络
58
3.4.2 各层间信号传递的实现
现在看一下从输入层到第 1 层的第 1 个神经元的信号传递过程,如图 3-17
所示。
1
x1
x2
y1
y2
图 3-17 从输入层到第 1 层的信号传递
图 3-17 中增加了表示偏置的神经元“1”。请注意,偏置的右下角的索引
号只有一个。这是因为前一层的偏置神经元(神经元“1”)只有一个 A 。
为了确认前面的内容,现在用数学式表示 。 通过加权信号和偏
置的和按如下方式进行计算。
(3.8)
A 任何前一层的偏置神经元“1”都只有一个。偏置权重的数量取决于后一层的神经元的数量(不包括 后一层的偏置神经元“1”)。——译者注


3.4 3 层神经网络的实现 59
此外,如果使用矩阵的乘法运算,则可以将第 1 层的加权和表示成下面
的式(3.9)。
A(1) = XW (1) + B (1) (3.9)
其中,A(1)、X、B (1)、W (1) 如下所示。
下面我们用 NumPy 多维数组来实现式(3.9),这里将输入信号、权重、
偏置设置成任意值。
X = np.array([1.0, 0.5]) W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) B1 = np.array([0.1, 0.2, 0.3])
print(W1.shape) # (2, 3) print(X.shape) # (2,) print(B1.shape) # (3,)
A1 = np.dot(X, W1) + B1
这个运算和上一节进行的运算是一样的。W1 是 2 × 3 的数组,X 是元素个
数为 2 的一维数组。这里,W1 和 X 的对应维度的元素个数也保持了一致。
接下来,我们观察第 1 层中激活函数的计算过程。如果把这个计算过程
用图来表示的话,则如图 3-18 所示。
如图 3-18 所示,隐藏层的加权和(加权信号和偏置的总和)用 a 表示,被
激活函数转换后的信号用 z 表示。此外,图中 h() 表示激活函数,这里我们
使用的是 sigmoid 函数。用 Python 来实现,代码如下所示。
Z1 = sigmoid(A1)
print(A1) # [0.3, 0.7, 1.1] print(Z1) # [0.57444252, 0.66818777, 0.75026011]


第 3 章 神经网络
60
图 3-18 从输入层到第 1 层的信号传递
1
1
1
h()
h()
h()
x1
x2
y1
y2
这个 sigmoid() 函数就是之前定义的那个函数。它会接收 NumPy 数组,
并返回元素个数相同的 NumPy 数组。
下面,我们来实现第 1 层到第 2 层的信号传递(图 3-19)。
W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) B2 = np.array([0.1, 0.2])
print(Z1.shape) # (3,) print(W2.shape) # (3, 2) print(B2.shape) # (2,)
A2 = np.dot(Z1, W2) + B2 Z2 = sigmoid(A2)
除了第 1 层的输出(Z1)变成了第 2 层的输入这一点以外,这个实现和刚
才的代码完全相同。由此可知,通过使用 NumPy 数组,可以将层到层的信
号传递过程简单地写出来。


3.4 3 层神经网络的实现 61
图 3-19 第 1 层到第 2 层的信号传递
h()
h()
1
1
1
x1
x2
y1
y2
最后是第 2 层到输出层的信号传递(图 3-20)。输出层的实现也和之前的
实现基本相同。不过,最后的激活函数和之前的隐藏层有所不同。
def identity_function(x): return x
W3 = np.array([[0.1, 0.3], [0.2, 0.4]]) B3 = np.array([0.1, 0.2])
A3 = np.dot(Z2, W3) + B3 Y = identity_function(A3) # 或者 Y = A3
这里我们定义了 identity_function() 函数(也称为“恒等函数”),并将
其作为输出层的激活函数。恒等函数会将输入按原样输出,因此,这个例子
中没有必要特意定义 identity_function()。这里这样实现只是为了和之前的
流程保持统一。另外,图 3-20 中,输出层的激活函数用 σ() 表示,不同于隐
藏层的激活函数 h()(σ 读作 sigma)。


第 3 章 神经网络
62
图 3-20 从第 2 层到输出层的信号传递
1
1
1
x1
x2
y1
y2
σ()
σ()
输出层所用的激活函数,要根据求解问题的性质决定。一般地,回
归问题可以使用恒等函数,二元分类问题可以使用 sigmoid 函数,
多元分类问题可以使用 softmax 函数。关于输出层的激活函数,我
们将在下一节详细介绍。
3.4.3 代码实现小结
至此,我们已经介绍完了 3 层神经网络的实现。现在我们把之前的代码
实现全部整理一下。这里,我们按照神经网络的实现惯例,只把权重记为大
写字母 W1,其他的(偏置或中间结果等)都用小写字母表示。
def init_network(): network = {} network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) network['b1'] = np.array([0.1, 0.2, 0.3]) network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) network['b2'] = np.array([0.1, 0.2]) network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])


3.5 输出层的设计 63
network['b3'] = np.array([0.1, 0.2])
return network
def forward(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3']
a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = identity_function(a3)
return y
network = init_network() x = np.array([1.0, 0.5]) y = forward(network, x) print(y) # [ 0.31682708 0.69627909]
这里定义了 init_network() 和 forward() 函数。init_network() 函数会进
行权重和偏置的初始化,并将它们保存在字典变量 network 中。这个字典变
量 network 中保存了每一层所需的参数(权重和偏置)。forward() 函数中则封
装了将输入信号转换为输出信号的处理过程。
另外,这里出现了 forward(前向)一词,它表示的是从输入到输出方向
的传递处理。后面在进行神经网络的训练时,我们将介绍后向(backward,
从输出到输入方向)的处理。
至此,神经网络的前向处理的实现就完成了。通过巧妙地使用 NumPy
多维数组,我们高效地实现了神经网络。
3.5 输出层的设计
神经网络可以用在分类问题和回归问题上,不过需要根据情况改变输出
层的激活函数。一般而言,回归问题用恒等函数,分类问题用 softmax 函数。


第 3 章 神经网络
64
机器学习的问题大致可以分为分类问题和回归问题。分类问题是数
据属于哪一个类别的问题。比如,区分图像中的人是男性还是女性
的问题就是分类问题。而回归问题是根据某个输入预测一个(连续的)
数值的问题。比如,根据一个人的图像预测这个人的体重的问题就
是回归问题(类似“57.4kg”这样的预测)。
3.5.1 恒等函数和 softmax 函数
恒等函数会将输入按原样输出,对于输入的信息,不加以任何改动地直
接输出。因此,在输出层使用恒等函数时,输入信号会原封不动地被输出。
另外,将恒等函数的处理过程用之前的神经网络图来表示的话,则如图 3-21
所示。和前面介绍的隐藏层的激活函数一样,恒等函数进行的转换处理可以
用一根箭头来表示。
a1 y1
a2 y2
a3 y3
σ()
σ()
σ()
图 3-21 恒等函数
分类问题中使用的 softmax 函数可以用下面的式(3.10)表示。
(3.10)
exp(x) 是表示 ex 的指数函数(e 是纳皮尔常数 2.7182 . . .)。式(3.10)表示
假设输出层共有 n 个神经元,计算第 k 个神经元的输出 yk。如式(3.10)所示,
softmax 函数的分子是输入信号 ak 的指数函数,分母是所有输入信号的指数
函数的和。


3.5 输出层的设计 65
用图表示 softmax 函数的话,如图 3-22 所示。图 3-22 中,softmax 函数
的输出通过箭头与所有的输入信号相连。这是因为,从式(3.10)可以看出,
输出层的各个神经元都受到所有输入信号的影响。
a1 y1
a2 y2
a3 y3
σ()
图 3-22 softmax 函数
现在我们来实现 softmax 函数。在这个过程中,我们将使用 Python 解释
器逐一确认结果。
>>> a = np.array([0.3, 2.9, 4.0]) >>>
>>> exp_a = np.exp(a) # 指数函数 >>> print(exp_a)
[ 1.34985881 18.17414537 54.59815003] >>>
>>> sum_exp_a = np.sum(exp_a) # 指数函数的和 >>> print(sum_exp_a) 74.1221542102 >>>
>>> y = exp_a / sum_exp_a >>> print(y)
[ 0.01821127 0.24519181 0.73659691]
这个 Python 实现是完全依照式(3.10)进行的,所以不需要特别的解释。
考虑到后面还要使用 softmax 函数,这里我们把它定义成如下的 Python 函数。
def softmax(a): exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a
return y


第 3 章 神经网络
66
3.5.2 实现 softmax 函数时的注意事项
上面的 softmax 函数的实现虽然正确描述了式(3.10),但在计算机的运算
上有一定的缺陷。这个缺陷就是溢出问题。softmax 函数的实现中要进行指
数函数的运算,但是此时指数函数的值很容易变得非常大。比如,e10 的值
会超过 20000,e100 会变成一个后面有 40 多个 0 的超大值,e1000 的结果会返回
一个表示无穷大的 inf。如果在这些超大值之间进行除法运算,结果会出现“不
确定”的情况。
计算机处理“数”时,数值必须在 4 字节或 8 字节的有限数据宽度内。
这意味着数存在有效位数,也就是说,可以表示的数值范围是有
限的。因此,会出现超大值无法表示的问题。这个问题称为溢出,
在进行计算机的运算时必须(常常)注意。
softmax 函数的实现可以像式(3.11)这样进行改进。
(3.11)
首先,式(3.11)在分子和分母上都乘上 C 这个任意的常数(因为同时对
分母和分子乘以相同的常数,所以计算结果不变)。然后,把这个 C 移动到
指数函数(exp)中,记为 log C。最后,把 log C 替换为另一个符号 C 。
式(3.11)说明,在进行 softmax 的指数函数的运算时,加上(或者减去)
某个常数并不会改变运算的结果。这里的 C  可以使用任何值,但是为了防
止溢出,一般会使用输入信号中的最大值。我们来看一个具体的例子。


3.5 输出层的设计 67
>>> a = np.array([1010, 1000, 990]) >>> np.exp(a) / np.sum(np.exp(a)) # softmax 函数的运算 array([ nan, nan, nan]) # 没有被正确计算 >>>
>>> c = np.max(a) # 1010 >>> a - c array([ 0, -10, -20]) >>>
>>> np.exp(a - c) / np.sum(np.exp(a - c))
array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])
如该例所示,通过减去输入信号中的最大值(上例中的 c),我们发现原
本为 nan(not a number,不确定)的地方,现在被正确计算了。综上,我们
可以像下面这样实现 softmax 函数。
def softmax(a): c = np.max(a) exp_a = np.exp(a - c) # 溢出对策 sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a
return y
3.5.3 softmax 函数的特征
使用 softmax() 函数,可以按如下方式计算神经网络的输出。
>>> a = np.array([0.3, 2.9, 4.0]) >>> y = softmax(a) >>> print(y)
[ 0.01821127 0.24519181 0.73659691] >>> np.sum(y) 1.0
如上所示,softmax 函数的输出是 0.0 到 1.0 之间的实数。并且,softmax
函数的输出值的总和是 1。输出总和为 1 是 softmax 函数的一个重要性质。正
因为有了这个性质,我们才可以把 softmax 函数的输出解释为“概率”。
比如,上面的例子可以解释成 y[0] 的概率是 0.018(1.8 %),y[1] 的概率
是 0.245(24.5 %),y[2] 的概率是 0.737(73.7 %)。从概率的结果来看,可以
说“因为第 2 个元素的概率最高,所以答案是第 2 个类别”。而且,还可以回


第 3 章 神经网络
68
答“有 74 % 的概率是第 2 个类别,有 25 % 的概率是第 1 个类别,有 1 % 的概
率是第 0 个类别”。也就是说,通过使用 softmax 函数,我们可以用概率的(统
计的)方法处理问题。
这里需要注意的是,即便使用了 softmax 函数,各个元素之间的大小关
系也不会改变。这是因为指数函数(y = exp(x))是单调递增函数。实际上,
上例中 a 的各元素的大小关系和 y 的各元素的大小关系并没有改变。比如,a
的最大值是第 2 个元素,y 的最大值也仍是第 2 个元素。
一般而言,神经网络只把输出值最大的神经元所对应的类别作为识别结果。
并且,即便使用 softmax 函数,输出值最大的神经元的位置也不会变。因此,
神经网络在进行分类时,输出层的 softmax 函数可以省略。在实际的问题中,
由于指数函数的运算需要一定的计算机运算量,因此输出层的 softmax 函数
一般会被省略。AB
求解机器学习问题的步骤可以分为“学习”A 和“推理”两个阶段。首
先,在学习阶段进行模型的学习 B,然后,在推理阶段,用学到的
模型对未知的数据进行推理(分类)。如前所述,推理阶段一般会省
略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和
神经网络的学习有关系(详细内容请参考下一章)。
3.5.4 输出层的神经元数量
输出层的神经元数量需要根据待解决的问题来决定。对于分类问题,输
出层的神经元数量一般设定为类别的数量。比如,对于某个输入图像,预测
是图中的数字 0 到 9 中的哪一个的问题(10 类别分类问题),可以像图 3-23 这样,
将输出层的神经元设定为 10 个。
如图 3-23 所示,在这个例子中,输出层的神经元从上往下依次对应数字
0, 1, . . ., 9。此外,图中输出层的神经元的值用不同的灰度表示。这个例子
A“学习”也称为“训练”,为了强调算法从数据中学习模型,本书使用“学习”一词。——译者注
B 这里的“学习”是指使用训练数据、自动调整参数的过程,具体请参考第 4 章。——译者注


3.6 手写数字识别 69
中神经元 y2 颜色最深,输出的值最大。这表明这个神经网络预测的是 y2 对应
的类别,也就是“2”。
图 3-23 输出层的神经元对应各个数字
= “0”
= “2”
= “1”
= “9”
输入层 输出层
某种运算
y0
y1
y2
y9
3.6 手写数字识别
介绍完神经网络的结构之后,现在我们来试着解决实际问题。这里我们
来进行手写数字图像的分类。假设学习已经全部结束,我们使用学习到的参
数,先实现神经网络的“推理处理”。这个推理处理也称为神经网络的前向
传播(forward propagation)。
和求解机器学习问题的步骤(分成学习和推理两个阶段进行)一样,
使用神经网络解决问题时,也需要首先使用训练数据(学习数据)进
行权重参数的学习;进行推理时,使用刚才学习到的参数,对输入
数据进行分类。


第 3 章 神经网络
70
3.6.1 MNIST 数据集
这里使用的数据集是 MNIST 手写数字图像集。MNIST 是机器学习领域
最有名的数据集之一,被应用于从简单的实验到发表的论文研究等各种场合。
实际上,在阅读图像识别或机器学习的论文时,MNIST 数据集经常作为实
验用的数据出现。
MNIST 数据集是由 0 到 9 的数字图像构成的(图 3-24)。训练图像有 6 万张,
测试图像有 1 万张,这些图像可以用于学习和推理。MNIST 数据集的一般
使用方法是,先用训练图像进行学习,再用学习到的模型度量能在多大程度
上对测试图像进行正确的分类。
图 3-24 MNIST 图像数据集的例子
MNIST 的图像数据是 28 像素 × 28 像素的灰度图像(1 通道),各个像素
的取值在 0 到 255 之间。每个图像数据都相应地标有“7”“2”“1”等标签。
本书提供了便利的 Python 脚本 mnist.py,该脚本支持从下载 MNIST 数据
集到将这些数据转换成 NumPy 数组等处理(mnist.py 在 dataset 目录下)。使用
mnist.py 时,当前目录必须是 ch01、ch02、ch03、...、ch08 目录中的一个。使
用 mnist.py 中的 load_mnist() 函数,就可以按下述方式轻松读入 MNIST 数据。
import sys, os sys.path.append(os.pardir) # 为了导入父目录中的文件而进行的设定 from dataset.mnist import load_mnist
# 第一次调用会花费几分钟 ......
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
# 输出各个数据的形状
print(x_train.shape) # (60000, 784)


3.6 手写数字识别 71
print(t_train.shape) # (60000,) print(x_test.shape) # (10000, 784) print(t_test.shape) # (10000,)
首 先,为 了 导 入 父 目 录 中 的 文 件,进 行 相 应 的 设 定 A。然 后,导 入
dataset/mnist.py 中 的 load_mnist 函 数。最 后,使 用 load_mnist 函 数,读 入
MNIST 数据集。第一次调用 load_mnist 函数时,因为要下载 MNIST 数据集,
所以需要接入网络。第 2 次及以后的调用只需读入保存在本地的文件(pickle
文件)即可,因此处理所需的时间非常短。
用来读入 MNIST 图像的文件在本书提供的源代码的 dataset 目
录下。并且,我们假定了这个 MNIST 数据集只能从 ch01、ch02、
ch03、...、ch08 目录中使用,因此,使用时需要从父目录(dataset
目录)中导入文件,为此需要添加sys.path.append(os.pardir) 语句。
load_mnist 函数以“( 训练图像 , 训练标签 ),( 测试图像,测试标签 )”的
形式返回读入的 MNIST 数据。此外,还可以像 load_mnist(normalize=True,
flatten=True, one_hot_label=False) 这 样,设 置 3 个 参 数。第 1 个 参 数
normalize 设置是否将输入图像正规化为 0.0~1.0 的值。如果将该参数设置
为 False,则输入图像的像素会保持原来的 0~255。第 2 个参数 flatten 设置
是否展开输入图像(变成一维数组)。如果将该参数设置为 False,则输入图
像为 1 × 28 × 28 的三维数组;若设置为 True,则输入图像会保存为由 784 个
元素构成的一维数组。第 3 个参数 one_hot_label 设置是否将标签保存为 one
hot 表示(one-hot representation)。one-hot 表示是仅正确解标签为 1,其余
皆为 0 的数组,就像 [0,0,1,0,0,0,0,0,0,0] 这样。当 one_hot_label 为 False 时,
只是像 7、2 这样简单保存正确解标签;当 one_hot_label 为 True 时,标签则
保存为 one-hot 表示。
A 观察本书源代码可知,上述代码在 mnist_show.py 文件中。mnist_show.py 文件的当前目录是 ch03, 但包含 load_mnist() 函数的 mnist.py 文件在 dataset 目录下。因此,mnist_show.py 文件不能跨目 录直接导入 mnist.py 文件。sys.path.append(os.pardir) 语句实际上是把父目录 deep-learningfrom-scratch 加入到 sys.path(Python 的搜索模块的路径集)中,从而可以导入 deep-learningfrom-scratch 下的任何目录(包括 dataset 目录)中的任何文件。——译者注


第 3 章 神经网络
72
Python 有 pickle 这个便利的功能。这个功能可以将程序运行中的对
象保存为文件。如果加载保存过的 pickle 文件,可以立刻复原之前
程序运行中的对象。用于读入 MNIST 数据集的 load_mnist() 函数内
部也使用了 pickle 功能(在第 2 次及以后读入时)。利用 pickle 功能,
可以高效地完成 MNIST 数据的准备工作。
现在,我们试着显示 MNIST 图像,同时也确认一下数据。图像的显示
使用 PIL(Python Image Library)模块。执行下述代码后,训练图像的第一
张就会显示出来,如图 3-25 所示(源代码在 ch03/mnist_show.py 中)。
import sys, os sys.path.append(os.pardir) import numpy as np from dataset.mnist import load_mnist from PIL import Image
def img_show(img): pil_img = Image.fromarray(np.uint8(img)) pil_img.show()
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False) img = x_train[0] label = t_train[0] print(label) # 5
print(img.shape) # (784,) img = img.reshape(28, 28) # 把图像的形状变成原来的尺寸 print(img.shape) # (28, 28)
img_show(img)
这里需要注意的是,flatten=True 时读入的图像是以一列(一维)NumPy
数组的形式保存的。因此,显示图像时,需要把它变为原来的 28 像素 × 28
像素的形状。可以通过 reshape() 方法的参数指定期望的形状,更改 NumPy
数组的形状。此外,还需要把保存为 NumPy 数组的图像数据转换为 PIL 用
的数据对象,这个转换处理由 Image.fromarray() 来完成。


3.6 手写数字识别 73
图 3-25 显示 MNIST 图像
3.6.2 神经网络的推理处理
下面,我们对这个 MNIST 数据集实现神经网络的推理处理。神经网络
的输入层有 784 个神经元,输出层有 10 个神经元。输入层的 784 这个数字来
源于图像大小的 28 × 28 = 784,输出层的 10 这个数字来源于 10 类别分类(数
字 0 到 9,共 10 类别)。此外,这个神经网络有 2 个隐藏层,第 1 个隐藏层有
50 个神经元,第 2 个隐藏层有 100 个神经元。这个 50 和 100 可以设置为任何值。
下面我们先定义 get_data()、init_network()、predict() 这 3 个函数(代码在
ch03/neuralnet_mnist.py 中)。
def get_data(): (x_train, t_train), (x_test, t_test) = \ load_mnist(normalize=True, flatten=True, one_hot_label=False) return x_test, t_test
def init_network(): with open("sample_weight.pkl", 'rb') as f: network = pickle.load(f)
return network
def predict(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3']


第 3 章 神经网络
74
a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = softmax(a3)
return y
init_network() 会读入保存在 pickle 文件 sample_weight.pkl 中的学习到的
权重参数 A。这个文件中以字典变量的形式保存了权重和偏置参数。剩余的 2
个函数,和前面介绍的代码实现基本相同,无需再解释。现在,我们用这 3
个函数来实现神经网络的推理处理。然后,评价它的识别精度(accuracy),
即能在多大程度上正确分类。
x, t = get_data() network = init_network()
accuracy_cnt = 0 for i in range(len(x)): y = predict(network, x[i]) p = np.argmax(y) # 获取概率最高的元素的索引 if p == t[i]: accuracy_cnt += 1
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
首先获得 MNIST 数据集,生成网络。接着,用 for 语句逐一取出保存
在 x 中的图像数据,用 predict() 函数进行分类。predict() 函数以 NumPy 数
组的形式输出各个标签对应的概率。比如输出 [0.1, 0.3, 0.2, ..., 0.04] 的
数组,该数组表示“0”的概率为 0.1,“1”的概率为 0.3,等等。然后,我们
取出这个概率列表中的最大值的索引(第几个元素的概率最高),作为预测结
果。可以用 np.argmax(x) 函数取出数组中的最大值的索引,np.argmax(x) 将
获取被赋给参数 x 的数组中的最大值元素的索引。最后,比较神经网络所预
测的答案和正确解标签,将回答正确的概率作为识别精度。
A 因为之前我们假设学习已经完成,所以学习到的参数被保存下来。假设保存在 sample_weight.pkl 文件中,在推理阶段,我们直接加载这些已经学习到的参数。——译者注


3.6 手写数字识别 75
执行上面的代码后,会显示“Accuracy:0.9352”。这表示有 93.52 % 的数
据被正确分类了。目前我们的目标是运行学习到的神经网络,所以不讨论识
别精度本身,不过以后我们会花精力在神经网络的结构和学习方法上,思考
如何进一步提高这个精度。实际上,我们打算把精度提高到 99 % 以上。
另外,在这个例子中,我们把 load_mnist 函数的参数 normalize 设置成了
True。将 normalize 设置成 True 后,函数内部会进行转换,将图像的各个像
素值除以 255,使得数据的值在 0.0~1.0 的范围内。像这样把数据限定到某
个范围内的处理称为正规化(normalization)。此外,对神经网络的输入数据
进行某种既定的转换称为预处理(pre-processing)。这里,作为对输入图像的
一种预处理,我们进行了正规化。
预处理在神经网络(深度学习)中非常实用,其有效性已在提高识别
性能和学习的效率等众多实验中得到证明。在刚才的例子中,作为
一种预处理,我们将各个像素值除以 255,进行了简单的正规化。
实际上,很多预处理都会考虑到数据的整体分布。比如,利用数据
整体的均值或标准差,移动数据,使数据整体以 0 为中心分布,或
者进行正规化,把数据的延展控制在一定范围内。除此之外,还有
将数据整体的分布形状均匀化的方法,即数据白化(whitening)等。
3.6.3 批处理
以上就是处理 MNIST 数据集的神经网络的实现,现在我们来关注输入
数据和权重参数的“形状”。再看一下刚才的代码实现。
下面我们使用 Python 解释器,输出刚才的神经网络的各层的权重的形状。
>>> x, _ = get_data() >>> network = init_network() >>> W1, W2, W3 = network['W1'], network['W2'], network['W3'] >>>
>>> x.shape (10000, 784) >>> x[0].shape (784,)
>>> W1.shape