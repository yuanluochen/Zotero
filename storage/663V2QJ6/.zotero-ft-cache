This is page i Printer: Opaque this
Springer Series in Operations Research and Financial Engineering
Editors:
Thomas V. Mikosch Sidney I. Resnick Stephen M. Robinson


This is pag Printer: O
Springer Series in Operation Research and Financial Engineering
Altiok: Performance Analysis of Manufacturing Systems Birge and Louveaux: Introduction to Stochastic Programming Bonnans and Shapiro: Perturbation Analysis of Optimization Problems Bramel, Chen, and Simchi-Levi: The Logic of Logistics: Theory, Algorithms, and Applications for Logistics and Supply Chain Management (second edition) Dantzig and Thapa: Linear Programming 1: Introduction Dantzig and Thapa: Linear Programming 2: Theory and Extensions Drezner (Editor): Facility Location: A Survey of Applications and Methods Facchinei and Pang: Finite-Dimensional Variational Inequalities and Complementarity Problems, Volume I Facchinei and Pang: Finite-Dimensional Variational Inequalities and Complementarity Problems, Volume II Fishman: Discrete-Event Simulation: Modeling, Programming, and Analysis Fishman: Monte Carlo: Concepts, Algorithms, and Applications Haas: Stochastic Petri Nets: Modeling, Stability, Simulation Klamroth: Single-Facility Location Problems with Barriers Muckstadt: Analysis and Algorithms for Service Parts Supply Chains Nocedal and Wright: Numerical Optimization Olson: Decision Aids for Selection Problems Pinedo: Planning and Scheduling in Manufacturing and Services Pochet and Wolsey: Production Planning by Mixed Integer Programming Whitt: Stochastic-Process Limits: An Introduction to Stochastic-Process Limits and Their Application to Queues Yao (Editor): Stochastic Modeling and Analysis of Manufacturing Systems Yao and Zheng: Dynamic Control of Quality in Production-Inventory Systems: Coordination and Optimization Yeung and Petrosyan: Cooperative Stochastic Differential Games


This is page iii Printer: Opaque this
Jorge Nocedal Stephen J. Wright
Numerical Optimization
Second Edition


This is pag Printer: O
Jorge Nocedal Stephen J. Wright EECS Department Computer Sciences Department Northwestern University University of Wisconsin Evanston, IL 60208-3118 1210 West Dayton Street USA Madison, WI 53706–1613 nocedal@eecs.northwestern.edu USA swright@cs.wisc.edu
Series Editors:
Thomas V. Mikosch University of Copenhagen Laboratory of Actuarial Mathematics DK-1017 Copenhagen Denmark mikosch@act.ku.dk
Sidney I. Resnick Cornell University School of Operations Research and Industrial Engineering Ithaca, NY 14853 USA sirl@cornell.edu
Stephen M. Robinson Department of Industrial and Systems Engineering University of Wisconsin 1513 University Avenue Madison, WI 53706–1539 USA smrobins@facstaff.wise.edu
Mathematics Subject Classification (2000): 90B30, 90C11, 90-01, 90-02
Library of Congress Control Number: 2006923897
ISBN-10: 0-387-30303-0 ISBN-13: 978-0387-30303-1
Printed on acid-free paper.
C© 2006 Springer Science+Business Media, LLC. All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden. The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.
Printed in the United States of America. (TB/HAM)
987654321
springer.com


This is page v Printer: Opaque this
To Sue, Isabel and Martin and To Mum and Dad


This is page vii Printer: Opaque this
Contents
Preface xvii
Preface to the Second Edition xxi
1 Introduction 1 Mathematical Formulation . . . . . . . . . . . . . . . . . . . . . . . . 2 Example: A Transportation Problem . . . . . . . . . . . . . . . . . . . 4 Continuous versus Discrete Optimization . . . . . . . . . . . . . . . . . 5 Constrained and Unconstrained Optimization . . . . . . . . . . . . . . 6 Global and Local Optimization . . . . . . . . . . . . . . . . . . . . . . 6 Stochastic and Deterministic Optimization . . . . . . . . . . . . . . . . 7 Convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Optimization Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 8 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2 Fundamentals of Unconstrained Optimization 10 2.1 What Is a Solution? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12


viii C O N T E N T S
Recognizing a Local Minimum . . . . . . . . . . . . . . . . . . . . . . 14 Nonsmooth Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.2 Overview of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Two Strategies: Line Search and Trust Region . . . . . . . . . . . . . . . 19 Search Directions for Line Search Methods . . . . . . . . . . . . . . . . 20 Models for Trust-Region Methods . . . . . . . . . . . . . . . . . . . . . 25 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3 Line Search Methods 30 3.1 Step Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 The Wolfe Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 The Goldstein Conditions . . . . . . . . . . . . . . . . . . . . . . . . . 36 Sufficient Decrease and Backtracking . . . . . . . . . . . . . . . . . . . 37 3.2 Convergence of Line Search Methods . . . . . . . . . . . . . . . . . . . 37 3.3 Rate of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 Convergence Rate of Steepest Descent . . . . . . . . . . . . . . . . . . . 42 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.4 Newton’s Method with Hessian Modification . . . . . . . . . . . . . . . 48 Eigenvalue Modification . . . . . . . . . . . . . . . . . . . . . . . . . . 49 Adding a Multiple of the Identity . . . . . . . . . . . . . . . . . . . . . 51 Modified Cholesky Factorization . . . . . . . . . . . . . . . . . . . . . 52 Modified Symmetric Indefinite Factorization . . . . . . . . . . . . . . . 54 3.5 Step-Length Selection Algorithms . . . . . . . . . . . . . . . . . . . . . 56 Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Initial Step Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 A Line Search Algorithm for the Wolfe Conditions . . . . . . . . . . . . 60 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4 Trust-Region Methods 66 Outline of the Trust-Region Approach . . . . . . . . . . . . . . . . . . 68 4.1 Algorithms Based on the Cauchy Point . . . . . . . . . . . . . . . . . . 71 The Cauchy Point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 Improving on the Cauchy Point . . . . . . . . . . . . . . . . . . . . . . 73 The Dogleg Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 Two-Dimensional Subspace Minimization . . . . . . . . . . . . . . . . 76 4.2 Global Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 Reduction Obtained by the Cauchy Point . . . . . . . . . . . . . . . . . 77 Convergence to Stationary Points . . . . . . . . . . . . . . . . . . . . . 79 4.3 Iterative Solution of the Subproblem . . . . . . . . . . . . . . . . . . . 83


C O N T E N T S ix
The Hard Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 Convergence of Algorithms Based on Nearly Exact Solutions . . . . . . . 91 4.4 Local Convergence of Trust-Region Newton Methods . . . . . . . . . . 92 4.5 Other Enhancements . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 Trust Regions in Other Norms . . . . . . . . . . . . . . . . . . . . . . . 97 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
5 Conjugate Gradient Methods 101 5.1 The Linear Conjugate Gradient Method . . . . . . . . . . . . . . . . . . 102 Conjugate Direction Methods . . . . . . . . . . . . . . . . . . . . . . . 102 Basic Properties of the Conjugate Gradient Method . . . . . . . . . . . 107 A Practical Form of the Conjugate Gradient Method . . . . . . . . . . . 111 Rate of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 Practical Preconditioners . . . . . . . . . . . . . . . . . . . . . . . . . 120 5.2 Nonlinear Conjugate Gradient Methods . . . . . . . . . . . . . . . . . 121 The Fletcher–Reeves Method . . . . . . . . . . . . . . . . . . . . . . . 121 The Polak–Ribi`ere Method and Variants . . . . . . . . . . . . . . . . . 122 Quadratic Termination and Restarts . . . . . . . . . . . . . . . . . . . . 124 Behavior of the Fletcher–Reeves Method . . . . . . . . . . . . . . . . . 125 Global Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 Numerical Performance . . . . . . . . . . . . . . . . . . . . . . . . . . 131 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
6 Quasi-Newton Methods 135 6.1 The BFGS Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 Properties of the BFGS Method . . . . . . . . . . . . . . . . . . . . . . 141 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 6.2 The SR1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 Properties of SR1 Updating . . . . . . . . . . . . . . . . . . . . . . . . 147 6.3 The Broyden Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 6.4 Convergence Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 Global Convergence of the BFGS Method . . . . . . . . . . . . . . . . . 153 Superlinear Convergence of the BFGS Method . . . . . . . . . . . . . . 156 Convergence Analysis of the SR1 Method . . . . . . . . . . . . . . . . . 160 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162


x CONTENTS
7 Large-Scale Unconstrained Optimization 164 7.1 Inexact Newton Methods . . . . . . . . . . . . . . . . . . . . . . . . . 165 Local Convergence of Inexact Newton Methods . . . . . . . . . . . . . . 166 Line Search Newton–CG Method . . . . . . . . . . . . . . . . . . . . . 168 Trust-Region Newton–CG Method . . . . . . . . . . . . . . . . . . . . 170 Preconditioning the Trust-Region Newton–CG Method . . . . . . . . . 174 Trust-Region Newton–Lanczos Method . . . . . . . . . . . . . . . . . . 175 7.2 Limited-Memory Quasi-Newton Methods . . . . . . . . . . . . . . . . 176 Limited-Memory BFGS . . . . . . . . . . . . . . . . . . . . . . . . . . 177 Relationship with Conjugate Gradient Methods . . . . . . . . . . . . . 180 General Limited-Memory Updating . . . . . . . . . . . . . . . . . . . . 181 Compact Representation of BFGS Updating . . . . . . . . . . . . . . . 181 Unrolling the Update . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 7.3 Sparse Quasi-Newton Updates . . . . . . . . . . . . . . . . . . . . . . 185 7.4 Algorithms for Partially Separable Functions . . . . . . . . . . . . . . . 186 7.5 Perspectives and Software . . . . . . . . . . . . . . . . . . . . . . . . . 189 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
8 Calculating Derivatives 193 8.1 Finite-Difference Derivative Approximations . . . . . . . . . . . . . . . 194 Approximating the Gradient . . . . . . . . . . . . . . . . . . . . . . . . 195 Approximating a Sparse Jacobian . . . . . . . . . . . . . . . . . . . . . 197 Approximating the Hessian . . . . . . . . . . . . . . . . . . . . . . . . 201 Approximating a Sparse Hessian . . . . . . . . . . . . . . . . . . . . . . 202 8.2 Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . 204 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 The Forward Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 The Reverse Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 Vector Functions and Partial Separability . . . . . . . . . . . . . . . . . 210 Calculating Jacobians of Vector Functions . . . . . . . . . . . . . . . . . 212 Calculating Hessians: Forward Mode . . . . . . . . . . . . . . . . . . . 213 Calculating Hessians: Reverse Mode . . . . . . . . . . . . . . . . . . . . 215 Current Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
9 Derivative-Free Optimization 220 9.1 Finite Differences and Noise . . . . . . . . . . . . . . . . . . . . . . . . 221 9.2 Model-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 Interpolation and Polynomial Bases . . . . . . . . . . . . . . . . . . . . 226 Updating the Interpolation Set . . . . . . . . . . . . . . . . . . . . . . 227


C O N T E N T S xi
A Method Based on Minimum-Change Updating . . . . . . . . . . . . . 228 9.3 Coordinate and Pattern-Search Methods . . . . . . . . . . . . . . . . . 229 Coordinate Search Method . . . . . . . . . . . . . . . . . . . . . . . . 230 Pattern-Search Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 231 9.4 A Conjugate-Direction Method . . . . . . . . . . . . . . . . . . . . . . 234 9.5 Nelder–Mead Method . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 9.6 Implicit Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
10 Least-Squares Problems 245 10.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 10.2 Linear Least-Squares Problems . . . . . . . . . . . . . . . . . . . . . . 250 10.3 Algorithms for Nonlinear Least-Squares Problems . . . . . . . . . . . . 254 The Gauss–Newton Method . . . . . . . . . . . . . . . . . . . . . . . . 254 Convergence of the Gauss–Newton Method . . . . . . . . . . . . . . . . 255 The Levenberg–Marquardt Method . . . . . . . . . . . . . . . . . . . . 258 Implementation of the Levenberg–Marquardt Method . . . . . . . . . . 259 Convergence of the Levenberg–Marquardt Method . . . . . . . . . . . . 261 Methods for Large-Residual Problems . . . . . . . . . . . . . . . . . . . 262 10.4 Orthogonal Distance Regression . . . . . . . . . . . . . . . . . . . . . . 265 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
11 Nonlinear Equations 270 11.1 Local Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 Newton’s Method for Nonlinear Equations . . . . . . . . . . . . . . . . 274 Inexact Newton Methods . . . . . . . . . . . . . . . . . . . . . . . . . 277 Broyden’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 Tensor Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283 11.2 Practical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 Merit Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 Line Search Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 Trust-Region Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 290 11.3 Continuation/Homotopy Methods . . . . . . . . . . . . . . . . . . . . 296 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296 Practical Continuation Methods . . . . . . . . . . . . . . . . . . . . . . 297 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
12 Theory of Constrained Optimization 304 Local and Global Solutions . . . . . . . . . . . . . . . . . . . . . . . . 305


xii C O N T E N T S
Smoothness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306 12.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 A Single Equality Constraint . . . . . . . . . . . . . . . . . . . . . . . . 308 A Single Inequality Constraint . . . . . . . . . . . . . . . . . . . . . . . 310 Two Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . . . 313 12.2 Tangent Cone and Constraint Qualifications . . . . . . . . . . . . . . . 315 12.3 First-Order Optimality Conditions . . . . . . . . . . . . . . . . . . . . 320 12.4 First-Order Optimality Conditions: Proof . . . . . . . . . . . . . . . . . 323 Relating the Tangent Cone and the First-Order Feasible Direction Set . . 323 A Fundamental Necessary Condition . . . . . . . . . . . . . . . . . . . 325 Farkas’ Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326 Proof of Theorem 12.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 329 12.5 Second-Order Conditions . . . . . . . . . . . . . . . . . . . . . . . . . 330 Second-Order Conditions and Projected Hessians . . . . . . . . . . . . 337 12.6 Other Constraint Qualifications . . . . . . . . . . . . . . . . . . . . . . 338 12.7 A Geometric Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . 340 12.8 Lagrange Multipliers and Sensitivity . . . . . . . . . . . . . . . . . . . . 341 12.9 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
13 Linear Programming: The Simplex Method 355 Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 13.1 Optimality and Duality . . . . . . . . . . . . . . . . . . . . . . . . . . 358 Optimality Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 358 The Dual Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359 13.2 Geometry of the Feasible Set . . . . . . . . . . . . . . . . . . . . . . . . 362 Bases and Basic Feasible Points . . . . . . . . . . . . . . . . . . . . . . 362 Vertices of the Feasible Polytope . . . . . . . . . . . . . . . . . . . . . . 365 13.3 The Simplex Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366 A Single Step of the Method . . . . . . . . . . . . . . . . . . . . . . . . 370 13.4 Linear Algebra in the Simplex Method . . . . . . . . . . . . . . . . . . 372 13.5 Other Important Details . . . . . . . . . . . . . . . . . . . . . . . . . . 375 Pricing and Selection of the Entering Index . . . . . . . . . . . . . . . . 375 Starting the Simplex Method . . . . . . . . . . . . . . . . . . . . . . . 378 Degenerate Steps and Cycling . . . . . . . . . . . . . . . . . . . . . . . 381 13.6 The Dual Simplex Method . . . . . . . . . . . . . . . . . . . . . . . . . 382 13.7 Presolving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385 13.8 Where Does the Simplex Method Fit? . . . . . . . . . . . . . . . . . . . 388 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389


C O N T E N T S xiii
14 Linear Programming: Interior-Point Methods 392 14.1 Primal-Dual Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 The Central Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397 Central Path Neighborhoods and Path-Following Methods . . . . . . . . 399 14.2 Practical Primal-Dual Algorithms . . . . . . . . . . . . . . . . . . . . . 407 Corrector and Centering Steps . . . . . . . . . . . . . . . . . . . . . . . 407 Step Lengths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409 Starting Point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410 A Practical Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 411 Solving the Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . 411 14.3 Other Primal-Dual Algorithms and Extensions . . . . . . . . . . . . . . 413 Other Path-Following Methods . . . . . . . . . . . . . . . . . . . . . . 413 Potential-Reduction Methods . . . . . . . . . . . . . . . . . . . . . . . 414 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 14.4 Perspectives and Software . . . . . . . . . . . . . . . . . . . . . . . . . 416 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
15 Fundamentals of Algorithms for Nonlinear Constrained Optimization 421 15.1 Categorizing Optimization Algorithms . . . . . . . . . . . . . . . . . . 422 15.2 The Combinatorial Difficulty of Inequality-Constrained Problems . . . . 424 15.3 Elimination of Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 426 Simple Elimination using Linear Constraints . . . . . . . . . . . . . . . 428 General Reduction Strategies for Linear Constraints . . . . . . . . . . . 431 Effect of Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . 434 15.4 Merit Functions and Filters . . . . . . . . . . . . . . . . . . . . . . . . 435 Merit Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435 Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437 15.5 The Maratos Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440 15.6 Second-Order Correction and Nonmonotone Techniques . . . . . . . . 443 Nonmonotone (Watchdog) Strategy . . . . . . . . . . . . . . . . . . . 444 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
16 Quadratic Programming 448 16.1 Equality-Constrained Quadratic Programs . . . . . . . . . . . . . . . . 451 Properties of Equality-Constrained QPs . . . . . . . . . . . . . . . . . . 451 16.2 Direct Solution of the KKT System . . . . . . . . . . . . . . . . . . . . 454 Factoring the Full KKT System . . . . . . . . . . . . . . . . . . . . . . 454 Schur-Complement Method . . . . . . . . . . . . . . . . . . . . . . . . 455 Null-Space Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457


xiv C O N T E N T S
16.3 Iterative Solution of the KKT System . . . . . . . . . . . . . . . . . . . 459 CG Applied to the Reduced System . . . . . . . . . . . . . . . . . . . . 459 The Projected CG Method . . . . . . . . . . . . . . . . . . . . . . . . . 461 16.4 Inequality-Constrained Problems . . . . . . . . . . . . . . . . . . . . . 463 Optimality Conditions for Inequality-Constrained Problems . . . . . . . 464 Degeneracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465 16.5 Active-Set Methods for Convex QPs . . . . . . . . . . . . . . . . . . . . 467 Specification of the Active-Set Method for Convex QP . . . . . . . . . . 472 Further Remarks on the Active-Set Method . . . . . . . . . . . . . . . . 476 Finite Termination of Active-Set Algorithm on Strictly Convex QPs . . . 477 Updating Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . 478 16.6 Interior-Point Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 480 Solving the Primal-Dual System . . . . . . . . . . . . . . . . . . . . . . 482 Step Length Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 483 A Practical Primal-Dual Method . . . . . . . . . . . . . . . . . . . . . 484 16.7 The Gradient Projection Method . . . . . . . . . . . . . . . . . . . . . 485 Cauchy Point Computation . . . . . . . . . . . . . . . . . . . . . . . . 486 Subspace Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . 488 16.8 Perspectives and Software . . . . . . . . . . . . . . . . . . . . . . . . . 490 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
17 Penalty and Augmented Lagrangian Methods 497 17.1 The Quadratic Penalty Method . . . . . . . . . . . . . . . . . . . . . . 498 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498 Algorithmic Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 501 Convergence of the Quadratic Penalty Method . . . . . . . . . . . . . . 502 Ill Conditioning and Reformulations . . . . . . . . . . . . . . . . . . . 505 17.2 Nonsmooth Penalty Functions . . . . . . . . . . . . . . . . . . . . . . 507 A Practical 1 Penalty Method . . . . . . . . . . . . . . . . . . . . . . . 511 A General Class of Nonsmooth Penalty Methods . . . . . . . . . . . . . 513 17.3 Augmented Lagrangian Method: Equality Constraints . . . . . . . . . . 514 Motivation and Algorithmic Framework . . . . . . . . . . . . . . . . . 514 Properties of the Augmented Lagrangian . . . . . . . . . . . . . . . . . 517 17.4 Practical Augmented Lagrangian Methods . . . . . . . . . . . . . . . . 519 Bound-Constrained Formulation . . . . . . . . . . . . . . . . . . . . . 519 Linearly Constrained Formulation . . . . . . . . . . . . . . . . . . . . 522 Unconstrained Formulation . . . . . . . . . . . . . . . . . . . . . . . . 523 17.5 Perspectives and Software . . . . . . . . . . . . . . . . . . . . . . . . . 525 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527


C O N T E N T S xv
18 Sequential Quadratic Programming 529 18.1 Local SQP Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530 SQP Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531 Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . 532 18.2 Preview of Practical SQP Methods . . . . . . . . . . . . . . . . . . . . . 533 IQP and EQP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533 Enforcing Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . 534 18.3 Algorithmic Development . . . . . . . . . . . . . . . . . . . . . . . . . 535 Handling Inconsistent Linearizations . . . . . . . . . . . . . . . . . . . 535 Full Quasi-Newton Approximations . . . . . . . . . . . . . . . . . . . . 536 Reduced-Hessian Quasi-Newton Approximations . . . . . . . . . . . . 538 Merit Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540 Second-Order Correction . . . . . . . . . . . . . . . . . . . . . . . . . 543 18.4 A Practical Line Search SQP Method . . . . . . . . . . . . . . . . . . . 545 18.5 Trust-Region SQP Methods . . . . . . . . . . . . . . . . . . . . . . . . 546 A Relaxation Method for Equality-Constrained Optimization . . . . . . 547 S 1QP (Sequential 1 Quadratic Programming) . . . . . . . . . . . . . 549 Sequential Linear-Quadratic Programming (SLQP) . . . . . . . . . . . 551 A Technique for Updating the Penalty Parameter . . . . . . . . . . . . . 553 18.6 Nonlinear Gradient Projection . . . . . . . . . . . . . . . . . . . . . . 554 18.7 Convergence Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 556 Rate of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557 18.8 Perspectives and Software . . . . . . . . . . . . . . . . . . . . . . . . . 560 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
19 Interior-Point Methods for Nonlinear Programming 563 19.1 Two Interpretations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564 19.2 A Basic Interior-Point Algorithm . . . . . . . . . . . . . . . . . . . . . 566 19.3 Algorithmic Development . . . . . . . . . . . . . . . . . . . . . . . . . 569 Primal vs. Primal-Dual System . . . . . . . . . . . . . . . . . . . . . . 570 Solving the Primal-Dual System . . . . . . . . . . . . . . . . . . . . . . 570 Updating the Barrier Parameter . . . . . . . . . . . . . . . . . . . . . . 572 Handling Nonconvexity and Singularity . . . . . . . . . . . . . . . . . . 573 Step Acceptance: Merit Functions and Filters . . . . . . . . . . . . . . . 575 Quasi-Newton Approximations . . . . . . . . . . . . . . . . . . . . . . 575 Feasible Interior-Point Methods . . . . . . . . . . . . . . . . . . . . . . 576 19.4 A Line Search Interior-Point Method . . . . . . . . . . . . . . . . . . . 577 19.5 A Trust-Region Interior-Point Method . . . . . . . . . . . . . . . . . . 578 An Algorithm for Solving the Barrier Problem . . . . . . . . . . . . . . 578 Step Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580 Lagrange Multipliers Estimates and Step Acceptance . . . . . . . . . . . 581


xvi C O N T E N T S
Description of a Trust-Region Interior-Point Method . . . . . . . . . . . 582 19.6 The Primal Log-Barrier Method . . . . . . . . . . . . . . . . . . . . . . 583 19.7 Global Convergence Properties . . . . . . . . . . . . . . . . . . . . . . 587 Failure of the Line Search Approach . . . . . . . . . . . . . . . . . . . . 587 Modified Line Search Methods . . . . . . . . . . . . . . . . . . . . . . 589 Global Convergence of the Trust-Region Approach . . . . . . . . . . . . 589 19.8 Superlinear Convergence . . . . . . . . . . . . . . . . . . . . . . . . . 591 19.9 Perspectives and Software . . . . . . . . . . . . . . . . . . . . . . . . . 592 Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
A Background Material 598 A.1 Elements of Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . 598 Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602 Eigenvalues, Eigenvectors, and the Singular-Value Decomposition . . . . 603 Determinant and Trace . . . . . . . . . . . . . . . . . . . . . . . . . . 605 Matrix Factorizations: Cholesky, LU, QR . . . . . . . . . . . . . . . . . 606 Symmetric Indefinite Factorization . . . . . . . . . . . . . . . . . . . . 610 Sherman–Morrison–Woodbury Formula . . . . . . . . . . . . . . . . . 612 Interlacing Eigenvalue Theorem . . . . . . . . . . . . . . . . . . . . . . 613 Error Analysis and Floating-Point Arithmetic . . . . . . . . . . . . . . . 613 Conditioning and Stability . . . . . . . . . . . . . . . . . . . . . . . . . 616 A.2 Elements of Analysis, Geometry, Topology . . . . . . . . . . . . . . . . 617 Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617 Rates of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . 619 Topology of the Euclidean Space IRn . . . . . . . . . . . . . . . . . . . . 620 Convex Sets in IRn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621 Continuity and Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . 623 Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625 Directional Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . 628 Mean Value Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . 629 Implicit Function Theorem . . . . . . . . . . . . . . . . . . . . . . . . 630 Order Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631 Root-Finding for Scalar Equations . . . . . . . . . . . . . . . . . . . . 633
B A Regularization Procedure 635
References 637
Index 653


This is page xvii Printer: Opaque this
Preface
This is a book for people interested in solving optimization problems. Because of the wide (and growing) use of optimization in science, engineering, economics, and industry, it is essential for students and practitioners alike to develop an understanding of optimization algorithms. Knowledge of the capabilities and limitations of these algorithms leads to a better understanding of their impact on various applications, and points the way to future research on improving and extending optimization algorithms and software. Our goal in this book is to give a comprehensive description of the most powerful, state-of-the-art, techniques for solving continuous optimization problems. By presenting the motivating ideas for each algorithm, we try to stimulate the reader’s intuition and make the technical details easier to follow. Formal mathematical requirements are kept to a minimum. Because of our focus on continuous problems, we have omitted discussion of important optimization topics such as discrete and stochastic optimization. However, there are a great many applications that can be formulated as continuous optimization problems; for instance,
finding the optimal trajectory for an aircraft or a robot arm;
identifying the seismic properties of a piece of the earth’s crust by fitting a model of the region under study to a set of readings from a network of recording stations;


xviii P R E F A C E
designing a portfolio of investments to maximize expected return while maintaining an acceptable level of risk;
controlling a chemical process or a mechanical device to optimize performance or meet standards of robustness;
computing the optimal shape of an automobile or aircraft component.
Every year optimization algorithms are being called on to handle problems that are much larger and complex than in the past. Accordingly, the book emphasizes largescale optimization techniques, such as interior-point methods, inexact Newton methods, limited-memory methods, and the role of partially separable functions and automatic differentiation. It treats important topics such as trust-region methods and sequential quadratic programming more thoroughly than existing texts, and includes comprehensive discussion of such “core curriculum” topics as constrained optimization theory, Newton and quasi-Newton methods, nonlinear least squares and nonlinear equations, the simplex method, and penalty and barrier methods for nonlinear programming.
The Audience
We intend that this book will be used in graduate-level courses in optimization, as offered in engineering, operations research, computer science, and mathematics departments. There is enough material here for a two-semester (or three-quarter) sequence of courses. We hope, too, that this book will be used by practitioners in engineering, basic science, and industry, and our presentation style is intended to facilitate self-study. Since the book treats a number of new algorithms and ideas that have not been described in earlier textbooks, we hope that this book will also be a useful reference for optimization researchers. Prerequisites for this book include some knowledge of linear algebra (including numerical linear algebra) and the standard sequence of calculus courses. To make the book as self-contained as possible, we have summarized much of the relevant material from these areas in the Appendix. Our experience in teaching engineering students has shown us that the material is best assimilated when combined with computer programming projects in which the student gains a good feeling for the algorithms—their complexity, memory demands, and elegance—and for the applications. In most chapters we provide simple computer exercises that require only minimal programming proficiency.
Emphasis and Writing Style
We have used a conversational style to motivate the ideas and present the numerical algorithms. Rather than being as concise as possible, our aim is to make the discussion flow in a natural way. As a result, the book is comparatively long, but we believe that it can be read relatively rapidly. The instructor can assign substantial reading assignments from the text and focus in class only on the main ideas. A typical chapter begins with a nonrigorous discussion of the topic at hand, including figures and diagrams and excluding technical details as far as possible. In subsequent sections,


P R E F A C E xix
the algorithms are motivated and discussed, and then stated explicitly. The major theoretical results are stated, and in many cases proved, in a rigorous fashion. These proofs can be skipped by readers who wish to avoid technical details. The practice of optimization depends not only on efficient and robust algorithms, but also on good modeling techniques, careful interpretation of results, and user-friendly software. In this book we discuss the various aspects of the optimization process—modeling, optimality conditions, algorithms, implementation, and interpretation of results—but not with equal weight. Examples throughout the book show how practical problems are formulated as optimization problems, but our treatment of modeling is light and serves mainly to set the stage for algorithmic developments. We refer the reader to Dantzig [86] and Fourer, Gay, and Kernighan [112] for more comprehensive discussion of this issue. Our treatment of optimality conditions is thorough but not exhaustive; some concepts are discussed more extensively in Mangasarian [198] and Clarke [62]. As mentioned above, we are quite comprehensive in discussing optimization algorithms.
Topics Not Covered
We omit some important topics, such as network optimization, integer programming, stochastic programming, nonsmooth optimization, and global optimization. Network and integer optimization are described in some excellent texts: for instance, Ahuja, Magnanti, and Orlin [1] in the case of network optimization and Nemhauser and Wolsey [224], Papadimitriou and Steiglitz [235], and Wolsey [312] in the case of integer programming. Books on stochastic optimization are only now appearing; we mention those of Kall and Wallace [174], Birge and Louveaux [22]. Nonsmooth optimization comes in many flavors. The relatively simple structures that arise in robust data fitting (which is sometimes based on the 1 norm) are treated by Osborne [232] and Fletcher [101]. The latter book also discusses algorithms for nonsmooth penalty functions that arise in constrained optimization; we discuss these briefly, too, in Chapter 18. A more analytical treatment of nonsmooth optimization is given by Hiriart-Urruty and Lemar ́echal [170]. We omit detailed treatment of some important topics that are the focus of intense current research, including interior-point methods for nonlinear programming and algorithms for complementarity problems.
Additional Resource
The material in the book is complemented by an online resource called the NEOS Guide, which can be found on the World-Wide Web at
http://www.mcs.anl.gov/otc/Guide/
The Guide contains information about most areas of optimization, and presents a number of case studies that describe applications of various optimization algorithms to real-world problems such as portfolio optimization and optimal dieting. Some of this material is interactive in nature and has been used extensively for class exercises.


xx P R E F A C E
For the most part, we have omitted detailed discussions of specific software packages, and refer the reader to Mor ́e and Wright [217] or to the Software Guide section of the NEOS Guide, which can be found at
http://www.mcs.anl.gov/otc/Guide/SoftwareGuide/
Users of optimization software refer in great numbers to this web site, which is being constantly updated to reflect new packages and changes to existing software.
Acknowledgments
We are most grateful to the following colleagues for their input and feedback on various sections of this work: Chris Bischof, Richard Byrd, George Corliss, Bob Fourer, David Gay, Jean-Charles Gilbert, Phillip Gill, Jean-Pierre Goux, Don Goldfarb, Nick Gould, Andreas Griewank, Matthias Heinkenschloss, Marcelo Marazzi, Hans Mittelmann, Jorge Mor ́e, Will Naylor, Michael Overton, Bob Plemmons, Hugo Scolnik, David Stewart, Philippe Toint, Luis Vicente, Andreas W ̈achter, and Ya-xiang Yuan. We thank Guanghui Liu, who provided help with many of the exercises, and Jill Lavelle who assisted us in preparing the figures. We also express our gratitude to our sponsors at the Department of Energy and the National Science Foundation, who have strongly supported our research efforts in optimization over the years. One of us (JN) would like to express his deep gratitude to Richard Byrd, who has taught him so much about optimization and who has helped him in very many ways throughout the course of his career.
Final Remark
In the preface to his 1987 book [101], Roger Fletcher described the field of optimization as a “fascinating blend of theory and computation, heuristics and rigor.” The ever-growing realm of applications and the explosion in computing power is driving optimization research in new and exciting directions, and the ingredients identified by Fletcher will continue to play important roles for many years to come.
Jorge Nocedal Stephen J. Wright Evanston, IL Argonne, IL


This is page xxi Printer: Opaque this
Preface to the
Second Edition
During the six years since the first edition of this book appeared, the field of continuous optimization has continued to grow and evolve. This new edition reflects a better understanding of constrained optimization at both the algorithmic and theoretical levels, and of the demands imposed by practical applications. Perhaps most notably, new chapters have been added on two important topics: derivative-free optimization (Chapter 9) and interiorpoint methods for nonlinear programming (Chapter 19). The former topic has proved to be of great interest in applications, while the latter topic has come into its own in recent years and now forms the basis of successful codes for nonlinear programming. Apart from the new chapters, we have revised and updated throughout the book, de-emphasizing or omitting less important topics, enhancing the treatment of subjects of evident interest, and adding new material in many places. The first part (unconstrained optimization) has been comprehensively reorganized to improve clarity. Discussion of Newton’s method—the touchstone method for unconstrained problems—is distributed more naturally throughout this part rather than being isolated in a single chapter. An expanded discussion of large-scale problems appears in Chapter 7. Some reorganization has taken place also in the second part (constrained optimization), with material common to sequential quadratic programming and interior-point methods now appearing in the chapter on fundamentals of nonlinear programming


xxii P R E F A C E T O T H E S E C O N D E D I T I O N
algorithms (Chapter 15) and the discussion of primal barrier methods moved to the new interior-point chapter. There is much new material in this part, including a treatment of nonlinear programming duality, an expanded discussion of algorithms for inequality constrained quadratic programming, a discussion of dual simplex and presolving in linear programming, a summary of practical issues in the implementation of interior-point linear programming algorithms, a description of conjugate-gradient methods for quadratic programming, and a discussion of filter methods and nonsmooth penalty methods in nonlinear programming algorithms. In many chapters we have added a Perspectives and Software section near the end, to place the preceding discussion in context and discuss the state of the art in software. The appendix has been rearranged with some additional topics added, so that it can be used in a more stand-alone fashion to cover some of the mathematical background required for the rest of the book. The exercises have been revised in most chapters. After these many additions, deletions, and changes, the second edition is only slightly longer than the first, reflecting our belief that careful selection of the material to include and exclude is an important responsibility for authors of books of this type. A manual containing solutions for selected problems will be available to bona fide instructors through the publisher. A list of typos will be maintained on the book’s web site, which is accessible from the web pages of both authors. We acknowledge with gratitude the comments and suggestions of many readers of the first edition, who sent corrections to many errors and provided valuable perspectives on the material, which led often to substantial changes. We mention in particular Frank Curtis, Michael Ferris, Andreas Griewank, Jacek Gondzio, Sven Leyffer, Philip Loewen, Rembert Reemtsen, and David Stewart. Our special thanks goes to Michael Overton, who taught from a draft of the second edition and sent many detailed and excellent suggestions. We also thank colleagues who read various chapters of the new edition carefully during development, including Richard Byrd, Nick Gould, Paul Hovland, Gabo Lop ́ez-Calva, Long Hei, Katya Scheinberg, Andreas Wa ̈chter, and Richard Waltz. We thank Jill Wright for improving some of the figures and for the new cover graphic. We mentioned in the original preface several areas of optimization that are not covered in this book. During the past six years, this list has only grown longer, as the field has continued to expand in new directions. In this regard, the following areas are particularly noteworthy: optimization problems with complementarity constraints, second-order cone and semidefinite programming, simulation-based optimization, robust optimization, and mixed-integer nonlinear programming. All these areas have seen theoretical and algorithmic advances in recent years, and in many cases developments are being driven by new classes of applications. Although this book does not cover any of these areas directly, it provides a foundation from which they can be studied.
Jorge Nocedal Stephen J. Wright Evanston, IL Madison, WI


This is page 1 Printer: Opaque this
CHAPTER1
Introduction
People optimize. Investors seek to create portfolios that avoid excessive risk while achieving a high rate of return. Manufacturers aim for maximum efficiency in the design and operation of their production processes. Engineers adjust parameters to optimize the performance of their designs. Nature optimizes. Physical systems tend to a state of minimum energy. The molecules in an isolated chemical system react with each other until the total potential energy of their electrons is minimized. Rays of light follow paths that minimize their travel time.


2 CHAPTER 1. INTRODUCTION
Optimization is an important tool in decision science and in the analysis of physical systems. To make use of this tool, we must first identify some objective, a quantitative measure of the performance of the system under study. This objective could be profit, time, potential energy, or any quantity or combination of quantities that can be represented by a single number. The objective depends on certain characteristics of the system, called variables or unknowns. Our goal is to find values of the variables that optimize the objective. Often the variables are restricted, or constrained, in some way. For instance, quantities such as electron density in a molecule and the interest rate on a loan cannot be negative. The process of identifying objective, variables, and constraints for a given problem is known as modeling. Construction of an appropriate model is the first step—sometimes the most important step—in the optimization process. If the model is too simplistic, it will not give useful insights into the practical problem. If it is too complex, it may be too difficult to solve. Once the model has been formulated, an optimization algorithm can be used to find its solution, usually with the help of a computer. There is no universal optimization algorithm but rather a collection of algorithms, each of which is tailored to a particular type of optimization problem. The responsibility of choosing the algorithm that is appropriate for a specific application often falls on the user. This choice is an important one, as it may determine whether the problem is solved rapidly or slowly and, indeed, whether the solution is found at all. After an optimization algorithm has been applied to the model, we must be able to recognize whether it has succeeded in its task of finding a solution. In many cases, there are elegant mathematical expressions known as optimality conditions for checking that the current set of variables is indeed the solution of the problem. If the optimality conditions are not satisfied, they may give useful information on how the current estimate of the solution can be improved. The model may be improved by applying techniques such as sensitivity analysis, which reveals the sensitivity of the solution to changes in the model and data. Interpretation of the solution in terms of the application may also suggest ways in which the model can be refined or improved (or corrected). If any changes are made to the model, the optimization problem is solved anew, and the process repeats.
MATHEMATICAL FORMULATION
Mathematically speaking, optimization is the minimization or maximization of a function subject to constraints on its variables. We use the following notation:
- x is the vector of variables, also called unknowns or parameters;
- f is the objective function, a (scalar) function of x that we want to maximize or minimize;
- ci are constraint functions, which are scalar functions of x that define certain equations and inequalities that the unknown vector x must satisfy.


CHAPTER 1. INTRODUCTION 3
1
2
x
c1 c2
x
x*
contours of f
feasible
region
Figure 1.1 Geometrical representation of the problem (1.2).
Using this notation, the optimization problem can be written as follows:
min
x∈IRn f (x) subject to ci (x) 0, i ∈ E,
ci (x) ≥ 0, i ∈ I. (1.1)
Here I and E are sets of indices for equality and inequality constraints, respectively. As a simple example, consider the problem
min (x1 − 2)2 + (x2 − 1)2 subject to x2
1 − x2 ≤ 0,
x1 + x2 ≤ 2. (1.2)
We can write this problem in the form (1.1) by defining
f (x) (x1 − 2)2 + (x2 − 1)2, x
[
x1
x2
]
,
c(x)
[
c1 (x )
c2 (x )
] [ −x2
1 + x2
−x1 − x2 + 2
]
, I {1, 2}, E ∅.
Figure 1.1 shows the contours of the objective function, that is, the set of points for which f (x) has a constant value. It also illustrates the feasible region, which is the set of points satisfying all the constraints (the area between the two constraint boundaries), and the point


4 CHAPTER 1. INTRODUCTION
x∗, which is the solution of the problem. Note that the “infeasible side” of the inequality constraints is shaded. The example above illustrates, too, that transformations are often necessary to express an optimization problem in the particular form (1.1). Often it is more natural or convenient to label the unknowns with two or three subscripts, or to refer to different variables by completely different names, so that relabeling is necessary to pose the problem in the form (1.1). Another common difference is that we are required to maximize rather than minimize f , but we can accommodate this change easily by minimizing − f in the formulation (1.1). Good modeling systems perform the conversion to standardized formulations such as (1.1) transparently to the user.
EXAMPLE: A TRANSPORTATION PROBLEM
We begin with a much simplified example of a problem that might arise in manufacturing and transportation. A chemical company has 2 factories F1 and F2 and a dozen retail outlets R1, R2, . . . , R12. Each factory Fi can produce ai tons of a certain chemical product each week; ai is called the capacity of the plant. Each retail outlet R j has a known weekly demand of b j tons of the product. The cost of shipping one ton of the product from factory Fi to retail outlet R j is ci j . The problem is to determine how much of the product to ship from each factory to each outlet so as to satisfy all the requirements and minimize cost. The variables of the problem are xi j , i 1, 2, j 1, . . . , 12, where xi j is the number of tons of the product shipped from factory Fi to retail outlet R j ; see Figure 1.2. We can write the problem as
min
∑
ij
ci j xi j (1.3a)
subject to
1 ∑2
j1
xi j ≤ ai , i 1, 2, (1.3b)
2 ∑
i1
xi j ≥ b j , j 1, . . . , 12, (1.3c)
xi j ≥ 0, i 1, 2, j 1, . . . , 12. (1.3d)
This type of problem is known as a linear programming problem, since the objective function and the constraints are all linear functions. In a more practical model, we would also include costs associated with manufacturing and storing the product. There may be volume discounts in practice for shipping the product; for example the cost (1.3a) could be represented by
∑
i j ci j
√δ + xi j , where δ > 0 is a small subscription fee. In this case, the problem is a nonlinear program because the objective function is nonlinear.


CHAPTER 1. INTRODUCTION 5
R
R
F
F
R
R12
1
3
X21 2
2
1
Figure 1.2 A transportation problem.
CONTINUOUS VERSUS DISCRETE OPTIMIZATION
In some optimization problems the variables make sense only if they take on integer values. For example, a variable xi could represent the number of power plants of type i that should be constructed by an electicity provider during the next 5 years, or it could indicate whether or not a particular factory should be located in a particular city. The mathematical formulation of such problems includes integrality constraints, which have the form xi ∈ Z, where Z is the set of integers, or binary constraints, which have the form xi ∈ {0, 1}, in addition to algebraic constraints like those appearing in (1.1). Problems of this type are called integer programming problems. If some of the variables in the problem are not restricted to be integer or binary variables, they are sometimes called mixed integer programming problems, or MIPs for short. Integer programming problems are a type of discrete optimization problem. Generally, discrete optimization problems may contain not only integers and binary variables, but also more abstract variable objects such as permutations of an ordered set. The defining feature of a discrete optimization problem is that the unknown x is drawn from a a finite (but often very large) set. By contrast, the feasible set for continuous optimization problems—the class of problems studied in this book—is usually uncountably infinite, as when the components of x are allowed to be real numbers. Continuous optimization problems are normally easier to solve because the smoothness of the functions makes it possible to use objective and constraint information at a particular point x to deduce information about the function’s behavior at all points close to x. In discrete problems, by constrast, the behavior of the objective and constraints may change significantly as we move from one feasible point to another, even if the two points are “close” by some measure. The feasible sets for discrete optimization problems can be thought of as exhibiting an extreme form of nonconvexity, as a convex combination of two feasible points is in general not feasible.


6 CHAPTER 1. INTRODUCTION
Discrete optimization problems are not addressed directly in this book; we refer the reader to the texts by Papadimitriou and Steiglitz [235], Nemhauser and Wolsey [224], Cook et al. [77], and Wolsey [312] for comprehensive treatments of this subject. We note, however, that continuous optimization techniques often play an important role in solving discrete optimization problems. For instance, the branch-and-bound method for integer linear programming problems requires the repeated solution of linear programming “relaxations,” in which some of the integer variables are fixed at integer values, while for other integer variables the integrality constraints are temporarily ignored. These subproblems are usually solved by the simplex method, which is discussed in Chapter 13 of this book.
CONSTRAINED AND UNCONSTRAINED OPTIMIZATION
Problems with the general form (1.1) can be classified according to the nature of the objective function and constraints (linear, nonlinear, convex), the number of variables (large or small), the smoothness of the functions (differentiable or nondifferentiable), and so on. An important distinction is between problems that have constraints on the variables and those that do not. This book is divided into two parts according to this classification. Unconstrained optimization problems, for which we have E I ∅ in (1.1), arise directly in many practical applications. Even for some problems with natural constraints on the variables, it may be safe to disregard them as they do not affect on the solution and do not interfere with algorithms. Unconstrained problems arise also as reformulations of constrained optimization problems, in which the constraints are replaced by penalization terms added to objective function that have the effect of discouraging constraint violations. Constrained optimization problems arise from models in which constraints play an essential role, for example in imposing budgetary constraints in an economic problem or shape constraints in a design problem. These constraints may be simple bounds such as
0 ≤ x1 ≤ 100, more general linear constraints such as ∑
i xi ≤ 1, or nonlinear inequalities that represent complex relationships among the variables. When the objective function and all the constraints are linear functions of x, the problem is a linear programming problem. Problems of this type are probably the most widely formulated and solved of all optimization problems, particularly in management, financial, and economic applications. Nonlinear programming problems, in which at least some of the constraints or the objective are nonlinear functions, tend to arise naturally in the physical sciences and engineering, and are becoming more widely used in management and economic sciences as well.
GLOBAL AND LOCAL OPTIMIZATION
Many algorithms for nonlinear optimization problems seek only a local solution, a point at which the objective function is smaller than at all other feasible nearby points. They do not always find the global solution, which is the point with lowest function value among all feasible points. Global solutions are needed in some applications, but for many problems they


CHAPTER 1. INTRODUCTION 7
are difficult to recognize and even more difficult to locate. For convex programming problems, and more particularly for linear programs, local solutions are also global solutions. General nonlinear problems, both constrained and unconstrained, may possess local solutions that are not global solutions. In this book we treat global optimization only in passing and focus instead on the computation and characterization of local solutions. We note, however, that many successful global optimization algorithms require the solution of many local optimization problems, to which the algorithms described in this book can be applied. Research papers on global optimization can be found in Floudas and Pardalos [109] and in the Journal of Global Optimization.
STOCHASTIC AND DETERMINISTIC OPTIMIZATION
In some optimization problems, the model cannot be fully specified because it depends on quantities that are unknown at the time of formulation. This characteristic is shared by many economic and financial planning models, which may depend for example on future interest rates, future demands for a product, or future commodity prices, but uncertainty can arise naturally in almost any type of application. Rather than just use a “best guess” for the uncertain quantities, modelers may obtain more useful solutions by incorporating additional knowledge about these quantities into the model. For example, they may know a number of possible scenarios for the uncertain demand, along with estimates of the probabilities of each scenario. Stochastic optimization algorithms use these quantifications of the uncertainty to produce solutions that optimize the expected performance of the model. Related paradigms for dealing with uncertain data in the model include chanceconstrained optimization, in which we ensure that the variables x satisfy the given constraints to some specified probability, and robust optimization, in which certain constraints are required to hold for all possible values of the uncertain data. We do not consider stochastic optimization problems further in this book, focusing instead on deterministic optimization problems, in which the model is completely known. Many algorithms for stochastic optimization do, however, proceed by formulating one or more deterministic subproblems, each of which can be solved by the techniques outlined here. Stochastic and robust optimization have seen a great deal of recent research activity. For further information on stochastic optimization, consult the books of Birge and Louveaux [22] and Kall and Wallace [174]. Robust optimization is discussed in Ben-Tal and Nemirovski [15].
CONVEXITY
The concept of convexity is fundamental in optimization. Many practical problems possess this property, which generally makes them easier to solve both in theory and practice.


8 CHAPTER 1. INTRODUCTION
The term “convex” can be applied both to sets and to functions. A set S ∈ IRn is a convex set if the straight line segment connecting any two points in S lies entirely inside S. Formally, for any two points x ∈ S and y ∈ S, we have αx + (1 − α)y ∈ S for all α ∈ [0, 1]. The function f is a convex function if its domain S is a convex set and if for any two points x and y in S, the following property is satisfied:
f (αx + (1 − α)y) ≤ α f (x) + (1 − α) f (y), for all α ∈ [0, 1]. (1.4)
Simple instances of convex sets include the unit ball {y ∈ IRn | ‖y‖2 ≤ 1}; and any polyhedron, which is a set defined by linear equalities and inequalities, that is,
{x ∈ IRn | Ax b, C x ≤ d},
where A and C are matrices of appropriate dimension, and b and d are vectors. Simple instances of convex functions include the linear function f (x) cT x + α, for any constant vector c ∈ IRn and scalar α; and the convex quadratic function f (x) x T H x, where H is a symmetric positive semidefinite matrix. We say that f is strictly convex if the inequality in (1.4) is strict whenever x y and α is in the open interval (0, 1). A function f is said to be concave if − f is convex. If the objective function in the optimization problem (1.1) and the feasible region are both convex, then any local solution of the problem is in fact a global solution. The term convex programming is used to describe a special case of the general constrained optimization problem (1.1) in which
• the objective function is convex,
• the equality constraint functions ci (·), i ∈ E, are linear, and
• the inequality constraint functions ci (·), i ∈ I, are concave.
OPTIMIZATION ALGORITHMS
Optimization algorithms are iterative. They begin with an initial guess of the variable x and generate a sequence of improved estimates (called “iterates”) until they terminate, hopefully at a solution. The strategy used to move from one iterate to the next distinguishes one algorithm from another. Most strategies make use of the values of the objective function f , the constraint functions ci , and possibly the first and second derivatives of these functions. Some algorithms accumulate information gathered at previous iterations, while others use only local information obtained at the current point. Regardless of these specifics (which will receive plenty of attention in the rest of the book), good algorithms should possess the following properties:
• Robustness. They should perform well on a wide variety of problems in their class, for all reasonable values of the starting point.


CHAPTER 1. INTRODUCTION 9
• Efficiency. They should not require excessive computer time or storage.
• Accuracy. They should be able to identify a solution with precision, without being overly sensitive to errors in the data or to the arithmetic rounding errors that occur when the algorithm is implemented on a computer.
These goals may conflict. For example, a rapidly convergent method for a large unconstrained nonlinear problem may require too much computer storage. On the other hand, a robust method may also be the slowest. Tradeoffs between convergence rate and storage requirements, and between robustness and speed, and so on, are central issues in numerical optimization. They receive careful consideration in this book. The mathematical theory of optimization is used both to characterize optimal points and to provide the basis for most algorithms. It is not possible to have a good understanding of numerical optimization without a firm grasp of the supporting theory. Accordingly, this book gives a solid (though not comprehensive) treatment of optimality conditions, as well as convergence analysis that reveals the strengths and weaknesses of some of the most important algorithms.
NOTES AND REFERENCES
Optimization traces its roots to the calculus of variations and the work of Euler and Lagrange. The development of linear programming n the 1940s broadened the field and stimulated much of the progress in modern optimization theory and practice during the past 60 years. Optimization is often called mathematical programming, a somewhat confusing term coined in the 1940s, before the word “programming” became inextricably linked with computer software. The original meaning of this word (and the intended one in this context) was more inclusive, with connotations of algorithm design and analysis. Modeling will not be treated extensively in the book. It is an essential subject in its own right, as it makes the connection between optimization algorithms and software on the one hand, and applications on the other hand. Information about modeling techniques for various application areas can be found in Dantzig [86], Ahuja, Magnanti, and Orlin [1], Fourer, Gay, and Kernighan [112], Winston [308], and Rardin [262].


This is pag Printer: O
CHAPTER2
Fundamentals of
Unconstrained
Optimization
In unconstrained optimization, we minimize an objective function that depends on real variables, with no restrictions at all on the values of these variables. The mathematical formulation is
mxin f (x), (2.1)
where x ∈ IRn is a real vector with n ≥ 1 components and f : IRn → IR is a smooth function.


C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N 11
y
.
.
.
.
.
y3
y2
y1
t1 t3 tm
t2
t
Figure 2.1 Least squares data fitting problem.
Usually, we lack a global perspective on the function f . All we know are the values of f and maybe some of its derivatives at a set of points x0, x1, x2, . . .. Fortunately, our algorithms get to choose these points, and they try to do so in a way that identifies a solution reliably and without using too much computer time or storage. Often, the information about f does not come cheaply, so we usually prefer algorithms that do not call for this information unnecessarily.
❏ EXAMPLE 2.1
Suppose that we are trying to find a curve that fits some experimental data. Figure 2.1 plots measurements y1, y2, . . . , ym of a signal taken at times t1, t2, . . . , tm. From the data and our knowledge of the application, we deduce that the signal has exponential and oscillatory behavior of certain types, and we choose to model it by the function
φ(t ; x ) x1 + x2e−(x3−t)2/x4 + x5 cos(x6t ).
The real numbers xi , i 1, 2, . . . , 6, are the parameters of the model; we would like to choose them to make the model values φ(t j ; x) fit the observed data y j as closely as possible. To state our objective as an optimization problem, we group the parameters xi into a vector of unknowns x (x1, x2, . . . , x6)T , and define the residuals
r j (x) y j − φ(t j ; x), j 1, 2, . . . , m, (2.2)
which measure the discrepancy between the model and the observed data. Our estimate of


12 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
x will be obtained by solving the problem
min
x∈IR6 f (x ) r 2
1 (x) + r2
2 (x) + · · · + r2
m(x). (2.3)
This is a nonlinear least-squares problem, a special case of unconstrained optimization. It illustrates that some objective functions can be expensive to evaluate even when the number of variables is small. Here we have n 6, but if the number of measurements m is large (105, say), evaluation of f (x) for a given parameter vector x is a significant computation. ❐
Suppose that for the data given in Figure 2.1 the optimal solution of (2.3) is approximately x∗ (1.1, 0.01, 1.2, 1.5, 2.0, 1.5) and the corresponding function value is f (x∗) 0.34. Because the optimal objective is nonzero, there must be discrepancies between the observed measurements y j and the model predictions φ(t j , x∗) for some (usually most) values of j—the model has not reproduced all the data points exactly. How, then, can we verify that x∗ is indeed a minimizer of f ? To answer this question, we need to define the term “solution” and explain how to recognize solutions. Only then can we discuss algorithms for unconstrained optimization problems.
2.1 WHAT IS A SOLUTION?
Generally, we would be happiest if we found a global minimizer of f , a point where the function attains its least value. A formal definition is
A point x∗ is a global minimizer if f (x∗) ≤ f (x) for all x,
where x ranges over all of IRn (or at least over the domain of interest to the modeler). The global minimizer can be difficult to find, because our knowledge of f is usually only local. Since our algorithm does not visit many points (we hope!), we usually do not have a good picture of the overall shape of f , and we can never be sure that the function does not take a sharp dip in some region that has not been sampled by the algorithm. Most algorithms are able to find only a local minimizer, which is a point that achieves the smallest value of f in its neighborhood. Formally, we say:
A point x∗ is a local minimizer if there is a neighborhood N of x∗ such that f (x∗) ≤ f (x) for all x ∈ N .
(Recall that a neighborhood of x∗ is simply an open set that contains x∗.) A point that satisfies this definition is sometimes called a weak local minimizer. This terminology distinguishes


2 . 1 . W H A T I S A S O L U T I O N ? 13
it from a strict local minimizer, which is the outright winner in its neighborhood. Formally,
A point x∗ is a strict local minimizer (also called a strong local minimizer) if there is a neighborhood N of x∗ such that f (x∗) < f (x) for all x ∈ N with x x∗.
For the constant function f (x) 2, every point x is a weak local minimizer, while the function f (x) (x − 2)4 has a strict local minimizer at x 2. A slightly more exotic type of local minimizer is defined as follows.
A point x∗ is an isolated local minimizer if there is a neighborhood N of x∗ such that x∗ is the only local minimizer in N .
Some strict local minimizers are not isolated, as illustrated by the function
f (x) x4 cos(1/x) + 2x4, f (0) 0,
which is twice continuously differentiable and has a strict local minimizer at x∗ 0. However, there are strict local minimizers at many nearby points x j , and we can label these points so that x j → 0 as j → ∞. While strict local minimizers are not always isolated, it is true that all isolated local minimizers are strict. Figure 2.2 illustrates a function with many local minimizers. It is usually difficult to find the global minimizer for such functions, because algorithms tend to be “trapped” at local minimizers. This example is by no means pathological. In optimization problems associated with the determination of molecular conformation, the potential function to be minimized may have millions of local minima.
f
x
Figure 2.2 A difficult case for global minimization.


14 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
Sometimes we have additional “global” knowledge about f that may help in identifying global minima. An important special case is that of convex functions, for which every local minimizer is also a global minimizer.
RECOGNIZING A LOCAL MINIMUM
From the definitions given above, it might seem that the only way to find out whether a point x∗ is a local minimum is to examine all the points in its immediate vicinity, to make sure that none of them has a smaller function value. When the function f is smooth, however, there are more efficient and practical ways to identify local minima. In particular, if f is twice continuously differentiable, we may be able to tell that x∗ is a local minimizer (and possibly a strict local minimizer) by examining just the gradient ∇ f (x∗) and the Hessian ∇2 f (x∗). The mathematical tool used to study minimizers of smooth functions is Taylor’s theorem. Because this theorem is central to our analysis throughout the book, we state it now. Its proof can be found in any calculus textbook.
Theorem 2.1 (Taylor’s Theorem).
Suppose that f : IRn → IR is continuously differentiable and that p ∈ IRn. Then we have that
f (x + p) f (x) + ∇ f (x + t p)T p, (2.4)
for some t ∈ (0, 1). Moreover, if f is twice continuously differentiable, we have that
∇ f (x + p) ∇ f (x) +
∫1
0
∇2 f (x + t p) p dt, (2.5)
and that
f (x + p) f (x) + ∇ f (x)T p + 1
2 pT ∇2 f (x + t p) p, (2.6)
for some t ∈ (0, 1).
Necessary conditions for optimality are derived by assuming that x∗ is a local minimizer and then proving facts about ∇ f (x∗) and ∇2 f (x∗).
Theorem 2.2 (First-Order Necessary Conditions).
If x∗ is a local minimizer and f is continuously differentiable in an open neighborhood of x∗, then ∇ f (x∗) 0.


2 . 1 . W H A T I S A S O L U T I O N ? 15
PROOF. Suppose for contradiction that ∇ f (x∗) 0. Define the vector p −∇ f (x∗) and note that pT ∇ f (x∗) −‖∇ f (x∗)‖2 < 0. Because ∇ f is continuous near x∗, there is a scalar T > 0 such that
pT ∇ f (x∗ + t p) < 0, for all t ∈ [0, T ].
For any t ̄ ∈ (0, T ], we have by Taylor’s theorem that
f (x∗ + t ̄p) f (x∗) + t ̄pT ∇ f (x∗ + t p), for some t ∈ (0, t ̄).
Therefore, f (x∗ + t ̄p) < f (x∗) for all t ̄ ∈ (0, T ]. We have found a direction leading away from x∗ along which f decreases, so x∗ is not a local minimizer, and we have a contradiction.
We call x∗ a stationary point if ∇ f (x∗) 0. According to Theorem 2.2, any local minimizer must be a stationary point. For the next result we recall that a matrix B is positive definite if pT Bp > 0 for all p 0, and positive semidefinite if pT Bp ≥ 0 for all p (see the Appendix).
Theorem 2.3 (Second-Order Necessary Conditions).
If x∗ is a local minimizer of f and ∇2 f exists and is continuous in an open neighborhood of x∗, then ∇ f (x∗) 0 and ∇2 f (x∗) is positive semidefinite.
PROOF. We know from Theorem 2.2 that ∇ f (x∗) 0. For contradiction, assume that ∇2 f (x∗) is not positive semidefinite. Then we can choose a vector p such that pT ∇2 f (x∗) p < 0, and because ∇2 f is continuous near x∗, there is a scalar T > 0 such that pT ∇2 f (x∗ + t p) p < 0 for all t ∈ [0, T ]. By doing a Taylor series expansion around x∗, we have for all t ̄ ∈ (0, T ] and some t ∈ (0, t ̄) that
f (x∗ + t ̄p) f (x∗) + t ̄pT ∇ f (x∗) + 1
2 t ̄2 pT ∇2 f (x∗ + t p) p < f (x∗).
As in Theorem 2.2, we have found a direction from x∗ along which f is decreasing, and so again, x∗ is not a local minimizer.
We now describe sufficient conditions, which are conditions on the derivatives of f at the point z∗ that guarantee that x∗ is a local minimizer.


16 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
Theorem 2.4 (Second-Order Sufficient Conditions).
Suppose that ∇2 f is continuous in an open neighborhood of x∗ and that ∇ f (x∗) 0 and ∇2 f (x∗) is positive definite. Then x∗ is a strict local minimizer of f .
PROOF. Because the Hessian is continuous and positive definite at x∗, we can choose a radius r > 0 so that ∇2 f (x) remains positive definite for all x in the open ball D {z | ‖z − x∗‖ < r }. Taking any nonzero vector p with ‖ p‖ < r , we have x∗ + p ∈ D and so
f (x∗ + p) f (x∗) + pT ∇ f (x∗) + 1
2 pT ∇2 f (z) p
f (x∗) + 1
2 pT ∇2 f (z) p,
where z x∗ + t p for some t ∈ (0, 1). Since z ∈ D, we have pT ∇2 f (z) p > 0, and therefore f (x∗ + p) > f (x∗), giving the result.
Note that the second-order sufficient conditions of Theorem 2.4 guarantee something stronger than the necessary conditions discussed earlier; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: A point x∗ may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function f (x) x4, for which the point x∗ 0 is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite). When the objective function is convex, local and global minimizers are simple to characterize.
Theorem 2.5.
When f is convex, any local minimizer x∗ is a global minimizer of f . If in addition f is differentiable, then any stationary point x∗ is a global minimizer of f .
PROOF. Suppose that x∗ is a local but not a global minimizer. Then we can find a point z ∈ IRn with f (z) < f (x∗). Consider the line segment that joins x∗ to z, that is,
x λz + (1 − λ)x∗, for some λ ∈ (0, 1]. (2.7)
By the convexity property for f , we have
f (x) ≤ λ f (z) + (1 − λ) f (x∗) < f (x∗). (2.8)
Any neighborhood N of x∗ contains a piece of the line segment (2.7), so there will always be points x ∈ N at which (2.8) is satisfied. Hence, x∗ is not a local minimizer.


2 . 1 . W H A T I S A S O L U T I O N ? 17
For the second part of the theorem, suppose that x∗ is not a global minimizer and choose z as above. Then, from convexity, we have
∇ f (x∗)T (z − x∗) d
dλ f (x∗ + λ(z − x∗)) |λ 0 (see the Appendix)
lλi↓m0
f (x∗ + λ(z − x∗)) − f (x∗) λ
≤ lλi↓m0
λ f (z) + (1 − λ) f (x∗) − f (x∗) λ f (z) − f (x∗) < 0.
Therefore, ∇ f (x∗) 0, and so x∗ is not a stationary point.
These results, which are based on elementary calculus, provide the foundations for unconstrained optimization algorithms. In one way or another, all algorithms seek a point where ∇ f (·) vanishes.
NONSMOOTH PROBLEMS
This book focuses on smooth functions, by which we generally mean functions whose second derivatives exist and are continuous. We note, however, that there are interesting problems in which the functions involved may be nonsmooth and even discontinuous. It is not possible in general to identify a minimizer of a general discontinuous function. If, however, the function consists of a few smooth pieces, with discontinuities between the pieces, it may be possible to find the minimizer by minimizing each smooth piece individually. If the function is continuous everywhere but nondifferentiable at certain points, as in Figure 2.3, we can identify a solution by examing the subgradient or generalized
*
f
xx
Figure 2.3 Nonsmooth function with minimum at a kink.


18 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
gradient, which are generalizations of the concept of gradient to the nonsmooth case. Nonsmooth optimization is beyond the scope of this book; we refer instead to HiriartUrruty and Lemar ́echal [170] for an extensive discussion of theory. Here, we mention only that the minimization of a function such as the one illustrated in Figure 2.3 (which contains a jump discontinuity in the first derivative f ′(x) at the minimum) is difficult because the behavior of f is not predictable near the point of nonsmoothness. That is, we cannot be sure that information about f obtained at one point can be used to infer anything about f at neighboring points, because points of nondifferentiability may intervene. However, minimization of certain special nondifferentiable functions, such as
f (x) ‖r (x)‖1, f (x) ‖r (x)‖∞ (2.9)
(where r (x) is a vector function), can be reformulated as smooth constrained optimization problems; see Exercise 12.5 in Chapter 12 and (17.31). The functions (2.9) are useful in data fitting, where r (x) is the residual vector whose components are defined in (2.2).
2.2 OVERVIEW OF ALGORITHMS
The last forty years have seen the development of a powerful collection of algorithms for unconstrained optimization of smooth functions. We now give a broad description of their main properties, and we describe them in more detail in Chapters 3, 4, 5, 6, and 7. All algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by x0. The user with knowledge about the application and the data set may be in a good position to choose x0 to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen by the algorithm, either by a systematic approach or in some arbitrary manner. Beginning at x0, optimization algorithms generate a sequence of iterates {xk}∞
k0
that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate xk to the next, the algorithms use information about the function f at xk, and possibly also information from earlier iterates x0, x1, . . . , xk−1. They use this information to find a new iterate xk+1 with a lower function value than xk. (There exist nonmonotone algorithms that do not insist on a decrease in f at every step, but even these algorithms require f to be decreased after some prescribed number m of iterations, that is, f (xk ) < f (xk−m).)
There are two fundamental strategies for moving from the current point xk to a new iterate xk+1. Most of the algorithms described in this book follow one of these approaches.


2 . 2 . O V E R V I E W O F A L G O R I T H M S 19
TWO STRATEGIES: LINE SEARCH AND TRUST REGION
In the line search strategy, the algorithm chooses a direction pk and searches along this direction from the current iterate xk for a new iterate with a lower function value. The distance to move along pk can be found by approximately solving the following onedimensional minimization problem to find a step length α:
αm>i0n f (xk + αpk). (2.10)
By solving (2.10) exactly, we would derive the maximum benefit from the direction pk, but an exact minimization may be expensive and is usually unnecessary. Instead, the line search algorithm generates a limited number of trial step lengths until it finds one that loosely approximates the minimum of (2.10). At the new point, a new search direction and step length are computed, and the process is repeated. In the second algorithmic strategy, known as trust region, the information gathered about f is used to construct a model function mk whose behavior near the current point xk is similar to that of the actual objective function f . Because the model mk may not be a good approximation of f when x is far from xk, we restrict the search for a minimizer of mk to some region around xk. In other words, we find the candidate step p by approximately solving the following subproblem:
mpin mk(xk + p), where xk + p lies inside the trust region. (2.11)
If the candidate solution does not produce a sufficient decrease in f , we conclude that the trust region is too large, and we shrink it and re-solve (2.11). Usually, the trust region is a ball defined by ‖ p‖2 ≤ , where the scalar > 0 is called the trust-region radius. Elliptical and box-shaped trust regions may also be used. The model mk in (2.11) is usually defined to be a quadratic function of the form
mk(xk + p) fk + pT ∇ fk + 1
2 pT Bk p, (2.12)
where fk, ∇ fk, and Bk are a scalar, vector, and matrix, respectively. As the notation indicates, fk and ∇ fk are chosen to be the function and gradient values at the point xk, so that mk and f are in agreement to first order at the current iterate xk. The matrix Bk is either the Hessian ∇2 fk or some approximation to it. Suppose that the objective function is given by f (x) 10(x2 − x2
1 )2 + (1 − x1)2. At the point xk (0, 1) its gradient and Hessian are
∇ fk
[ −2
20
]
, ∇2 fk
[ −38 0
0 20
]
.


20 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
k
k
k
f
x p
p * unconstrained
minimizer
contours
1
m=
m=
contours
12
of model
of
Figure 2.4 Two possible trust regions (circles) and their corresponding steps pk. The solid lines are contours of the model function mk.
The contour lines of the quadratic model (2.12) with Bk ∇2 fk are depicted in Figure 2.4, which also illustrates the contours of the objective function f and the trust region. We have indicated contour lines where the model mk has values 1 and 12. Note from Figure 2.4 that each time we decrease the size of the trust region after failure of a candidate iterate, the step from xk to the new candidate will be shorter, and it usually points in a different direction from the previous candidate. The trust-region strategy differs in this respect from line search, which stays with a single search direction. In a sense, the line search and trust-region approaches differ in the order in which they choose the direction and distance of the move to the next iterate. Line search starts by fixing the direction pk and then identifying an appropriate distance, namely the step length αk. In trust region, we first choose a maximum distance—the trust-region radius k—and then seek a direction and step that attain the best improvement possible subject to this distance constraint. If this step proves to be unsatisfactory, we reduce the distance measure k and try again. The line search approach is discussed in more detail in Chapter 3. Chapter 4 discusses the trust-region strategy, including techniques for choosing and adjusting the size of the region and for computing approximate solutions to the trust-region problems (2.11). We now preview two major issues: choice of the search direction pk in line search methods, and choice of the Hessian Bk in trust-region methods. These issues are closely related, as we now observe.
SEARCH DIRECTIONS FOR LINE SEARCH METHODS
The steepest descent direction −∇ fk is the most obvious choice for search direction for a line search method. It is intuitive; among all the directions we could move from xk,


2 . 2 . O V E R V I E W O F A L G O R I T H M S 21
it is the one along which f decreases most rapidly. To verify this claim, we appeal again to Taylor’s theorem (Theorem 2.1), which tells us that for any search direction p and step-length parameter α, we have
f (xk + αp) f (xk) + αpT ∇ fk + 1
2 α2 pT ∇2 f (xk + t p) p, for some t ∈ (0, α)
(see (2.6)). The rate of change in f along the direction p at xk is simply the coefficient of α, namely, pT ∇ fk. Hence, the unit direction p of most rapid decrease is the solution to the problem
mpin pT ∇ fk, subject to ‖ p‖ 1. (2.13)
Since pT ∇ fk ‖ p‖ ‖∇ fk‖ cos θ ‖∇ fk‖ cos θ , where θ is the angle between p and ∇ fk, it is easy to see that the minimizer is attained when cos θ −1 and
p −∇ fk/‖∇ fk‖,
as claimed. As we illustrate in Figure 2.5, this direction is orthogonal to the contours of the function. The steepest descent method is a line search method that moves along pk −∇ fk at every step. It can choose the step length αk in a variety of ways, as we discuss in Chapter 3. One advantage of the steepest descent direction is that it requires calculation of the gradient ∇ fk but not of second derivatives. However, it can be excruciatingly slow on difficult problems. Line search methods may use search directions other than the steepest descent direction. In general, any descent direction—one that makes an angle of strictly less than π/2 radians with −∇ fk—is guaranteed to produce a decrease in f , provided that the step length
*
x
px
k
k
.
Figure 2.5 Steepest descent direction for a function of two variables.


22 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
k
∆
– fk
p
Figure 2.6
A downhill direction pk.
is sufficiently small (see Figure 2.6). We can verify this claim by using Taylor’s theorem. From (2.6), we have that
f (xk + pk) f (xk) + pT
k ∇ fk + O( 2).
When pk is a downhill direction, the angle θk between pk and ∇ fk has cos θk < 0, so that
pT
k ∇ fk ‖ pk ‖ ‖∇ fk ‖ cos θk < 0.
It follows that f (xk + pk) < f (xk) for all positive but sufficiently small values of . Another important search direction—perhaps the most important one of allis the Newton direction. This direction is derived from the second-order Taylor series approximation to f (xk + p), which is
f (xk + p) ≈ fk + pT ∇ fk + 1
2 pT ∇2 fk p def mk ( p). (2.14)
Assuming for the moment that ∇2 fk is positive definite, we obtain the Newton direction by finding the vector p that minimizes mk( p). By simply setting the derivative of mk( p) to zero, we obtain the following explicit formula:
pN
k − (∇2 fk
)−1 ∇ fk. (2.15)
The Newton direction is reliable when the difference between the true function f (xk + p) and its quadratic model mk( p) is not too large. By comparing (2.14) with (2.6), we see that the only difference between these functions is that the matrix ∇2 f (xk + t p) in the third term of the expansion has been replaced by ∇2 fk. If ∇2 f is sufficiently smooth, this difference introduces a perturbation of only O(‖ p‖3) into the expansion, so that when ‖ p‖ is small, the approximation f (xk + p) ≈ mk( p) is quite accurate.


2 . 2 . O V E R V I E W O F A L G O R I T H M S 23
The Newton direction can be used in a line search method when ∇2 fk is positive definite, for in this case we have
∇ fT
k pN
k − pN
k
T ∇2 fk pN
k ≤ −σk ‖ pN
k ‖2
for some σk > 0. Unless the gradient ∇ fk (and therefore the step pN
k ) is zero, we have that ∇ fT
k pN
k < 0, so the Newton direction is a descent direction. Unlike the steepest descent direction, there is a “natural” step length of 1 associated with the Newton direction. Most line search implementations of Newton’s method use the unit step α 1 where possible and adjust α only when it does not produce a satisfactory reduction in the value of f . When ∇2 fk is not positive definite, the Newton direction may not even be defined,
since (∇2 fk
)−1 may not exist. Even when it is defined, it may not satisfy the descent property ∇ fT
k pN
k < 0, in which case it is unsuitable as a search direction. In these situations, line search methods modify the definition of pk to make it satisfy the descent condition while retaining the benefit of the second-order information contained in ∇2 fk. We describe these modifications in Chapter 3. Methods that use the Newton direction have a fast rate of local convergence, typically quadratic. After a neighborhood of the solution is reached, convergence to high accuracy often occurs in just a few iterations. The main drawback of the Newton direction is the need for the Hessian ∇2 f (x). Explicit computation of this matrix of second derivatives can sometimes be a cumbersome, error-prone, and expensive process. Finite-difference and automatic differentiation techniques described in Chapter 8 may be useful in avoiding the need to calculate second derivatives by hand. Quasi-Newton search directions provide an attractive alternative to Newton’s method in that they do not require computation of the Hessian and yet still attain a superlinear rate of convergence. In place of the true Hessian ∇2 fk, they use an approximation Bk, which is updated after each step to take account of the additional knowledge gained during the step. The updates make use of the fact that changes in the gradient g provide information about the second derivative of f along the search direction. By using the expression (2.5) from our statement of Taylor’s theorem, we have by adding and subtracting the term ∇2 f (x) p that
∇ f (x + p) ∇ f (x) + ∇2 f (x)p +
∫1
0
[∇2 f (x + t p) − ∇2 f (x)] p dt.
Because ∇ f (·) is continuous, the size of the final integral term is o(‖ p‖). By setting x xk and p xk+1 − xk, we obtain
∇ fk+1 ∇ fk + ∇2 fk (xk+1 − xk ) + o(‖xk+1 − xk ‖).
When xk and xk+1 lie in a region near the solution x∗, within which ∇2 f is positive definite, the final term in this expansion is eventually dominated by the ∇2 fk(xk+1 − xk) term, and


24 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
we can write
∇2 fk (xk+1 − xk ) ≈ ∇ fk+1 − ∇ fk . (2.16)
We choose the new Hessian approximation Bk+1 so that it mimics the property (2.16) of the true Hessian, that is, we require it to satisfy the following condition, known as the secant equation:
Bk+1sk yk , (2.17)
where
sk xk+1 − xk , yk ∇ fk+1 − ∇ fk .
Typically, we impose additional conditions on Bk+1, such as symmetry (motivated by symmetry of the exact Hessian), and a requirement that the difference between successive approximations Bk and Bk+1 have low rank. Two of the most popular formulae for updating the Hessian approximation Bk are the symmetric-rank-one (SR1) formula, defined by
Bk+1 Bk + (yk − Bk sk )(yk − Bk sk )T
(yk − Bk sk )T sk
, (2.18)
and the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno, which is defined by
Bk+1 Bk − Bk sk sT
k Bk sT
k Bk sk
+ yk yT
k yT
k sk
. (2.19)
Note that the difference between the matrices Bk and Bk+1 is a rank-one matrix in the case of (2.18) and a rank-two matrix in the case of (2.19). Both updates satisfy the secant equation and both maintain symmetry. One can show that BFGS update (2.19) generates positive definite approximations whenever the initial approximation B0 is positive definite and sT
k yk > 0. We discuss these issues further in Chapter 6. The quasi-Newton search direction is obtained by using Bk in place of the exact Hessian in the formula (2.15), that is,
pk −B−1
k ∇ fk. (2.20)
Some practical implementations of quasi-Newton methods avoid the need to factorize Bk at each iteration by updating the inverse of Bk, instead of Bk itself. In fact, the equivalent


2 . 2 . O V E R V I E W O F A L G O R I T H M S 25
formula for (2.18) and (2.19), applied to the inverse approximation Hk
def B−1
k , is
Hk+1
(I − ρk sk yT
k
) Hk
(I − ρk yk sT
k
) + ρk sk sT
k , ρk
1 yT
k sk
. (2.21)
Calculation of pk can then be performed by using the formula pk −Hk∇ fk. This matrixvector multiplication is simpler than the factorization/back-substitution procedure that is needed to implement the formula (2.20). Two variants of quasi-Newton methods designed to solve large problems—partially separable and limited-memory updating—are described in Chapter 7. The last class of search directions we preview here is that generated by nonlinear conjugate gradient methods. They have the form
pk −∇ f (xk ) + βk pk−1,
where βk is a scalar that ensures that pk and pk−1 are conjugate—an important concept in the minimization of quadratic functions that will be defined in Chapter 5. Conjugate gradient methods were originally designed to solve systems of linear equations Ax b, where the coefficient matrix A is symmetric and positive definite. The problem of solving this linear system is equivalent to the problem of minimizing the convex quadratic function defined by
φ(x) 1
2 xT Ax − bT x,
so it was natural to investigate extensions of these algorithms to more general types of unconstrained minimization problems. In general, nonlinear conjugate gradient directions are much more effective than the steepest descent direction and are almost as simple to compute. These methods do not attain the fast convergence rates of Newton or quasiNewton methods, but they have the advantage of not requiring storage of matrices. An extensive discussion of nonlinear conjugate gradient methods is given in Chapter 5. All of the search directions discussed so far can be used directly in a line search framework. They give rise to the steepest descent, Newton, quasi-Newton, and conjugate gradient line search methods. All except conjugate gradients have an analogue in the trustregion framework, as we now discuss.
MODELS FOR TRUST-REGION METHODS
If we set Bk 0 in (2.12) and define the trust region using the Euclidean norm, the trust-region subproblem (2.11) becomes
mpin fk + pT ∇ fk subject to ‖ p‖2 ≤ k.


26 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
We can write the solution to this problem in closed form as
pk − k ∇ fk
‖∇ fk‖ .
This is simply a steepest descent step in which the step length is determined by the trustregion radius; the trust-region and line search approaches are essentially the same in this case. A more interesting trust-region algorithm is obtained by choosing Bk to be the exact Hessian ∇2 fk in the quadratic model (2.12). Because of the trust-region restriction ‖ p‖2 ≤ k, the subproblem (2.11) is guaranteed to have a solution even when ∇2 fk is not positive definite pk, as we see in Figure 2.4. The trust-region Newton method has proved to be highly effective in practice, as we discuss in Chapter 7. If the matrix Bk in the quadratic model function mk of (2.12) is defined by means of a quasi-Newton approximation, we obtain a trust-region quasi-Newton method.
SCALING
The performance of an algorithm may depend crucially on how the problem is formulated. One important issue in problem formulation is scaling. In unconstrained optimization, a problem is said to be poorly scaled if changes to x in a certain direction produce much larger variations in the value of f than do changes to x in another direction. A simple example is provided by the function f (x) 109x2
1 + x2
2 , which is very sensitive to small changes in x1 but not so sensitive to perturbations in x2. Poorly scaled functions arise, for example, in simulations of physical and chemical systems where different processes are taking place at very different rates. To be more specific, consider a chemical system in which four reactions occur. Associated with each reaction is a rate constant that describes the speed at which the reaction takes place. The optimization problem is to find values for these rate constants by observing the concentrations of each chemical in the system at different times. The four constants differ greatly in magnitude, since the reactions take place at vastly different speeds. Suppose we have the following rough estimates for the final values of the constants, each correct to within, say, an order of magnitude:
x1 ≈ 10−10, x2 ≈ x3 ≈ 1, x4 ≈ 105.
Before solving this problem we could introduce a new variable z defined by
⎡
⎢⎢⎢⎢⎣
x1
x2
x3
x4
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
10−10 0 0 0
0 10 0
0 01 0
0 0 0 105
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
z1
z2
z3
z4
⎤
⎥⎥⎥⎥⎦ ,
and then define and solve the optimization problem in terms of the new variable z. The


2 . 2 . O V E R V I E W O F A L G O R I T H M S 27
∆
k
∆
fk

–f
Figure 2.7 Poorly scaled and well scaled problems, and performance of the steepest descent direction.
optimal values of z will be within about an order of magnitude of 1, making the solution more balanced. This kind of scaling of the variables is known as diagonal scaling. Scaling is performed (sometimes unintentionally) when the units used to represent variables are changed. During the modeling process, we may decide to change the units of some variables, say from meters to millimeters. If we do, the range of those variables and their size relative to the other variables will both change. Some optimization algorithms, such as steepest descent, are sensitive to poor scaling, while others, such as Newton’s method, are unaffected by it. Figure 2.7 shows the contours of two convex nearly quadratic functions, the first of which is poorly scaled, while the second is well scaled. For the poorly scaled problem, the one with highly elongated contours, the steepest descent direction does not yield much reduction in the function, while for the well-scaled problem it performs much better. In both cases, Newton’s method will produce a much better step, since the second-order quadratic model (mk in (2.14)) happens to be a good approximation of f . Algorithms that are not sensitive to scaling are preferable, because they can handle poor problem formulations in a more robust fashion. In designing complete algorithms, we try to incorporate scale invariance into all aspects of the algorithm, including the line search or trust-region strategies and convergence tests. Generally speaking, it is easier to preserve scale invariance for line search algorithms than for trust-region algorithms.
✐ EXERCISES
✐ 2.1 Compute the gradient ∇ f (x) and Hessian ∇2 f (x) of the Rosenbrock function
f (x) 100(x2 − x2
1 )2 + (1 − x1)2. (2.22)


28 C H A P T E R 2 . F U N D A M E N T A L S O F U N C O N S T R A I N E D O P T I M I Z A T I O N
Show that x∗ (1, 1)T is the only local minimizer of this function, and that the Hessian matrix at that point is positive definite.
✐ 2.2 Show that the function f (x) 8x1 + 12x2 + x2
1 − 2x2
2 has only one stationary point, and that it is neither a maximum or minimum, but a saddle point. Sketch the contour lines of f .
✐ 2.3 Let a be a given n-vector, and A be a given n × n symmetric matrix. Compute the gradient and Hessian of f1(x) aT x and f2(x) x T Ax.
✐ 2.4 Write the second-order Taylor expansion (2.6) for the function cos(1/x) around a nonzero point x, and the third-order Taylor expansion of cos(x) around any point x. Evaluate the second expansion for the specific case of x 1.
✐ 2.5 Consider the function f : IR2 → IR defined by f (x) ‖x‖2. Show that the sequence of iterates {xk} defined by
xk
(
1+ 1
2k
)[
cos k
sin k
]
satisfies f (xk+1) < f (xk) for k 0, 1, 2, . . . . Show that every point on the unit circle {x | ‖x‖2 1} is a limit point for {xk}. Hint: Every value θ ∈ [0, 2π ] is a limit point of the subsequence {ξk} defined by
ξk k(mod 2π ) k − 2π
⌊k
2π
⌋ ,
where the operator · denotes rounding down to the next integer.
✐ 2.6 Prove that all isolated local minimizers are strict. (Hint: Take an isolated local minimizer x∗ and a neighborhood N . Show that for any x ∈ N , x x∗ we must have f (x) > f (x∗).)
✐ 2.7 Suppose that f (x) x T Qx, where Q is an n × n symmetric positive semidefinite matrix. Show using the definition (1.4) that f (x) is convex on the domain IRn. Hint: It may be convenient to prove the following equivalent inequality:
f (y + α(x − y)) − α f (x) − (1 − α) f (y) ≤ 0,
for all α ∈ [0, 1] and all x, y ∈ IRn.
✐ 2.8 Suppose that f is a convex function. Show that the set of global minimizers of f is a convex set.


2 . 2 . O V E R V I E W O F A L G O R I T H M S 29
✐ 2.9 Consider the function f (x1, x2) (x1 + x2
2
)2. At the point x T (1, 0) we consider the search direction pT (−1, 1). Show that p is a descent direction and find all minimizers of the problem (2.10).
✐ 2.10 Suppose that f ̃(z) f (x), where x Sz + s for some S ∈ IRn×n and s ∈ IRn. Show that
∇ f ̃(z) ST ∇ f (x), ∇2 f ̃(z) ST ∇2 f (x)S.
(Hint: Use the chain rule to express d f ̃/dz j in terms of d f /d xi and d xi /dz j for all i, j 1, 2, . . . , n.)
✐ 2.11 Show that the symmetric rank-one update (2.18) and the BFGS update (2.19) are scale-invariant if the initial Hessian approximations B0 are chosen appropriately. That is, using the notation of the previous exercise, show that if these methods are applied to f (x) starting from x0 Sz0 + s with initial Hessian B0, and to f ̃(z) starting from z0 with initial Hessian ST B0 S, then all iterates are related by xk Szk + s. (Assume for simplicity that the methods take unit step lengths.)
✐ 2.12 Suppose that a function f of two variables is poorly scaled at the solution x∗. Write two Taylor expansions of f around x∗—one along each coordinate direction—and use them to show that the Hessian ∇2 f (x∗) is ill-conditioned.
✐ 2.13 (For this and the following three questions, refer to the material on “Rates of Convergence” in Section A.2 of the Appendix.) Show that the sequence xk 1/k is not Q-linearly convergent, though it does converge to zero. (This is called sublinear convergence.)
✐ 2.14 Show that the sequence xk 1 + (0.5)2k is Q-quadratically convergent to 1.
✐ 2.15 Does the sequence xk 1/k! converge Q-superlinearly? Q-quadratically?
✐ 2.16 Consider the sequence {xk} defined by
xk
{ (1
4
)2k , k even,
(xk−1)/k, k odd.
Is this sequence Q-superlinearly convergent? Q-quadratically convergent? R-quadratically convergent?


This is pag Printer: O
CHAPTER3
Line Search
Methods
Each iteration of a line search method computes a search direction pk and then decides how far to move along that direction. The iteration is given by
xk+1 xk + αk pk , (3.1)
where the positive scalar αk is called the step length. The success of a line search method depends on effective choices of both the direction pk and the step length αk. Most line search algorithms require pk to be a descent direction—one for which pT
k ∇ fk < 0—because this property guarantees that the function f can be reduced along


3 . 1 . S T E P L E N G T H 31
this direction, as discussed in the previous chapter. Moreover, the search direction often has the form
pk −B−1
k ∇ fk, (3.2)
where Bk is a symmetric and nonsingular matrix. In the steepest descent method, Bk is simply the identity matrix I , while in Newton’s method, Bk is the exact Hessian ∇2 f (xk). In quasi-Newton methods, Bk is an approximation to the Hessian that is updated at every iteration by means of a low-rank formula. When pk is defined by (3.2) and Bk is positive definite, we have
pT
k ∇ fk −∇ f T
k B−1
k ∇ fk < 0,
and therefore pk is a descent direction. In this chapter, we discuss how to choose αk and pk to promote convergence from remote starting points. We also study the rate of convergence of steepest descent, quasiNewton, and Newton methods. Since the pure Newton iteration is not guaranteed to produce descent directions when the current iterate is not close to a solution, we discuss modifications in Section 3.4 that allow it to start from any initial point. We now give careful consideration to the choice of the step-length parameter αk.
3.1 STEP LENGTH
In computing the step length αk, we face a tradeoff. We would like to choose αk to give a substantial reduction of f , but at the same time we do not want to spend too much time making the choice. The ideal choice would be the global minimizer of the univariate function φ(·) defined by
φ(α) f (xk + αpk), α > 0, (3.3)
but in general, it is too expensive to identify this value (see Figure 3.1). To find even a local minimizer of φ to moderate precision generally requires too many evaluations of the objective function f and possibly the gradient ∇ f . More practical strategies perform an inexact line search to identify a step length that achieves adequate reductions in f at minimal cost. Typical line search algorithms try out a sequence of candidate values for α, stopping to accept one of these values when certain conditions are satisfied. The line search is done in two stages: A bracketing phase finds an interval containing desirable step lengths, and a bisection or interpolation phase computes a good step length within this interval. Sophisticated line search algorithms can be quite complicated, so we defer a full description until Section 3.5.


32 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
φ (α)
point
stationary
first
minimizer
local
first
global minimizer
α
Figure 3.1 The ideal step length is the global minimizer.
We now discuss various termination conditions for line search algorithms and show that effective step lengths need not lie near minimizers of the univariate function φ(α) defined in (3.3). A simple condition we could impose on αk is to require a reduction in f , that is, f (xk + αk pk) < f (xk). That this requirement is not enough to produce convergence to x∗ is illustrated in Figure 3.2, for which the minimum function value is f ∗ −1, but a sequence of iterates {xk} for which f (xk) 5/k, k 0, 1, . . . yields a decrease at each iteration but has a limiting function value of zero. The insufficient reduction in f at each step causes it to fail to converge to the minimizer of this convex function. To avoid this behavior we need to enforce a sufficient decrease condition, a concept we discuss next.
x2 x0
x1
x
f(x)
Figure 3.2 Insufficient reduction in f .


3 . 1 . S T E P L E N G T H 33
THE WOLFE CONDITIONS
A popular inexact line search condition stipulates that αk should first of all give sufficient decrease in the objective function f , as measured by the following inequality:
f (xk + αpk) ≤ f (xk) + c1α∇ f T
k pk, (3.4)
for some constant c1 ∈ (0, 1). In other words, the reduction in f should be proportional to both the step length αk and the directional derivative ∇ f T
k pk. Inequality (3.4) is sometimes called the Armijo condition.
The sufficient decrease condition is illustrated in Figure 3.3. The right-hand-side of (3.4), which is a linear function, can be denoted by l(α). The function l(·) has negative slope c1∇ f T
k pk, but because c1 ∈ (0, 1), it lies above the graph of φ for small positive values of α. The sufficient decrease condition states that α is acceptable only if φ(α) ≤ l(α). The intervals on which this condition is satisfied are shown in Figure 3.3. In practice, c1 is chosen to be quite small, say c1 10−4. The sufficient decrease condition is not enough by itself to ensure that the algorithm makes reasonable progress because, as we see from Figure 3.3, it is satisfied for all sufficiently small values of α. To rule out unacceptably short steps we introduce a second requirement, called the curvature condition, which requires αk to satisfy
∇ f (xk + αk pk )T pk ≥ c2∇ f T
k pk, (3.5)
for some constant c2 ∈ (c1, 1), where c1 is the constant from (3.4). Note that the left-handside is simply the derivative φ′(αk), so the curvature condition ensures that the slope of φ at αk is greater than c2 times the initial slope φ′(0). This makes sense because if the slope φ′(α)
l(α)
φ (α) = f(xk+α pk )
acceptable
acceptable
α
Figure 3.3 Sufficient decrease condition.


34 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
desired slope
φ (α) =f(xk+α pk )
tangent α
acceptable
acceptable
Figure 3.4 The curvature condition.
is strongly negative, we have an indication that we can reduce f significantly by moving further along the chosen direction. On the other hand, if φ′(αk) is only slightly negative or even positive, it is a sign that we cannot expect much more decrease in f in this direction, so it makes sense to terminate the line search. The curvature condition is illustrated in Figure 3.4. Typical values of c2 are 0.9 when the search direction pk is chosen by a Newton or quasi-Newton method, and 0.1 when pk is obtained from a nonlinear conjugate gradient method. The sufficient decrease and curvature conditions are known collectively as the Wolfe conditions. We illustrate them in Figure 3.5 and restate them here for future reference:
f (xk + αk pk ) ≤ f (xk ) + c1αk ∇ f T
k pk, (3.6a)
∇ f (xk + αk pk )T pk ≥ c2∇ f T
k pk, (3.6b)
with 0 < c1 < c2 < 1. A step length may satisfy the Wolfe conditions without being particularly close to a minimizer of φ, as we show in Figure 3.5. We can, however, modify the curvature condition to force αk to lie in at least a broad neighborhood of a local minimizer or stationary point of φ. The strong Wolfe conditions require αk to satisfy
f (xk + αk pk ) ≤ f (xk ) + c1αk ∇ f T
k pk, (3.7a)
|∇ f (xk + αk pk )T pk | ≤ c2|∇ f T
k pk|, (3.7b)
with 0 < c1 < c2 < 1. The only difference with the Wolfe conditions is that we no longer allow the derivative φ′(αk) to be too positive. Hence, we exclude points that are far from stationary points of φ.


3 . 1 . S T E P L E N G T H 35
slope
desired
line of sufficient decrease l(α )
acceptable
α
φ (α ) = f(x + α pk
k)
acceptable
Figure 3.5 Step lengths satisfying the Wolfe conditions.
It is not difficult to prove that there exist step lengths that satisfy the Wolfe conditions for every function f that is smooth and bounded below.
Lemma 3.1.
Suppose that f : IRn → IR is continuously differentiable. Let pk be a descent direction at xk, and assume that f is bounded below along the ray {xk + αpk|α > 0}. Then if 0 < c1 < c2 < 1, there exist intervals of step lengths satisfying the Wolfe conditions (3.6) and the strong Wolfe conditions (3.7).
PROOF. Note that φ(α) f (xk + αpk) is bounded below for all α > 0. Since 0 < c1 < 1, the line l(α) f (xk) + αc1∇ f T
k pk is unbounded below and must therefore intersect the
graph of φ at least once. Let α′ > 0 be the smallest intersecting value of α, that is,
f (xk + α′ pk ) f (xk ) + α′c1∇ f T
k pk. (3.8)
The sufficient decrease condition (3.6a) clearly holds for all step lengths less than α′. By the mean value theorem (see (A.55)), there exists α′′ ∈ (0, α′) such that
f (xk + α′ pk) − f (xk ) α′∇ f (xk + α′′ pk )T pk . (3.9)
By combining (3.8) and (3.9), we obtain
∇ f (xk + α′′ pk )T pk c1∇ f T
k pk > c2∇ f T
k pk, (3.10)
since c1 < c2 and ∇ f T
k pk < 0. Therefore, α′′ satisfies the Wolfe conditions (3.6), and the inequalities hold strictly in both (3.6a) and (3.6b). Hence, by our smoothness assumption on f , there is an interval around α′′ for which the Wolfe conditions hold. Moreover, since


36 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
the term in the left-hand side of (3.10) is negative, the strong Wolfe conditions (3.7) hold in the same interval.
The Wolfe conditions are scale-invariant in a broad sense: Multiplying the objective function by a constant or making an affine change of variables does not alter them. They can be used in most line search methods, and are particularly important in the implementation of quasi-Newton methods, as we see in Chapter 6.
THE GOLDSTEIN CONDITIONS
Like the Wolfe conditions, the Goldstein conditions ensure that the step length α achieves sufficient decrease but is not too short. The Goldstein conditions can also be stated as a pair of inequalities, in the following way:
f (xk) + (1 − c)αk∇ f T
k pk ≤ f (xk + αk pk ) ≤ f (xk ) + cαk ∇ f T
k pk, (3.11)
with 0 < c < 1/2. The second inequality is the sufficient decrease condition (3.4), whereas the first inequality is introduced to control the step length from below; see Figure 3.6 A disadvantage of the Goldstein conditions vis-`a-vis the Wolfe conditions is that the first inequality in (3.11) may exclude all minimizers of φ. However, the Goldstein and Wolfe conditions have much in common, and their convergence theories are quite similar. The Goldstein conditions are often used in Newton-type methods but are not well suited for quasi-Newton methods that maintain a positive definite Hessian approximation.
fk
Tpk
αc
kTpk
α (1_ c) f
φ ( = f(xk+α p
α) k)
acceptable steplengths
α
Figure 3.6 The Goldstein conditions.


3 . 2 . C O N V E R G E N C E O F L I N E S E A R C H M E T H O D S 37
SUFFICIENT DECREASE AND BACKTRACKING
We have mentioned that the sufficient decrease condition (3.6a) alone is not sufficient to ensure that the algorithm makes reasonable progress along the given search direction. However, if the line search algorithm chooses its candidate step lengths appropriately, by using a so-called backtracking approach, we can dispense with the extra condition (3.6b) and use just the sufficient decrease condition to terminate the line search procedure. In its most basic form, backtracking proceeds as follows.
Algorithm 3.1 (Backtracking Line Search).
Choose α ̄ > 0, ρ ∈ (0, 1), c ∈ (0, 1); Set α ← α ̄ ; repeat until f (xk + αpk) ≤ f (xk) + cα∇ f T
k pk
α ← ρα; end (repeat)
Terminate with αk α.
In this procedure, the initial step length α ̄ is chosen to be 1 in Newton and quasiNewton methods, but can have different values in other algorithms such as steepest descent or conjugate gradient. An acceptable step length will be found after a finite number of trials, because αk will eventually become small enough that the sufficient decrease condition holds (see Figure 3.3). In practice, the contraction factor ρ is often allowed to vary at each iteration of the line search. For example, it can be chosen by safeguarded interpolation, as we describe later. We need ensure only that at each iteration we have ρ ∈ [ρlo, ρhi], for some fixed constants 0 < ρlo < ρhi < 1. The backtracking approach ensures either that the selected step length αk is some fixed value (the initial choice α ̄ ), or else that it is short enough to satisfy the sufficient decrease condition but not too short. The latter claim holds because the accepted value αk is within a factor ρ of the previous trial value, αk/ρ, which was rejected for violating the sufficient decrease condition, that is, for being too long. This simple and popular strategy for terminating a line search is well suited for Newton methods but is less appropriate for quasi-Newton and conjugate gradient methods.
3.2 CONVERGENCE OF LINE SEARCH METHODS
To obtain global convergence, we must not only have well chosen step lengths but also well chosen search directions pk. We discuss requirements on the search direction in this section, focusing on one key property: the angle θk between pk and the steepest descent direction −∇ fk, defined by
cos θk
−∇ f T
k pk
‖∇ fk‖ ‖ pk‖ . (3.12)


38 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
The following theorem, due to Zoutendijk, has far-reaching consequences. It quantifies the effect of properly chosen step lengths αk, and shows, for example, that the steepest descent method is globally convergent. For other algorithms, it describes how far pk can deviate from the steepest descent direction and still produce a globally convergent iteration. Various line search termination conditions can be used to establish this result, but for concreteness we will consider only the Wolfe conditions (3.6). Though Zoutendijk’s result appears at first to be technical and obscure, its power will soon become evident.
Theorem 3.2.
Consider any iteration of the form (3.1), where pk is a descent direction and αk satisfies the Wolfe conditions (3.6). Suppose that f is bounded below in IRn and that f is continuously
differentiable in an open set N containing the level set L def {x : f (x) ≤ f (x0)}, where x0 is the starting point of the iteration. Assume also that the gradient ∇ f is Lipschitz continuous on N , that is, there exists a constant L > 0 such that
‖∇ f (x) − ∇ f (x ̃)‖ ≤ L‖x − x ̃‖, for all x, x ̃ ∈ N . (3.13)
Then
∑
k≥0
cos2 θk ‖∇ fk‖2 < ∞. (3.14)
PROOF. From (3.6b) and (3.1) we have that
(∇ fk+1 − ∇ fk )T pk ≥ (c2 − 1)∇ f T
k pk ,
while the Lipschitz condition (3.13) implies that
(∇ fk+1 − ∇ fk )T pk ≤ αk L‖ pk ‖2.
By combining these two relations, we obtain
αk ≥ c2 − 1
L
∇ fT
k pk
‖ pk ‖2 .
By substituting this inequality into the first Wolfe condition (3.6a), we obtain
fk+1 ≤ fk − c1
1 − c2
L
(∇ f T
k pk )2
‖ pk ‖2 .
From the definition (3.12), we can write this relation as
fk+1 ≤ fk − c cos2 θk ‖∇ fk ‖2,


3 . 2 . C O N V E R G E N C E O F L I N E S E A R C H M E T H O D S 39
where c c1(1 − c2)/L. By summing this expression over all indices less than or equal to k, we obtain
fk+1 ≤ f0 − c
k ∑
j0
cos2 θ j ‖∇ f j ‖2. (3.15)
Since f is bounded below, we have that f0 − fk+1 is less than some positive constant, for all k. Hence, by taking limits in (3.15), we obtain
∑ ∞
k0
cos2 θk‖∇ fk ‖2 < ∞,
which concludes the proof.
Similar results to this theorem hold when the Goldstein conditions (3.11) or strong Wolfe conditions (3.7) are used in place of the Wolfe conditions. For all these strategies, the step length selection implies inequality (3.14), which we call the Zoutendijk condition. Note that the assumptions of Theorem 3.2 are not too restrictive. If the function f were not bounded below, the optimization problem would not be well defined. The smoothness assumption—Lipschitz continuity of the gradient—is implied by many of the smoothness conditions that are used in local convergence theorems (see Chapters 6 and 7) and are often satisfied in practice. The Zoutendijk condition (3.14) implies that
cos2 θk‖∇ fk‖2 → 0. (3.16)
This limit can be used in turn to derive global convergence results for line search algorithms. If our method for choosing the search direction pk in the iteration (3.1) ensures that the angle θk defined by (3.12) is bounded away from 90◦, there is a positive constant δ such that
cos θk ≥ δ > 0, for all k. (3.17)
It follows immediately from (3.16) that
kli→m∞ ‖∇ fk‖ 0. (3.18)
In other words, we can be sure that the gradient norms ‖∇ fk‖ converge to zero, provided that the search directions are never too close to orthogonality with the gradient. In particular, the method of steepest descent (for which the search direction pk is parallel to the negative


40 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
gradient) produces a gradient sequence that converges to zero, provided that it uses a line search satisfying the Wolfe or Goldstein conditions. We use the term globally convergent to refer to algorithms for which the property (3.18) is satisfied, but note that this term is sometimes used in other contexts to mean different things. For line search methods of the general form (3.1), the limit (3.18) is the strongest global convergence result that can be obtained: We cannot guarantee that the method converges to a minimizer, but only that it is attracted by stationary points. Only by making additional requirements on the search direction pk—by introducing negative curvature information from the Hessian ∇2 f (xk), for example—can we strengthen these results to include convergence to a local minimum. See the Notes and References at the end of this chapter for further discussion of this point. Consider now the Newton-like method (3.1), (3.2) and assume that the matrices Bk are positive definite with a uniformly bounded condition number. That is, there is a constant M such that
‖Bk ‖ ‖B−1
k ‖ ≤ M, for all k.
It is easy to show from the definition (3.12) that
cos θk ≥ 1/M (3.19)
(see Exercise 3.5). By combining this bound with (3.16) we find that
kli→m∞ ‖∇ fk‖ 0. (3.20)
Therefore, we have shown that Newton and quasi-Newton methods are globally convergent if the matrices Bk have a bounded condition number and are positive definite (which is needed to ensure that pk is a descent direction), and if the step lengths satisfy the Wolfe conditions. For some algorithms, such as conjugate gradient methods, we will be able to prove the limit (3.18), but only the weaker result
lim inf
k→∞ ‖∇ fk ‖ 0. (3.21)
In other words, just a subsequence of the gradient norms ‖∇ fkj ‖ converges to zero, rather than the whole sequence (see Appendix A). This result, too, can be proved by using Zoutendijk’s condition (3.14), but instead of a constructive proof, we outline a proof by contradiction. Suppose that (3.21) does not hold, so that the gradients remain bounded away from zero, that is, there exists γ > 0 such that
‖∇ fk‖ ≥ γ , for all k sufficiently large. (3.22)


3 . 3 . R A T E O F C O N V E R G E N C E 41
Then from (3.16) we conclude that
cos θk → 0, (3.23)
that is, the entire sequence {cos θk} converges to 0. To establish (3.21), therefore, it is enough to show that a subsequence {cos θkj } is bounded away from zero. We will use this strategy in Chapter 5 to study the convergence of nonlinear conjugate gradient methods. By applying this proof technique, we can prove global convergence in the sense of (3.20) or (3.21) for a general class of algorithms. Consider any algorithm for which (i) every iteration produces a decrease in the objective function, and (ii) every mth iteration is a steepest descent step, with step length chosen to satisfy the Wolfe or Goldstein conditions. Then, since cos θk 1 for the steepest descent steps, the result (3.21) holds. Of course, we would design the algorithm so that it does something “better" than steepest descent at the other m − 1 iterates. The occasional steepest descent steps may not make much progress, but they at least guarantee overall global convergence. Note that throughout this section we have used only the fact that Zoutendijk’s condition implies the limit (3.16). In later chapters we will make use of the bounded sum condition (3.14), which forces the sequence {cos2 θk‖∇ fk‖2} to converge to zero at a sufficiently rapid rate.
3.3 RATE OF CONVERGENCE
It would seem that designing optimization algorithms with good convergence properties is easy, since all we need to ensure is that the search direction pk does not tend to become orthogonal to the gradient ∇ fk, or that steepest descent steps are taken regularly. We could simply compute cos θk at every iteration and turn pk toward the steepest descent direction if cos θk is smaller than some preselected constant δ > 0. Angle tests of this type ensure global convergence, but they are undesirable for two reasons. First, they may impede a fast rate of convergence, because for problems with an ill-conditioned Hessian, it may be necessary to produce search directions that are almost orthogonal to the gradient, and an inappropriate choice of the parameter δ may cause such steps to be rejected. Second, angle tests destroy the invariance properties of quasi-Newton methods. Algorithmic strategies that achieve rapid convergence can sometimes conflict with the requirements of global convergence, and vice versa. For example, the steepest descent method is the quintessential globally convergent algorithm, but it is quite slow in practice, as we shall see below. On the other hand, the pure Newton iteration converges rapidly when started close enough to a solution, but its steps may not even be descent directions away from the solution. The challenge is to design algorithms that incorporate both properties: good global convergence guarantees and a rapid rate of convergence. We begin our study of convergence rates of line search methods by considering the most basic approach of all: the steepest descent method.


42 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
Figure 3.7 Steepest descent steps.
CONVERGENCE RATE OF STEEPEST DESCENT
We can learn much about the steepest descent method by considering the ideal case, in which the objective function is quadratic and the line searches are exact. Let us suppose that
f (x) 1
2 x T Qx − bT x, (3.24)
where Q is symmetric and positive definite. The gradient is given by ∇ f (x) Qx − b and the minimizer x∗ is the unique solution of the linear system Qx b. It is easy to compute the step length αk that minimizes f (xk −α∇ fk). By differentiating the function
f (xk − α∇ fk) 1
2 (xk − α∇ fk)T Q(xk − α∇ fk) − bT (xk − α∇ fk)
with respect to α, and setting the derivative to zero, we obtain
αk
∇ fT
k ∇ fk
∇ fT
k Q∇ fk
. (3.25)
If we use this exact minimizer αk, the steepest descent iteration for (3.24) is given by
xk+1 xk −
( ∇fT
k ∇ fk
∇ fT
k Q∇ fk
)
∇ fk. (3.26)
Since ∇ fk Qxk − b, this equation yields a closed-form expression for xk+1 in terms of xk. In Figure 3.7 we plot a typical sequence of iterates generated by the steepest descent method on a two-dimensional quadratic objective function. The contours of f are ellipsoids whose


3 . 3 . R A T E O F C O N V E R G E N C E 43
axes lie along the orthogonal eigenvectors of Q. Note that the iterates zigzag toward the solution. To quantify the rate of convergence we introduce the weighted norm ‖x‖2
Q xT Qx.
By using the relation Qx∗ b, we can show that
1
2 ‖x − x∗‖2
Q f (x) − f (x∗), (3.27)
so this norm measures the difference between the current objective value and the optimal value. By using the equality (3.26) and noting that ∇ fk Q(xk − x∗), we can derive the equality
‖xk+1 − x ∗‖2
Q
{
1−
(∇ f T
k ∇ fk
)2
(∇ f T
k Q∇ fk
) (∇ f T
k Q−1∇ fk
)
}
‖xk − x∗‖2
Q (3.28)
(see Exercise 3.7). This expression describes the exact decrease in f at each iteration, but since the term inside the brackets is difficult to interpret, it is more useful to bound it in terms of the condition number of the problem.
Theorem 3.3.
When the steepest descent method with exact line searches (3.26) is applied to the strongly convex quadratic function (3.24), the error norm (3.27) satisfies
‖xk+1 − x ∗‖2
Q≤
( λn − λ1 λn + λ1
)2
‖xk − x∗‖2
Q, (3.29)
where 0 < λ1 ≤ λ2 ≤ · · · ≤ λn are the eigenvalues of Q.
The proof of this result is given by Luenberger [195]. The inequalities (3.29) and (3.27) show that the function values fk converge to the minimum f∗ at a linear rate. As a special case of this result, we see that convergence is achieved in one iteration if all the eigenvalues are equal. In this case, Q is a multiple of the identity matrix, so the contours in Figure 3.7 are circles and the steepest descent direction always points at the solution. In general, as the condition number κ(Q) λn/λ1 increases, the contours of the quadratic become more elongated, the zigzagging in Figure 3.7 becomes more pronounced, and (3.29) implies that the convergence degrades. Even though (3.29) is a worst-case bound, it gives an accurate indication of the behavior of the algorithm when n > 2. The rate-of-convergence behavior of the steepest descent method is essentially the same on general nonlinear objective functions. In the following result we assume that the step length is the global minimizer along the search direction.
Theorem 3.4.
Suppose that f : IRn → IR is twice continuously differentiable, and that the iterates generated by the steepest-descent method with exact line searches converge to a point x∗ at


44 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
which the Hessian matrix ∇2 f (x∗) is positive definite. Let r be any scalar satisfying
r∈
( λn − λ1 λn + λ1
,1
) ,
where λ1 ≤ λ2 ≤ · · · ≤ λn are the eigenvalues of ∇2 f (x∗). Then for all k sufficiently large, we have
f (xk+1) − f (x∗) ≤ r 2[ f (xk) − f (x∗)].
In general, we cannot expect the rate of convergence to improve if an inexact line search is used. Therefore, Theorem 3.4 shows that the steepest descent method can have an unacceptably slow rate of convergence, even when the Hessian is reasonably well conditioned. For example, if κ(Q) 800, f (x1) 1, and f (x∗) 0, Theorem 3.4 suggests that the function value will still be about 0.08 after one thousand iterations of the steepest descent method with exact line search.
NEWTON’S METHOD
We now consider the Newton iteration, for which the search is given by
pN
k −∇2 f −1
k ∇ fk. (3.30)
Since the Hessian matrix ∇2 fk may not always be positive definite, pN
k may not always be a descent direction, and many of the ideas discussed so far in this chapter no longer apply. In Section 3.4 and Chapter 4 we will describe two approaches for obtaining a globally convergent iteration based on the Newton step: a line search approach, in which the Hessian ∇2 fk is modified, if necessary, to make it positive definite and thereby yield descent, and a trust region approach, in which ∇2 fk is used to form a quadratic model that is minimized in a ball around the current iterate xk. Here we discuss just the local rate-of-convergence properties of Newton’s method. We know that for all x in the vicinity of a solution point x∗ such that ∇2 f (x∗) is positive definite, the Hessian ∇2 f (x) will also be positive definite. Newton’s method will be well defined in this region and will converge quadratically, provided that the step lengths αk are eventually always 1.
Theorem 3.5.
Suppose that f is twice differentiable and that the Hessian ∇2 f (x) is Lipschitz continuous (see (A.42)) in a neighborhood of a solution x∗ at which the sufficient conditions (Theorem 2.4) are satisfied. Consider the iteration xk+1 xk + pk, where pk is given by (3.30). Then
(i) if the starting point x0 is sufficiently close to x∗, the sequence of iterates converges to x∗;
(ii) the rate of convergence of {xk} is quadratic; and
(iii) the sequence of gradient norms {‖∇ fk‖} converges quadratically to zero.


3 . 3 . R A T E O F C O N V E R G E N C E 45
PROOF. From the definition of the Newton step and the optimality condition ∇ f∗ 0 we have that
xk + pN
k − x ∗ xk − x ∗ − ∇2 f −1
k ∇ fk
∇2 f −1
k
[∇2 fk(xk − x∗) − (∇ fk − ∇ f∗)] . (3.31)
Since Taylor’s theorem (Theorem 2.1) tells us that
∇ fk − ∇ f∗
∫1
0
∇2 f (xk + t(x∗ − xk))(xk − x∗) dt,
we have
∥∥∇2 f (xk)(xk − x∗) − (∇ fk − ∇ f (x∗))∥∥
∥∥∥∥
∫1
0
[∇2 f (xk) − ∇2 f (xk + t(x∗ − xk))] (xk − x∗) dt
∥∥∥∥
≤
∫1
0
∥∥∇2 f (xk) − ∇2 f (xk + t(x∗ − xk))∥∥ ‖xk − x∗‖ dt
≤ ‖xk − x∗‖2
∫1
0
Lt dt 1
2 L‖xk − x∗‖2, (3.32)
where L is the Lipschitz constant for ∇2 f (x) for x near x∗. Since ∇2 f (x∗) is nonsingular, there is a radius r > 0 such that ‖∇2 f −1
k ‖ ≤ 2‖∇2 f (x∗)−1‖ for all xk with ‖xk − x∗‖ ≤ r . By substituting in (3.31) and (3.32), we obtain
‖xk + pN
k − x∗‖ ≤ L‖∇2 f (x∗)−1‖‖xk − x∗‖2 L ̃ ‖xk − x∗‖2, (3.33)
where L ̃ L‖∇2 f (x∗)−1‖. Choosing x0 so that ‖x0 − x∗‖ ≤ min(r, 1/(2L ̃ )), we can use this inequality inductively to deduce that the sequence converges to x∗, and the rate of convergence is quadratic. By using the relations xk+1 − xk pN
k and ∇ fk + ∇2 fk pN
k 0, we obtain that
‖∇ f (xk+1)‖ ‖∇ f (xk+1) − ∇ fk − ∇2 f (xk ) pN
k‖
∥∥∥∥
∫1
0
∇2 f (xk + t pN
k )(xk+1 − xk ) dt − ∇2 f (xk ) pN
k
∥∥∥∥
≤
∫1
0
∥∥∇2 f (xk + t pN
k ) − ∇2 f (xk )∥∥ ‖ pN
k ‖ dt
≤1
2 L‖pN
k ‖2
≤1
2 L‖∇2 f (xk )−1‖2‖∇ fk ‖2
≤ 2L‖∇2 f (x∗)−1‖2‖∇ fk ‖2,
proving that the gradient norms converge to zero quadratically.


46 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
As the iterates generated by Newton’s method approach the solution, the Wolfe (or Goldstein) conditions will accept the step length αk 1 for all large k. This observation follows from Theorem 3.6 below. Indeed, when the search direction is given by Newton’s method, the limit (3.35) is satisfied—the ratio is zero for all k! Implementations of Newton’s method using these line search conditions, and in which the line search always tries the unit step length first, will set αk 1 for all large k and attain a local quadratic rate of convergence.
QUASI-NEWTON METHODS
Suppose now that the search direction has the form
pk −B−1
k ∇ fk, (3.34)
where the symmetric and positive definite matrix Bk is updated at every iteration by a quasi-Newton updating formula. We already encountered one quasi-Newton formula, the BFGS formula, in Chapter 2; others will be discussed in Chapter 6. We assume here that the step length αk is computed by an inexact line search that satisfies the Wolfe or strong Wolfe conditions, with the same proviso mentioned above for Newton’s method: The line search algorithm will always try the step length α 1 first, and will accept this value if it satisfies the Wolfe conditions. (We could enforce this condition by setting α ̄ 1 in Algorithm 3.1, for example.) This implementation detail turns out to be crucial in obtaining a fast rate of convergence. The following result shows that if the search direction of a quasi-Newton method approximates the Newton direction well enough, then the unit step length will satisfy the Wolfe conditions as the iterates converge to the solution. It also specifies a condition that the search direction must satisfy in order to give rise to a superlinearly convergent iteration. To bring out the full generality of this result, we state it first in terms of a general descent iteration, and then examine its consequences for quasi-Newton and Newton methods.
Theorem 3.6.
Suppose that f : IRn → IR is twice continuously differentiable. Consider the iteration xk+1 xk + αk pk, where pk is a descent direction and αk satisfies the Wolfe conditions (3.6) with c1 ≤ 1/2. If the sequence {xk} converges to a point x∗ such that ∇ f (x∗) 0 and ∇2 f (x∗) is positive definite, and if the search direction satisfies
kli→m∞
‖∇ fk + ∇2 fk pk ‖
‖ pk‖ 0, (3.35)
then
(i) the step length αk 1 is admissible for all k greater than a certain index k0; and
(ii) if αk 1 for all k > k0, {xk} converges to x∗ superlinearly.


3 . 3 . R A T E O F C O N V E R G E N C E 47
It is easy to see that if c1 > 1/2, then the line search would exclude the minimizer of a quadratic, and unit step lengths may not be admissible. If pk is a quasi-Newton search direction of the form (3.34), then (3.35) is equivalent to
kli→m∞
‖(Bk − ∇2 f (x∗)) pk‖
‖ pk‖ 0. (3.36)
Hence, we have the surprising (and delightful) result that a superlinear convergence rate can be attained even if the sequence of quasi-Newton matrices Bk does not converge to ∇2 f (x∗); it suffices that the Bk become increasingly accurate approximations to ∇2 f (x∗) along the search directions pk. Importantly, condition (3.36) is both necessary and sufficient for the superlinear convergence of quasi-Newton methods.
Theorem 3.7.
Suppose that f : IRn → IR is twice continuously differentiable. Consider the iteration xk+1 xk + pk (that is, the step length αk is uniformly 1) and that pk is given by (3.34). Let us assume also that {xk} converges to a point x∗ such that ∇ f (x∗) 0 and ∇2 f (x∗) is positive definite. Then {xk} converges superlinearly if and only if (3.36) holds.
PROOF. We first show that (3.36) is equivalent to
pk − pN
k o(‖ pk‖), (3.37)
where pN
k −∇2 f −1
k ∇ fk is the Newton step. Assuming that (3.36) holds, we have that
pk − pN
k ∇2 f −1
k (∇2 fk pk + ∇ fk )
∇2 f −1
k (∇2 fk − Bk ) pk
O(‖(∇2 fk − Bk) pk‖)
o(‖ pk‖),
where we have used the fact that ‖∇2 f −1
k ‖ is bounded above for xk sufficiently close to x∗,
since the limiting Hessian ∇2 f (x∗) is positive definite. The converse follows readily if we multiply both sides of (3.37) by ∇2 fk and recall (3.34). By combining (3.33) and (3.37), we obtain that
‖xk + pk − x∗‖ ≤ ‖xk + pN
k − x∗‖ + ‖ pk − pN
k ‖ O(‖xk − x∗‖2) + o(‖ pk‖).
A simple manipulation of this inequality reveals that ‖ pk‖ O(‖xk − x∗‖), so we obtain
‖xk + pk − x∗‖ ≤ o(‖xk − x∗‖),
giving the superlinear convergence result.


48 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
We will see in Chapter 6 that quasi-Newton methods normally satisfy condition (3.36) and are therefore superlinearly convergent.
3.4 NEWTON’S METHOD WITH HESSIAN MODIFICATION
Away from the solution, the Hessian matrix ∇2 f (x) may not be positive definite, so the Newton direction pN
k defined by
∇2 f (xk)pN
k −∇ f (xk) (3.38)
(see (3.30)) may not be a descent direction. We now describe an approach to overcome this difficulty when a direct linear algebra technique, such as Gaussian elimination, is used to solve the Newton equations (3.38). This approach obtains the step pk from a linear system identical to (3.38), except that the coefficient matrix is replaced with a positive definite approximation, formed before or during the solution process. The modified Hessian is obtained by adding either a positive diagonal matrix or a full matrix to the true Hessian ∇2 f (xk). A general description of this method follows.
Algorithm 3.2 (Line Search Newton with Modification). Given initial point x0; for k 0, 1, 2, . . .
Factorize the matrix Bk ∇2 f (xk) + Ek, where Ek 0 if ∇2 f (xk) is sufficiently positive definite; otherwise, Ek is chosen to ensure that Bk is sufficiently positive definite; Solve Bk pk −∇ f (xk); Set xk+1 ← xk + αk pk, where αk satisfies the Wolfe, Goldstein, or Armijo backtracking conditions; end
Some approaches do not compute Ek explicitly, but rather introduce extra steps and tests into standard factorization procedures, modifying these procedures “on the fly” so that the computed factors are the factors of a positive definite matrix. Strategies based on modifying a Cholesky factorization and on modifying a symmetric indefinite factorization of the Hessian are described in this section. Algorithm 3.2 is a practical Newton method that can be applied from any starting point. We can establish fairly satisfactory global convergence results for it, provided that the strategy for choosing Ek (and hence Bk) satisfies the bounded modified factorization property. This property is that the matrices in the sequence {Bk} have bounded condition number whenever the sequence of Hessians {∇2 f (xk)} is bounded; that is,
κ(Bk) ‖Bk ‖ ‖B−1
k ‖ ≤ C, some C > 0 and all k 0, 1, 2, . . . . (3.39)


3 . 4 . N E W T O N ’ S M E T H O D W I T H H E S S I A N M O D I F I C A T I O N 49
If this property holds, global convergence of the modified line search Newton method follows from the results of Section 3.2.
Theorem 3.8.
Let f be twice continuously differentiable on an open set D, and assume that the starting point x0 of Algorithm 3.2 is such that the level set L {x ∈ D : f (x) ≤ f (x0)} is compact. Then if the bounded modified factorization property holds, we have that
kli→m∞ ∇ f (xk ) 0.
For a proof this result see [215]. We now consider the convergence rate of Algorithm 3.2. Suppose that the sequence of iterates xk converges to a point x∗ where ∇2 f (x∗) is sufficiently positive definite in the sense that the modification strategies described in the next section return the modification Ek 0 for all sufficiently large k. By Theorem 3.6, we have that αk 1 for all sufficiently large k, so that Algorithm 3.2 reduces to a pure Newton method, and the rate of convergence is quadratic. For problems in which ∇ f ∗ is close to singular, there is no guarantee that the modification Ek will eventually vanish, and the convergence rate may be only linear. Besides requiring the modified matrix Bk to be well conditioned (so that Theorem 3.8 holds), we would like the modification to be as small as possible, so that the second-order information in the Hessian is preserved as far as possible. Naturally, we would also like the modified factorization to be computable at moderate cost. To set the stage for the matrix factorization techniques that will be used in Algorithm 3.2, we will begin by assuming that the eigenvalue decomposition of ∇2 f (xk) is available. This is not realistic for large-scale problems because this decomposition is generally too expensive to compute, but it will motivate several practical modification strategies.
EIGENVALUE MODIFICATION
Consider a problem in which, at the current iterate xk, ∇ f (xk) (1, −3, 2)T and ∇2 f (xk) diag(10, 3, −1), which is clearly indefinite. By the spectral decomposition theorem (see Appendix A) we can define Q I and diag(λ1, λ2, λ3), and write
∇2 f (xk) Q QT
n ∑
i1
λi qi q T
i . (3.40)
The pure Newton step—the solution of (3.38)—is pN
k (−0.1, 1, 2)T , which is not a de
scent direction, since ∇ f (xk)T pN
k > 0. One might suggest a modified strategy in which we replace ∇2 f (xk) by a positive definite approximation Bk, in which all negative eigenvalues in ∇2 f (xk) are replaced by a small positive number δ that is somewhat larger than ma
chine precision u; say δ √u. For a machine precision of 10−16, the resulting matrix in


50 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
our example is
Bk
2 ∑
i1
λi qi q T
i + δq3qT
3 diag (10, 3, 10−8) , (3.41)
which is numerically positive definite and whose curvature along the eigenvectors q1 and q2 has been preserved. Note, however, that the search direction based on this modified Hessian is
pk −B−1
k ∇ fk −
∑2
i1
1
λi
qi
(q T
i ∇ fk
)− 1
δ q3
(q T
3 ∇ f (xk))
≈ −(2 × 108)q3. (3.42)
For small δ, this step is nearly parallel to q3 (with relatively small contributions from q1 and q2) and quite long. Although f decreases along the direction pk, its extreme length violates the spirit of Newton’s method, which relies on a quadratic approximation of the objective function that is valid in a neighborhood of the current iterate xk. It is therefore not clear that this search direction is effective. Various other modification strategies are possible. We could flip the signs of the negative eigenvalues in (3.40), which amounts to setting δ 1 in our example. We could set the last term in (3.42) to zero, so that the search direction has no components along the negative curvature directions. We could adapt the choice of δ to ensure that the length of the step is not excessive, a strategy that has the flavor of trust-region methods. As this discussion shows, there is a great deal of freedom in devising modification strategies, and there is currently no agreement on which strategy is best. Setting the issue of the choice of δ aside for the moment, let us look more closely at the process of modifying a matrix so that it becomes positive definite. The modification (3.41) to the example matrix (3.40) can be shown to be optimal in the following sense. If A is a symmetric matrix with spectral decomposition A Q QT , then the correction matrix A of minimum Frobenius norm that ensures that λmin( A + A) ≥ δ is given by
A Q diag (τi )QT , with τi
{
0, λi ≥ δ,
δ − λi , λi < δ. (3.43)
Here, λmin( A) denotes the smallest eigenvalue of A, and the Frobenius norm of a matrix is defined as ‖ A‖2
F
∑n
i, j 1 a2
i j (see (A.9)). Note that A is not diagonal in general, and that the modified matrix is given by
A + A Q( + diag(τi ))QT .
By using a different norm we can obtain a diagonal modification. Suppose again that A is a symmetric matrix with spectral decomposition A Q QT . A correction matrix


3 . 4 . N E W T O N ’ S M E T H O D W I T H H E S S I A N M O D I F I C A T I O N 51
A with minimum Euclidean norm that satisfies λmin( A + A) ≥ δ is given by
A τ I, with τ max(0, δ − λmin( A)). (3.44)
The modified matrix now has the form
A + τ I, (3.45)
which happens to have the same form as the matrix occurring in (unscaled) trust–region methods (see Chapter 4). All the eigenvalues of (3.45) have thus been shifted, and all are greater than δ. These results suggest that both diagonal and nondiagonal modifications can be considered. Even though we have not answered the question of what constitutes a good modification, various practical diagonal and nondiagonal modifications have been proposed and implemented in software. They do not make use of the spectral decomposition of the Hessian, since it is generally too expensive to compute. Instead, they use Gaussian elimination, choosing the modifications indirectly and hoping that somehow they will produce good steps. Numerical experience indicates that the strategies described next often (but not always) produce good search directions.
ADDING A MULTIPLE OF THE IDENTITY
Perhaps the simplest idea is to find a scalar τ > 0 such that ∇2 f (xk) + τ I is sufficiently positive definite. From the previous discussion we know that τ must satisfy (3.44), but a good estimate of the smallest eigenvalue of the Hessian is normally not available. The following algorithm describes a method that tries successively larger values of τ . (Here, aii denotes a diagonal element of A.)
Algorithm 3.3 (Cholesky with Added Multiple of the Identity). Choose β > 0; if mini aii > 0
set τ0 ← 0; else
τ0 − min(aii ) + β; end (if)
for k 0, 1, 2, . . .
Attempt to apply the Cholesky algorithm to obtain L LT A + τk I ; if the factorization is completed successfully stop and return L; else
τk+1 ← max(2τk, β); end (if) end (for)


52 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
The choice of β is heuristic; a typical value is β 10−3. We could choose the first nonzero shift τ0 to be proportional to be the final value of τ used in the latest Hessian modification; see also Algorithm B.1. The strategy implemented in Algorithm 3.3 is quite simple and may be preferable to the modified factorization techniques described next, but it suffers from one drawback. Every value of τk requires a new factorization of A + τk I , and the algorithm can be quite expensive if several trial values are generated. Therefore it may be advantageous to increase τ more rapidly, say by a factor of 10 instead of 2 in the last else clause.
MODIFIED CHOLESKY FACTORIZATION
Another approach for modifying a Hessian matrix that is not positive definite is to perform a Cholesky factorization of ∇2 f (xk), but to increase the diagonal elements encountered during the factorization (where necessary) to ensure that they are sufficiently positive. This modified Cholesky approach is designed to accomplish two goals: It guarantees that the modified Cholesky factors exist and are bounded relative to the norm of the actual Hessian, and it does not modify the Hessian if it is sufficiently positive definite. We begin our description of this approach by briefly reviewing the Cholesky factorization. Every symmetric positive definite matrix A can be written as
A L D LT , (3.46)
where L is a lower triangular matrix with unit diagonal elements and D is a diagonal matrix with positive elements on the diagonal. By equating the elements in (3.46), column by column, it is easy to derive formulas for computing L and D.
❏ EXAMPLE 3.1
Consider the case n 3. The equation A L DLT is given by
⎡
⎢⎣
a11 a21 a31
a21 a22 a32
a31 a32 a33
⎤
⎥⎦
⎡
⎢⎢⎣
1 00
l21 1 0
l31 l32 1
⎤
⎥⎥⎦
⎡
⎢⎣
d1 0 0
0 d2 0
0 0 d3
⎤
⎥⎦
⎡
⎢⎣
1 l21 l31
0 1 l32
00 1
⎤
⎥⎦ .
(The notation indicates that A is symmetric.) By equating the elements of the first column, we have
a11 d1,
a21 d1l21 ⇒ l21 a21/d1,
a31 d1l31 ⇒ l31 a31/d1.


3 . 4 . N E W T O N ’ S M E T H O D W I T H H E S S I A N M O D I F I C A T I O N 53
Proceeding with the next two columns, we obtain
a22 d1l2
21 + d2 ⇒ d2 a22 − d1l2
21,
a32 d1l31l21 + d2l32 ⇒ l32 (a32 − d1l31l21) /d2,
a33 d1l2
31 + d2l2
32 + d3 ⇒ d3 a33 − d1l2
31 − d2l2
32.
❐
This procedure is generalized in the following algorithm.
Algorithm 3.4 (Cholesky Factorization, L DLT Form). for j 1, 2, . . . , n
c j j ← a j j − ∑ j−1
s 1 dsl2
js;
dj ← cjj;
for i j + 1, . . . , n
ci j ← ai j − ∑ j−1
s 1 dslisl js ;
li j ← ci j /d j ; end end
One can show (see, for example, Golub and Van Loan [136, Section 4.2.3]) that the diagonal elements d j j are all positive whenever A is positive definite. The scalars ci j have been introduced only to facilitate the description of the modified factorization discussed below. We should note that Algorithm 3.4 differs a little from the standard form of the Cholesky factorization, which produces a lower triangular matrix M such that
A M M T . (3.47)
In fact, we can make the identification M L D1/2 to relate M to the factors L and D computed in Algorithm 3.4. The technique for computing M appears as Algorithm A.2 in Appendix A. If A is indefinite, the factorization A L DLT may not exist. Even if it does exist, Algorithm 3.4 is numerically unstable when applied to such matrices, in the sense that the elements of L and D can become arbitrarily large. It follows that a strategy of computing the L DLT factorization and then modifying the diagonal after the fact to force its elements to be positive may break down, or may result in a matrix that is drastically different from A. Instead, we can modify the matrix A during the course of the factorization in such a way that all elements in D are sufficiently positive, and so that the elements of D and L are not too large. To control the quality of the modification, we choose two positive parameters δ and β, and require that during the computation of the jth columns of L and D in Algorithm 3.4 (that is, for each j in the outer loop of the algorithm) the following


54 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
bounds be satisfied:
d j ≥ δ, |mi j | ≤ β, i j + 1, j + 2, . . . , n, (3.48)
where mi j li j
√d j . To satisfy these bounds we only need to change one step in Algorithm 3.4: The formula for computing the diagonal element d j in Algorithm 3.4 is replaced by
d j max
(
|c j j |,
(θj β
)2
,δ
)
, with θ j max
j<i≤n |ci j |. (3.49)
To verify that (3.48) holds, we note from Algorithm 3.4 that ci j li j d j , and therefore
|mi j | |li j
√d j | |ci j |
√d j
≤ |ci j |β
θj
≤ β, for all i > j.
We note that θ j can be computed prior to d j because the elements ci j in the second for loop of Algorithm 3.4 do not involve d j . In fact, this is the reason for introducing the quantities ci j into the algorithm. These observations are the basis of the modified Cholesky algorithm described in detail in Gill, Murray, and Wright [130], which introduces symmetric interchanges of rows and columns to try to reduce the size of the modification. If P denotes the permutation matrix associated with the row and column interchanges, the algorithm produces the Cholesky factorization of the permuted, modified matrix P A PT + E, that is,
P A P T + E L D LT M M T , (3.50)
where E is a nonnegative diagonal matrix that is zero if A is sufficiently positive definite. One can show (Mor ́e and Sorensen [215]) that the matrices Bk obtained by this modified Cholesky algorithm to the exact Hessians ∇2 f (xk) have bounded condition numbers, that is, the bound (3.39) holds for some value of C.
MODIFIED SYMMETRIC INDEFINITE FACTORIZATION
Another strategy for modifying an indefinite Hessian is to use a procedure based on a symmetric indefinite factorization. Any symmetric matrix A, whether positive definite or not, can be written as
P A P T L B LT , (3.51)
where L is unit lower triangular, B is a block diagonal matrix with blocks of dimension 1 or 2, and P is a permutation matrix (see our discussion in Appendix A and also Golub and


3 . 4 . N E W T O N ’ S M E T H O D W I T H H E S S I A N M O D I F I C A T I O N 55
Van Loan [136, Section 4.4]). We mentioned earlier that attempting to compute the L DLT factorization of an indefinite matrix (where D is a diagonal matrix) is inadvisable because even if the factors L and D are well defined, they may contain entries that are larger than the original elements of A, thus amplifying rounding errors that arise during the computation. However, by using the block diagonal matrix B, which allows 2 × 2 blocks as well as 1 × 1 blocks on the diagonal, we can guarantee that the factorization (3.51) always exists and can be computed by a numerically stable process.
❏ EXAMPLE 3.2
The matrix
A
⎡
⎢⎢⎢⎢⎣
0123
1222
2233
3234
⎤
⎥⎥⎥⎥⎦
can be written in the form (3.51) with P [e1, e4, e3, e2],
L
⎡
⎢⎢⎢⎢⎢⎢⎣
1 0 00
0 1 00 1
9
2
3 10
2
9
1
3 01
⎤
⎥⎥⎥⎥⎥⎥⎦
,B
⎡
⎢⎢⎢⎢⎢⎢⎣
03 0 0
34 0 0
00 7
9
5
9
00 5
9
10
9
⎤
⎥⎥⎥⎥⎥⎥⎦
. (3.52)
Note that both diagonal blocks in B are 2 × 2. Several algorithms for computing symmetric indefinite factorizations are discussed in Section A.1 of Appendix A. ❐
The symmetric indefinite factorization allows us to determine the inertia of a matrix, that is, the number of positive, zero, and negative eigenvalues. One can show that the inertia of B equals the inertia of A. Moreover, the 2 × 2 blocks in B are always constructed to have one positive and one negative eigenvalue. Thus the number of positive eigenvalues in A equals the number of positive 1 × 1 blocks plus the number of 2 × 2 blocks. As for the Cholesky factorization, an indefinite symmetric factorization algorithm can be modified to ensure that the modified factors are the factors of a positive definite matrix. The strategy is first to compute the factorization (3.51), as well as the spectral decomposition B Q QT , which is inexpensive to compute because B is block diagonal


56 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
(see Exercise 3.12). We then construct a modification matrix F such that
L(B + F)LT
is sufficiently positive definite. Motivated by the modified spectral decomposition (3.43), we choose a parameter δ > 0 and define F to be
F Q diag(τi ) QT , τi
{
0, λi ≥ δ,
δ − λi , λi < δ, i 1, 2, . . . , n, (3.53)
where λi are the eigenvalues of B. The matrix F is thus the modification of minimum Frobenius norm that ensures that all eigenvalues of the modified matrix B + F are no less than δ. This strategy therefore modifies the factorization (3.51) as follows:
P( A + E)P T L(B + F)LT , where E P T L F LT P.
(Note that E will not be diagonal, in general.) Hence, in contrast to the modified Cholesky approach, this modification strategy changes the entire matrix A, not just its diagonal. The aim of strategy (3.53) is that the modified matrix satisfies λmin( A + E) ≈ δ whenever the original matrix A has λmin( A) < δ. It is not clear, however, whether it always comes close to attaining this goal.
3.5 STEP-LENGTH SELECTION ALGORITHMS
We now consider techniques for finding a minimum of the one-dimensional function
φ(α) f (xk + αpk), (3.54)
or for simply finding a step length αk satisfying one of the termination conditions described in Section 3.1. We assume that pk is a descent direction—that is, φ′(0) < 0—so that our search can be confined to positive values of α. If f is a convex quadratic, f (x) 1
2 x T Qx − bT x, its one-dimensional minimizer along the ray xk + αpk can be computed analytically and is given by
αk − ∇ f T
k pk pT
k Q pk
. (3.55)
For general nonlinear functions, it is necessary to use an iterative procedure. The line search procedure deserves particular attention because it has a major impact on the robustness and efficiency of all nonlinear optimization methods.


3 . 5 . S T E P - L E N G T H S E L E C T I O N A L G O R I T H M S 57
Line search procedures can be classified according to the type of derivative information they use. Algorithms that use only function values can be inefficient since, to be theoretically sound, they need to continue iterating until the search for the minimizer is narrowed down to a small interval. In contrast, knowledge of gradient information allows us to determine whether a suitable step length has been located, as stipulated, for example, by the Wolfe conditions (3.6) or Goldstein conditions (3.11). Often, particularly when xk is close to the solution, the very first choice of α satisfies these conditions, so the line search need not be invoked at all. In the rest of this section, we discuss only algorithms that make use of derivative information. More information on derivative-free procedures is given in the notes at the end of this chapter. All line search procedures require an initial estimate α0 and generate a sequence {αi } that either terminates with a step length satisfying the conditions specified by the user (for example, the Wolfe conditions) or determines that such a step length does not exist. Typical procedures consist of two phases: a bracketing phase that finds an interval [a ̄, b ̄] containing acceptable step lengths, and a selection phase that zooms in to locate the final step length. The selection phase usually reduces the bracketing interval during its search for the desired step length and interpolates some of the function and derivative information gathered on earlier steps to guess the location of the minimizer. We first discuss how to perform this interpolation. In the following discussion we let αk and αk−1 denote the step lengths used at iterations k and k − 1 of the optimization algorithm, respectively. On the other hand, we denote the trial step lengths generated during the line search by αi and αi−1 and also α j . We use α0 to denote the initial guess.
INTERPOLATION
We begin by describing a line search procedure based on interpolation of known function and derivative values of the function φ. This procedure can be viewed as an enhancement of Algorithm 3.1. The aim is to find a value of α that satisfies the sufficient decrease condition (3.6a), without being “too small.” Accordingly, the procedures here generate a decreasing sequence of values αi such that each value αi is not too much smaller than its predecessor αi−1. Note that we can write the sufficient decrease condition in the notation of (3.54) as
φ(αk) ≤ φ(0) + c1αkφ′(0), (3.56)
and that since the constant c1 is usually chosen to be small in practice (c1 10−4, say), this condition asks for little more than descent in f . We design the procedure to be “efficient” in the sense that it computes the derivative ∇ f (x) as few times as possible. Suppose that the initial guess α0 is given. If we have
φ(α0) ≤ φ(0) + c1α0φ′(0),


58 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
this step length satisfies the condition, and we terminate the search. Otherwise, we know that the interval [0, α0] contains acceptable step lengths (see Figure 3.3). We form a quadratic approximation φq (α) to φ by interpolating the three pieces of information available—φ(0), φ′(0), and φ(α0)—to obtain
φq (α)
( φ(α0) − φ(0) − α0φ′(0) α2
0
)
α2 + φ′(0)α + φ(0). (3.57)
(Note that this function is constructed so that it satisfies the interpolation conditions φq (0) φ(0), φ′
q (0) φ′(0), and φq (α0) φ(α0).) The new trial value α1 is defined as the minimizer of this quadratic, that is, we obtain
α1 − φ′(0)α2
0
2 [φ(α0) − φ(0) − φ′(0)α0] . (3.58)
If the sufficient decrease condition (3.56) is satisfied at α1, we terminate the search. Otherwise, we construct a cubic function that interpolates the four pieces of information φ(0), φ′(0), φ(α0), and φ(α1), obtaining
φc(α) aα3 + bα2 + αφ′(0) + φ(0),
where
[ a
b
] 1 α2
0α2
1 (α1 − α0)
[ α2
0 −α2
1
−α3
0 α3
1
] [ φ(α1) − φ(0) − φ′(0)α1
φ(α0) − φ(0) − φ′(0)α0
]
.
By differentiating φc(x), we see that the minimizer α2 of φc lies in the interval [0, α1] and is given by
α2
−b + √b2 − 3aφ′(0)
3a .
If necessary, this process is repeated, using a cubic interpolant of φ(0), φ′(0) and the two most recent values of φ, until an α that satisfies (3.56) is located. If any αi is either too close to its predecessor αi−1 or else too much smaller than αi−1, we reset αi αi−1/2. This safeguard procedure ensures that we make reasonable progress on each iteration and that the final α is not too small. The strategy just described assumes that derivative values are significantly more expensive to compute than function values. It is often possible, however, to compute the directional derivative simultaneously with the function, at little additional cost; see Chapter 8. Accordingly, we can design an alternative strategy based on cubic interpolation of the values of φ and φ′ at the two most recent values of α.


3 . 5 . S T E P - L E N G T H S E L E C T I O N A L G O R I T H M S 59
Cubic interpolation provides a good model for functions with significant changes of curvature. Suppose we have an interval [a ̄, b ̄] known to contain desirable step lengths, and two previous step length estimates αi−1 and αi in this interval. We use a cubic function to interpolate φ(αi−1), φ′(αi−1), φ(αi ), and φ′(αi ). (This cubic function always exists and is unique; see, for example, Bulirsch and Stoer [41, p. 52].) The minimizer of this cubic in [a ̄, b ̄] is either at one of the endpoints or else in the interior, in which case it is given by
αi+1 αi − (αi − αi−1)
[ φ′(αi ) + d2 − d1
φ′(αi ) − φ′(αi−1) + 2d2
]
, (3.59)
with
d1 φ′(αi−1) + φ′(αi ) − 3 φ(αi−1) − φ(αi )
αi−1 − αi
,
d2 sign(αi − αi−1) [d2
1 − φ′(αi−1)φ′(αi )]1/2 .
The interpolation process can be repeated by discarding the data at one of the step lengths αi−1 or αi and replacing it by φ(αi+1) and φ′(αi+1). The decision on which of αi−1 and αi should be kept and which discarded depends on the specific conditions used to terminate the line search; we discuss this issue further below in the context of the Wolfe conditions. Cubic interpolation is a powerful strategy, since it usually produces a quadratic rate of convergence of the iteration (3.59) to the minimizing value of α.
INITIAL STEP LENGTH
For Newton and quasi-Newton methods, the step α0 1 should always be used as the initial trial step length. This choice ensures that unit step lengths are taken whenever they satisfy the termination conditions and allows the rapid rate-of-convergence properties of these methods to take effect. For methods that do not produce well scaled search directions, such as the steepest descent and conjugate gradient methods, it is important to use current information about the problem and the algorithm to make the initial guess. A popular strategy is to assume that the first-order change in the function at iterate xk will be the same as that obtained at the previous step. In other words, we choose the initial guess α0 so that α0∇ f T
k pk αk−1∇ f T
k−1 pk−1,
that is,
α0 αk−1
∇ fT
k−1 pk−1
∇ fT
k pk
.
Another useful strategy is to interpolate a quadratic to the data f (xk−1), f (xk), and ∇ fT
k−1 pk−1 and to define α0 to be its minimizer. This strategy yields
α0
2( fk − fk−1)
φ′(0) . (3.60)


60 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
It can be shown that if xk → x∗ superlinearly, then the ratio in this expression converges to 1. If we adjust the choice (3.60) by setting
α0 ← min(1, 1.01α0),
we find that the unit step length α0 1 will eventually always be tried and accepted, and the superlinear convergence properties of Newton and quasi-Newton methods will be observed.
A LINE SEARCH ALGORITHM FOR THE WOLFE CONDITIONS
The Wolfe (or strong Wolfe) conditions are among the most widely applicable and useful termination conditions. We now describe in some detail a one-dimensional search procedure that is guaranteed to find a step length satisfying the strong Wolfe conditions (3.7) for any parameters c1 and c2 satisfying 0 < c1 < c2 < 1. As before, we assume that p is a descent direction and that f is bounded below along the direction p. The algorithm has two stages. This first stage begins with a trial estimate α1, and keeps increasing it until it finds either an acceptable step length or an interval that brackets the desired step lengths. In the latter case, the second stage is invoked by calling a function called zoom (Algorithm 3.6, below), which successively decreases the size of the interval until an acceptable step length is identified. A formal specification of the line search algorithm follows. We refer to (3.7a) as the sufficient decrease condition and to (3.7b) as the curvature condition. The parameter αmax is a user-supplied bound on the maximum step length allowed. The line search algorithm terminates with α∗ set to a step length that satisfies the strong Wolfe conditions.
Algorithm 3.5 (Line Search Algorithm).
Set α0 ← 0, choose αmax > 0 and α1 ∈ (0, αmax); i ← 1; repeat
Evaluate φ(αi ); if φ(αi ) > φ(0) + c1αi φ′(0) or [φ(αi ) ≥ φ(αi−1) and i > 1] α∗ ←zoom(αi−1, αi ) and stop; Evaluate φ′(αi );
if |φ′(αi )| ≤ −c2φ′(0)
set α∗ ← αi and stop; if φ′(αi ) ≥ 0 set α∗ ←zoom(αi , αi−1) and stop; Choose αi+1 ∈ (αi , αmax); i ← i + 1; end (repeat)


3 . 5 . S T E P - L E N G T H S E L E C T I O N A L G O R I T H M S 61
Note that the sequence of trial step lengths {αi } is monotonically increasing, but that the order of the arguments supplied to the zoom function may vary. The procedure uses the knowledge that the interval (αi−1, αi ) contains step lengths satisfying the strong Wolfe conditions if one of the following three conditions is satisfied:
(i) αi violates the sufficient decrease condition;
(ii) φ(αi ) ≥ φ(αi−1);
(iii) φ′(αi ) ≥ 0.
The last step of the algorithm performs extrapolation to find the next trial value αi+1. To implement this step we can use approaches like the interpolation procedures above, or we can simply set αi+1 to some constant multiple of αi . Whichever strategy we use, it is important that the successive steps increase quickly enough to reach the upper limit αmax in a finite number of iterations. We now specify the function zoom, which requires a little explanation. The order of its input arguments is such that each call has the form zoom(αlo, αhi), where
(a) the interval bounded by αlo and αhi contains step lengths that satisfy the strong Wolfe conditions;
(b) αlo is, among all step lengths generated so far and satisfying the sufficient decrease condition, the one giving the smallest function value; and
(c) αhi is chosen so that φ′(αlo)(αhi − αlo) < 0.
Each iteration of zoom generates an iterate α j between αlo and αhi, and then replaces one of these endpoints by α j in such a way that the properties (a), (b), and (c) continue to hold.
Algorithm 3.6 (zoom). repeat
Interpolate (using quadratic, cubic, or bisection) to find a trial step length α j between αlo and αhi; Evaluate φ(α j ); if φ(α j ) > φ(0) + c1α j φ′(0) or φ(α j ) ≥ φ(αlo) αhi ← α j ; else
Evaluate φ′(α j );
if |φ′(α j )| ≤ −c2φ′(0)
Set α∗ ← α j and stop; if φ′(α j )(αhi − αlo) ≥ 0
αhi ← αlo; αlo ← α j ; end (repeat)


62 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
If the new estimate α j happens to satisfy the strong Wolfe conditions, then zoom has served its purpose of identifying such a point, so it terminates with α∗ α j . Otherwise, if α j satisfies the sufficient decrease condition and has a lower function value than xlo, then we set αlo ← α j to maintain condition (b). If this setting results in a violation of condition (c), we remedy the situation by setting αhi to the old value of αlo. Readers should sketch some graphs to see for themselves how zoom works! As mentioned earlier, the interpolation step that determines α j should be safeguarded to ensure that the new step length is not too close to the endpoints of the interval. Practical line search algorithms also make use of the properties of the interpolating polynomials to make educated guesses of where the next step length should lie; see [39, 216]. A problem that can arise is that as the optimization algorithm approaches the solution, two consecutive function values f (xk) and f (xk−1) may be indistinguishable in finite-precision arithmetic. Therefore, the line search must include a stopping test if it cannot attain a lower function value after a certain number (typically, ten) of trial step lengths. Some procedures also stop if the relative change in x is close to machine precision, or to some user-specified threshold. A line search algorithm that incorporates all these features is difficult to code. We advocate the use of one of the several good software implementations available in the public domain. See Dennis and Schnabel [92], Lemar ́echal [189], Fletcher [101], Mor ́e and Thuente [216] (in particular), and Hager and Zhang [161]. One may ask how much more expensive it is to require the strong Wolfe conditions instead of the regular Wolfe conditions. Our experience suggests that for a “loose” line search (with parameters such as c1 10−4 and c2 0.9), both strategies require a similar amount of work. The strong Wolfe conditions have the advantage that by decreasing c2 we can directly control the quality of the search, by forcing the accepted value of α to lie closer to a local minimum. This feature is important in steepest descent or nonlinear conjugate gradient methods, and therefore a step selection routine that enforces the strong Wolfe conditions has wide applicability.
NOTES AND REFERENCES
For an extensive discussion of line search termination conditions see Ortega and Rheinboldt [230]. Akaike [2] presents a probabilistic analysis of the steepest descent method with exact line searches on quadratic functions. He shows that when n > 2, the worst-case bound (3.29) can be expected to hold for most starting points. The case n 2 can be studied in closed form; see Bazaraa, Sherali, and Shetty [14]. Theorem 3.6 is due to Dennis and Mor ́e. Some line search methods (see Goldfarb [132] and Mor ́e and Sorensen [213]) compute a direction of negative curvature, whenever it exists, to prevent the iteration from converging to nonminimizing stationary points. A direction of negative curvature p− is one that satisfies pT−∇2 f (xk) p− < 0. These algorithms generate a search direction by combining p− with the steepest descent direction −∇ fk, often performing a curvilinear backtracking line search.


3 . 5 . S T E P - L E N G T H S E L E C T I O N A L G O R I T H M S 63
It is difficult to determine the relative contributions of the steepest descent and negative curvature directions. Because of this fact, the approach fell out of favor after the introduction of trust-region methods. For a more thorough treatment of the modified Cholesky factorization see Gill, Murray, and Wright [130] or Dennis and Schnabel [92]. A modified Cholesky factorization based on Gershgorin disk estimates is described in Schnabel and Eskow [276]. The modified indefinite factorization is from Cheng and Higham [58]. Another strategy for implementing a line search Newton method when the Hessian contains negative eigenvalues is to compute a direction of negative curvature and use it to define the search direction (see Mor ́e and Sorensen [213] and Goldfarb [132]). Derivative-free line search algorithms include golden section and Fibonacci search. They share some of the features with the line search method given in this chapter. They typically store three trial points that determine an interval containing a one-dimensional minimizer. Golden section and Fibonacci differ in the way in which the trial step lengths are generated; see, for example, [79, 39]. Our discussion of interpolation follows Dennis and Schnabel [92], and the algorithm for finding a step length satisfying the strong Wolfe conditions can be found in Fletcher [101].
✐ EXERCISES
✐ 3.1 Program the steepest descent and Newton algorithms using the backtracking line search, Algorithm 3.1. Use them to minimize the Rosenbrock function (2.22). Set the initial step length α0 1 and print the step length used by each method at each iteration. First try the initial point x0 (1.2, 1.2)T and then the more difficult starting point x0 (−1.2, 1)T .
✐ 3.2 Show that if 0 < c2 < c1 < 1, there may be no step lengths that satisfy the Wolfe conditions.
✐ 3.3 Show that the one-dimensional minimizer of a strongly convex quadratic function is given by (3.55).
✐ 3.4 Show that the one-dimensional minimizer of a strongly convex quadratic function always satisfies the Goldstein conditions (3.11).
✐ 3.5 Prove that ‖Bx‖ ≥ ‖x‖/‖B−1‖ for any nonsingular matrix B. Use this fact to establish (3.19).
✐ 3.6 Consider the steepest descent method with exact line searches applied to the convex quadratic function (3.24). Using the properties given in this chapter, show that if the initial point is such that x0 − x∗ is parallel to an eigenvector of Q, then the steepest descent method will find the solution in one step.


64 C H A P T E R 3 . L I N E S E A R C H M E T H O D S
✐ 3.7 Prove the result (3.28) by working through the following steps. First, use (3.26) to show that
‖xk − x∗‖2
Q − ‖xk+1 − x ∗‖2
Q 2αk ∇ f T
k Q(xk − x∗) − α2
k∇ f T
k Q∇ fk,
where ‖ · ‖Q is defined by (3.27). Second, use the fact that ∇ fk Q(xk − x∗) to obtain
‖xk − x∗‖2
Q − ‖xk+1 − x ∗‖2
Q
2(∇ f T
k ∇ fk )2
(∇ f T
k Q∇ fk ) − (∇ f T
k ∇ fk )2
(∇ f T
k Q∇ fk)
and
‖xk − x∗‖2
Q ∇fT
k Q−1∇ fk .
✐ 3.8 Let Q be a positive definite symmetric matrix. Prove that for any vector x, we have
(x T x)2
(x T Qx)(x T Q−1x) ≥ 4λnλ1
(λn + λ1)2 ,
where λn and λ1 are, respectively, the largest and smallest eigenvalues of Q. (This relation, which is known as the Kantorovich inequality, can be used to deduce (3.29) from (3.28).)
✐ 3.9 Program the BFGS algorithm using the line search algorithm described in this chapter that implements the strong Wolfe conditions. Have the code verify that yT
k sk is always positive. Use it to minimize the Rosenbrock function using the starting points given in Exercise 3.1.
✐ 3.10 Compute the eigenvalues of the 2 diagonal blocks of (3.52) and verify that each block has a positive and a negative eigenvalue. Then compute the eigenvalues of A and verify that its inertia is the same as that of B.
✐ 3.11 Describe the effect that the modified Cholesky factorization (3.50) would have on the Hessian ∇2 f (xk) diag(−2, 12, 4).
✐ 3.12 Consider a block diagonal matrix B with 1 × 1 and 2 × 2 blocks. Show that the eigenvalues and eigenvectors of B can be obtained by computing the spectral decomposition of each diagonal block separately.
✐ 3.13 Show that the quadratic function that interpolates φ(0), φ′(0), and φ(α0) is given by (3.57). Then, make use of the fact that the sufficient decrease condition (3.6a) is not satisfied at α0 to show that this quadratic has positive curvature and that the minimizer satisfies
α1 < α0
2(1 − c1) .


3 . 5 . S T E P - L E N G T H S E L E C T I O N A L G O R I T H M S 65
Since c1 is chosen to be quite small in practice, this inequality indicates that α1 cannot be much greater than 1
2 (and may be smaller), which gives us an idea of the new step length.
✐ 3.14 If φ(α0) is large, (3.58) shows that α1 can be quite small. Give an example of a function and a step length α0 for which this situation arises. (Drastic changes to the estimate of the step length are not desirable, since they indicate that the current interpolant does not provide a good approximation to the function and that it should be modified before being trusted to produce a good step length estimate. In practice, one imposes a lower boundtypically, ρ 0.1—and defines the new step length as αi max(ραi−1, αˆi ), where αˆi is the minimizer of the interpolant.)
✐ 3.15 Suppose that the sufficient decrease condition (3.6a) is not satisfied at the step lengths α0, and α1, and consider the cubic interpolating φ(0), φ′(0), φ(α0) and φ(α1). By drawing graphs illustrating the two situations that can arise, show that the minimizer of the cubic lies in [0, α1]. Then show that if φ(0) < φ(α1), the minimizer is less than 2
3 α1.


This is pag Printer: O
CHAPTER4
Trust-Region
Methods
Line search methods and trust-region methods both generate steps with the help of a quadratic model of the objective function, but they use this model in different ways. Line search methods use it to generate a search direction, and then focus their efforts on finding a suitable step length α along this direction. Trust-region methods define a region around the current iterate within which they trust the model to be an adequate representation of the objective function, and then choose the step to be the approximate minimizer of the model in this region. In effect, they choose the direction and length of the step simultaneously. If a step is not acceptable, they reduce the size of the region and find a new


C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S 67
minimizer. In general, the direction of the step changes whenever the size of the trust region is altered. The size of the trust region is critical to the effectiveness of each step. If the region is too small, the algorithm misses an opportunity to take a substantial step that will move it much closer to the minimizer of the objective function. If too large, the minimizer of the model may be far from the minimizer of the objective function in the region, so we may have to reduce the size of the region and try again. In practical algorithms, we choose the size of the region according to the performance of the algorithm during previous iterations. If the model is consistently reliable, producing good steps and accurately predicting the behavior of the objective function along these steps, the size of the trust region may be increased to allow longer, more ambitious, steps to be taken. A failed step is an indication that our model is an inadequate representation of the objective function over the current trust region. After such a step, we reduce the size of the region and try again. Figure 4.1 illustrates the trust-region approach on a function f of two variables in which the current point xk and the minimizer x∗ lie at opposite ends of a curved valley. The quadratic model function mk, whose elliptical contours are shown as dashed lines, is constructed from function and derivative information at xk and possibly also on information accumulated from previous iterations and steps. A line search method based on this model searches along the step to the minimizer of mk (shown), but this direction will yield at most a small reduction in f , even if the optimal steplength is used. The trust-region method steps to the minimizer of mk within the dotted circle (shown), yielding a more significant reduction in f and better progress toward the solution. In this chapter, we will assume that the model function mk that is used at each iterate xk is quadratic. Moreover, mk is based on the Taylor-series expansion of f around
k
contours of
contours of f
Trust region step
Trust region
m
Line search direction
Figure 4.1 Trust-region and line search steps.


68 C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S
xk, which is
f (xk + p) fk + gT
k p+ 1
2 pT ∇2 f (xk + t p) p, (4.1)
where fk f (xk) and gk ∇ f (xk), and t is some scalar in the interval (0, 1). By using an approximation Bk to the Hessian in the second-order term, mk is defined as follows:
mk(p) fk + gT
k p+ 1
2 pT Bk p, (4.2)
where Bk is some symmetric matrix. The difference between mk( p) and f (xk + p) is
O (‖ p‖2), which is small when p is small. When Bk is equal to the true Hessian ∇2 f (xk), the approximation error in the model
function mk is O (‖ p‖3), so this model is especially accurate when ‖ p‖ is small. This choice Bk ∇2 f (xk) leads to the trust-region Newton method, and will be discussed further in Section 4.4. In other sections of this chapter, we emphasize the generality of the trust-region approach by assuming little about Bk except symmetry and uniform boundedness. To obtain each step, we seek a solution of the subproblem
min
p∈IRn mk ( p) fk + gT
k p+ 1
2 pT Bk p s.t. ‖ p‖ ≤ k, (4.3)
where k > 0 is the trust-region radius. In most of our discussions, we define ‖ · ‖ to be the Euclidean norm, so that the solution p∗
k of (4.3) is the minimizer of mk in the ball of radius k. Thus, the trust-region approach requires us to solve a sequence of subproblems (4.3) in which the objective function and constraint (which can be written as pT p ≤ 2
k)
are both quadratic. When Bk is positive definite and ‖B−1
k gk‖ ≤ k, the solution of (4.3) is easy to identify—it is simply the unconstrained minimum pB
k −B−1
k gk of the quadratic mk( p). In this case, we call pB
k the full step. The solution of (4.3) is not so obvious in other cases, but it can usually be found without too much computational expense. In any case, as described below, we need only an approximate solution to obtain convergence and good practical behavior.
OUTLINE OF THE TRUST-REGION APPROACH
One of the key ingredients in a trust-region algorithm is the strategy for choosing the trust-region radius k at each iteration. We base this choice on the agreement between the model function mk and the objective function f at previous iterations. Given a step pk we define the ratio
ρk
f (xk) − f (xk + pk)
mk(0) − mk( pk) ; (4.4)
the numerator is called the actual reduction, and the denominator is the predicted reduction (that is, the reduction in f predicted by the model function). Note that since the step pk


C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S 69
is obtained by minimizing the model mk over a region that includes p 0, the predicted reduction will always be nonnegative. Hence, if ρk is negative, the new objective value f (xk + pk) is greater than the current value f (xk), so the step must be rejected. On the other hand, if ρk is close to 1, there is good agreement between the model mk and the function f over this step, so it is safe to expand the trust region for the next iteration. If ρk is positive but significantly smaller than 1, we do not alter the trust region, but if it is close to zero or negative, we shrink the trust region by reducing k at the next iteration. The following algorithm describes the process.
Algorithm 4.1 (Trust Region).
Given ˆ > 0, 0 ∈ (0, ˆ ), and η ∈ [0, 1
4
):
for k 0, 1, 2, . . .
Obtain pk by (approximately) solving (4.3); Evaluate ρk from (4.4); if ρk < 1
4
k+1 1
4k
else if ρk > 3
4 and ‖ pk ‖ k
k+1 min(2 k , ˆ ) else
k+1 k ;
if ρk > η
xk+1 xk + pk
else
xk+1 xk ;
end (for).
Here ˆ is an overall bound on the step lengths. Note that the radius is increased only if ‖ pk‖ actually reaches the boundary of the trust region. If the step stays strictly inside the region, we infer that the current value of k is not interfering with the progress of the algorithm, so we leave its value unchanged for the next iteration. To turn Algorithm 4.1 into a practical algorithm, we need to focus on solving the trust-region subproblem (4.3). In discussing this matter, we sometimes drop the iteration subscript k and restate the problem (4.3) as follows:
min
p∈IRn m( p) def f + gT p + 1
2 pT Bp s.t. ‖ p‖ ≤ . (4.5)
A first step to characterizing exact solutions of (4.5) is given by the following theorem (due to Mor ́e and Sorensen [214]), which shows that the solution p∗ of (4.5) satisfies
(B + λI ) p∗ −g (4.6)
for some λ ≥ 0.


70 C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S
Theorem 4.1.
The vector p∗ is a global solution of the trust-region problem
min
p∈IRn m( p) f + gT p + 1
2 pT Bp, s.t. ‖ p‖ ≤ , (4.7)
if and only if p∗ is feasible and there is a scalar λ ≥ 0 such that the following conditions are satisfied:
(B + λI ) p∗ −g, (4.8a)
λ( − || p∗||) 0, (4.8b)
(B + λI ) is positive semidefinite. (4.8c)
We delay the proof of this result until Section 4.3, and instead discuss just its key features here with the help of Figure 4.2. The condition (4.8b) is a complementarity condition that states that at least one of the nonnegative quantities λ and ( − ‖ p∗‖) must be zero. Hence, when the solution lies strictly inside the trust region (as it does when 1 in Figure 4.2), we must have λ 0 and so Bp∗ −g with B positive semidefinite, from (4.8a) and (4.8c), respectively. In the other cases 2 and 3, we have ‖ p∗‖ , and so λ is allowed to take a positive value. Note from (4.8a) that
λp∗ −Bp∗ − g −∇m( p∗).
m
1
contours of
p*3
∆
∆
∆
2
3
p p*2
*1
Figure 4.2 Solution of trust-region subproblem for different radii 1, 2, 3.


4 . 1 . A L G O R I T H M S B A S E D O N T H E C A U C H Y P O I N T 71
Thus, when λ > 0, the solution p∗ is collinear with the negative gradient of m and normal to its contours. These properties can be seen in Figure 4.2. In Section 4.1, we describe two strategies for finding approximate solutions of the subproblem (4.3), which achieve at least as much reduction in mk as the reduction achieved by the so-called Cauchy point. This point is simply the minimizer of mk along the steepest descent direction −gk. subject to the trust-region bound. The first approximate strategy is the dogleg method, which is appropriate when the model Hessian Bk is positive definite. The second strategy, known as two-dimensional subspace minimization, can be applied when Bk is indefinite, though it requires an estimate of the most negative eigenvalue of this matrix. A third strategy, described in Section 7.1, uses an approach based on the conjugate gradient method to minimize mk, and can therefore be applied when B is large and sparse. Section 4.3 is devoted to a strategy in which an iterative method is used to identify the value of λ for which (4.6) is satisfied by the solution of the subproblem. We prove global convergence results in Section 4.2. Section 4.4 discusses the trust-region Newton method, in which the Hessian Bk of the model function is equal to the Hessian ∇2 f (xk) of the objective function. The key result of this section is that, when the trust-region Newton algorithm converges to a point x∗ satisfying second-order sufficient conditions, it converges superlinearly.
4.1 ALGORITHMS BASED ON THE CAUCHY POINT
THE CAUCHY POINT
As we saw in Chapter 3, line search methods can be globally convergent even when the optimal step length is not used at each iteration. In fact, the step length αk need only satisfy fairly loose criteria. A similar situation applies in trust-region methods. Although in principle we seek the optimal solution of the subproblem (4.3), it is enough for purposes of global convergence to find an approximate solution pk that lies within the trust region and gives a sufficient reduction in the model. The sufficient reduction can be quantified in terms of the Cauchy point, which we denote by pC
k and define in terms of the following simple procedure.
Algorithm 4.2 (Cauchy Point Calculation). Find the vector pS
k that solves a linear version of (4.3), that is,
pS
k arg min
p∈IRn fk + gT
k p s.t. ‖ p‖ ≤ k; (4.9)
Calculate the scalar τk > 0 that minimizes mk(τ pS
k) subject to satisfying the trust-region bound, that is,
τk arg τm≥i0n mk (τ pS
k ) s.t. ‖τ pS
k‖ ≤ k; (4.10)
Set pC
k τk pS
k.


72 C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S
It is easy to write down a closed-form definition of the Cauchy point. For a start, the solution of (4.9) is simply
pS
k −k
‖gk‖ gk.
To obtain τk explicitly, we consider the cases of gT
k Bk gk ≤ 0 and gT
k Bk gk > 0 separately. For the former case, the function mk(τ pS
k) decreases monotonically with τ whenever gk 0, so τk is simply the largest value that satisfies the trust-region bound, namely, τk 1. For the case gT
k Bk gk > 0, mk (τ pS
k) is a convex quadratic in τ , so τk is either the unconstrained minimizer of this quadratic, ‖gk‖3/( k gT
k Bk gk), or the boundary value 1, whichever comes first. In summary, we have
pC
k −τk
k
‖gk‖ gk, (4.11)
where
τk
{
1 if gT
k Bk gk ≤ 0;
min (‖gk‖3/( k gT
k Bk gk), 1) otherwise. (4.12)
Figure 4.3 illustrates the Cauchy point for a subproblem in which Bk is positive definite. In this example, pC
k lies strictly inside the trust region. The Cauchy step pC
k is inexpensive to calculate—no matrix factorizations are required—and is of crucial importance in deciding if an approximate solution of the trust-region subproblem is acceptable. Specifically, a trust-region method will be globally

k
C
mk
gk
Trust region
contours of
p
Figure 4.3 The Cauchy point.


4 . 1 . A L G O R I T H M S B A S E D O N T H E C A U C H Y P O I N T 73
convergent if its steps pk give a reduction in the model mk that is at least some fixed positive multiple of the decrease attained by the Cauchy step.
IMPROVING ON THE CAUCHY POINT
Since the Cauchy point pC
k provides sufficient reduction in the model function mk to yield global convergence, and since the cost of calculating it is so small, why should we look any further for a better approximate solution of (4.3)? The reason is that by always taking the Cauchy point as our step, we are simply implementing the steepest descent method with a particular choice of step length. As we have seen in Chapter 3, steepest descent performs poorly even if an optimal step length is used at each iteration. The Cauchy point does not depend very strongly on the matrix Bk, which is used only in the calculation of the step length. Rapid convergence can be expected only if Bk plays a role in determining the direction of the step as well as its length, and if Bk contains valid curvature information about the function. A number of trust-region algorithms compute the Cauchy point and then try to improve on it. The improvement strategy is often designed so that the full step pB
k −B−1
k gk
is chosen whenever Bk is positive definite and ‖ pB
k‖ ≤ k. When Bk is the exact Hessian ∇2 f (xk) or a quasi-Newton approximation, this strategy can be expected to yield superlinear convergence. We now consider three methods for finding approximate solutions to (4.3) that have the features just described. Throughout this section we will be focusing on the internal workings of a single iteration, so we simplify the notation by dropping the subscript “k” from the quantities k, pk, mk, and gk and refer to the formulation (4.5) of the subproblem. In this section, we denote the solution of (4.5) by p∗( ), to emphasize the dependence on .
THE DOGLEG METHOD
The first approach we discuss goes by the descriptive title of the dogleg method. It can be used when B is positive definite. To motivate this method, we start by examining the effect of the trust-region radius on the solution p∗( ) of the subproblem (4.5). When B is positive definite, we have already noted that the unconstrained minimizer of m is pB −B−1g. When this point is feasible for (4.5), it is obviously a solution, so we have
p∗( ) pB, when ≥ ‖ pB‖. (4.13)
When is small relative to pB, the restriction ‖ p‖ ≤ ensures that the quadratic term in m has little effect on the solution of (4.5). For such , we can get an approximation to p( )


74 C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S
∆)
pB (full step)
—g)
pU
—g
Trust region
p
Optimal trajectory
dogleg path
(unconstrained min along
(
Figure 4.4 Exact trajectory and dogleg approximation.
by simply omitting the quadratic term from (4.5) and writing
p∗( ) ≈ − g
‖g‖ , when is small. (4.14)
For intermediate values of , the solution p∗( ) typically follows a curved trajectory like the one in Figure 4.4. The dogleg method finds an approximate solution by replacing the curved trajectory for p∗( ) with a path consisting of two line segments. The first line segment runs from the origin to the minimizer of m along the steepest descent direction, which is
pU − gT g
gT Bg g, (4.15)
while the second line segment runs from pU to pB (see Figure 4.4). Formally, we denote this trajectory by p ̃(τ ) for τ ∈ [0, 2], where
p ̃(τ )
{ τ pU, 0 ≤ τ ≤ 1,
pU + (τ − 1)( pB − pU), 1 ≤ τ ≤ 2. (4.16)
The dogleg method chooses p to minimize the model m along this path, subject to the trust-region bound. The following lemma shows that the minimum along the dogleg path can be found easily.


4 . 1 . A L G O R I T H M S B A S E D O N T H E C A U C H Y P O I N T 75
Lemma 4.2.
Let B be positive definite. Then
(i) ‖ p ̃(τ )‖ is an increasing function of τ , and
(ii) m( p ̃(τ )) is a decreasing function of τ .
PROOF. It is easy to show that (i) and (ii) both hold for τ ∈ [0, 1], so we restrict our attention to the case of τ ∈ [1, 2]. For (i), define h(α) by
h(α) 1
2 ‖ p ̃(1 + α)‖2
1
2 ‖ pU + α( pB − pU)‖2
1
2 ‖ pU‖2 + α( pU)T ( pB − pU) + 1
2 α2‖ pB − pU‖2.
Our result is proved if we can show that h′(α) ≥ 0 for α ∈ (0, 1). Now,
h′(α) −( pU)T ( pU − pB) + α‖ pU − pB‖2
≥ −( pU)T ( pU − pB)
gT g
gT Bg gT
(
− gT g
gT Bg g + B−1g
)
gT g g B−1g
gT Bg
[
1 − (gT g)2
(gT Bg)(gT B−1g)
]
≥ 0,
where the final inequality is a consequence of the Cauchy-Schwarz inequality. (We leave the details as an exercise.) For (ii), we define hˆ(α) m( p ̃(1 + α)) and show that hˆ′(α) ≤ 0 for α ∈ (0, 1). Substitution of (4.16) into (4.5) and differentiation with respect to the argument leads to
hˆ′(α) ( pB − pU)T (g + B pU) + α( pB − pU)T B( pB − pU)
≤ ( pB − pU)T (g + B pU + B( pB − pU))
( pB − pU)T (g + B pB) 0,
giving the result.
It follows from this lemma that the path p ̃(τ ) intersects the trust-region boundary ‖ p‖ at exactly one point if ‖ pB‖ ≥ , and nowhere otherwise. Since m is decreasing along the path, the chosen value of p will be at pB if ‖ pB‖ ≤ , otherwise at the point of intersection of the dogleg and the trust-region boundary. In the latter case, we compute the appropriate value of τ by solving the following scalar quadratic equation:
‖ pU + (τ − 1)( pB − pU)‖2 2.


76 C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S
Consider now the case in which the exact Hessian ∇2 f (xk) is available for use in the model problem (4.5). When ∇2 f (xk) is positive definite, we can simply set B ∇2 f (xk) (that is, pB (∇2 f (xk))−1gk) and apply the procedure above to find the Newton-dogleg step. Otherwise, we can define pB by choosing B to be one of the positive definite modified Hessians described in Section 3.4, then proceed as above to find the dogleg step. Near a solution satisfying second-order sufficient conditions (see Theorem 2.4), pB will be set to the usual Newton step, allowing the possibility of rapid local convergence of Newton’s method (see Section 4.4). The use of a modified Hessian in the Newton-dogleg method is not completely satisfying from an intuitive viewpoint, however. A modified factorization perturbs the diagonals of ∇2 f (xk) in a somewhat arbitrary manner, and the benefits of the trust-region approach may not be realized. In fact, the modification introduced during the factorization of the Hessian is redundant in some sense because the trust-region strategy introduces its own modification. As we show in Section 4.3, the exact solution of the trust-region problem (4.3) with Bk ∇2 f (xk) is (∇2 f (xk) + λI )−1gk, where λ is chosen large enough to make (∇2 f (xk) + λI ) positive definite, and its value depends on the trust-region radius k. We conclude that the Newton-dogleg method is most appropriate when the objective function is convex (that is, ∇2 f (xk) is always positive semidefinite). The techniques described below may be more suitable for the general case. The dogleg strategy can be adapted to handle indefinite matrices B, but there is not much point in doing so because the full step pB is not the unconstrained minimizer of m in this case. Instead, we now describe another strategy, which aims to include directions of negative curvature (that is, directions d for which dT Bd < 0) in the space of candidate trust-region steps.
TWO-DIMENSIONAL SUBSPACE MINIMIZATION
When B is positive definite, the dogleg method strategy can be made slightly more sophisticated by widening the search for p to the entire two-dimensional subspace spanned by pU and pB (equivalently, g and −B−1g). The subproblem (4.5) is replaced by
mpin m( p) f + gT p + 1
2 pT Bp s.t. ‖ p‖ ≤ , p ∈ span[g, B−1g]. (4.17)
This is a problem in two variables that is computationally inexpensive to solve. (After some algebraic manipulation it can be reduced to finding the roots of a fourth degree polynomial.) Clearly, the Cauchy point pC is feasible for (4.17), so the optimal solution of this subproblem yields at least as much reduction in m as the Cauchy point, resulting in global convergence of the algorithm. The two-dimensional subspace minimization strategy is obviously an extension of the dogleg method as well, since the entire dogleg path lies in span[g, B−1g]. This strategy can be modified to handle the case of indefinite B in a way that is intuitive, practical, and theoretically sound. We mention just the salient points of the handling of the


4 . 2 . G L O B A L C O N V E R G E N C E 77
indefiniteness here, and refer the reader to papers by Byrd, Schnabel, and Schultz (see [54] and [279]) for details. When B has negative eigenvalues, the two-dimensional subspace in (4.17) is changed to
span[g, (B + α I )−1g], for some α ∈ (−λ1, −2λ1], (4.18)
where λ1 denotes the most negative eigenvalue of B. (This choice of α ensures that B + α I is positive definite, and the flexibility in the choice of α allows us to use a numerical procedure such as the Lanczos method to compute it.) When ‖(B + α I )−1g‖ ≤ , we discard the subspace search of (4.17), (4.18) and instead define the step to be
p −(B + α I )−1g + v, (4.19)
where v is a vector that satisfies vT (B + α I )−1g ≤ 0. (This condition ensures that ‖ p‖ ≥ ‖(B + α I )−1g‖.) When B has zero eigenvalues but no negative eigenvalues, we define the step to be the Cauchy point p pC. When the exact Hessian is available, we can set B ∇2 f (xk), and note that B−1g is the Newton step. Hence, when the Hessian is positive definite at the solution x∗ and when xk is close to x∗ and is sufficiently large, the subspace minimization problem (4.17) will be solved by the Newton step. The reduction in model function m achieved by the two-dimensional subspace minimization strategy often is close to the reduction achieved by the exact solution of (4.5). Most of the computational effort lies in a single factorization of B or B + α I (estimation of α and solution of (4.17) are less significant), while strategies that find nearly exact solutions of (4.5) typically require two or three such factorizations (see Section 4.3).
4.2 GLOBAL CONVERGENCE
REDUCTION OBTAINED BY THE CAUCHY POINT
In the preceding discussion of algorithms for approximately solving the trust-region subproblem, we have repeatedly emphasized that global convergence depends on the approximate solution obtaining at least as much decrease in the model function m as the Cauchy point. (In fact, a fixed positive fraction of the Cauchy decrease suffices.) We start the global convergence analysis by obtaining an estimate of the decrease in m achieved by the Cauchy point. We then use this estimate to prove that the sequence of gradients {gk} generated by Algorithm 4.1 has an accumulation point at zero, and in fact converges to zero when η is strictly positive. Our first main result is that the dogleg and two-dimensional subspace minimization algorithms and Steihaug’s algorithm (Algorithm 7.2) produce approximate solutions pk of the subproblem (4.3) that satisfy the following estimate of decrease in the model function:
mk(0) − mk( pk) ≥ c1‖gk‖ min
(
k , ‖gk ‖
‖ Bk ‖
)
, (4.20)


78 C H A P T E R 4 . T R U S T - R E G I O N M E T H O D S
for some constant c1 ∈ (0, 1]. The usefulness of this estimate will become clear in the following two sections. For now, we note that when k is the minimum value in (4.20), the condition is slightly reminiscent of the first Wolfe condition: The desired reduction in the model is proportional to the gradient and the size of the step. We show now that the Cauchy point pC
k satisfies (4.20), with c1 1
2.
Lemma 4.3.
The Cauchy point pC
k satisfies (4.20) with c1 1
2 , that is,
mk (0) − mk ( pC
k) ≥ 1
2 ‖gk‖ min
(
k , ‖gk ‖
‖ Bk ‖
)
. (4.21)
PROOF. For simplicity, we drop the iteration index k in the proof. We consider first the case gT Bg ≤ 0. Here, we have
m( pC) − m(0) m(− g/‖g‖) − f
− ‖g‖ ‖g‖2 + 1
2
2
‖g‖2 gT Bg
≤ − ‖g‖
≤ −‖g‖ min
(
, ‖g‖
‖B‖
) ,
and so (4.21) certainly holds. For the next case, consider gT Bg > 0 and
‖g‖3
gT Bg ≤ 1. (4.22)
From (4.12), we have τ ‖g‖3/ ( gT Bg), and so from (4.11) it follows that
m( pC) − m(0) − ‖g‖4
gT Bg + 1
2 gT Bg ‖g‖4
(gT Bg)2
−1
2
‖g‖4
gT Bg
≤ −1
2
‖g‖4 ‖ B ‖‖g ‖2
−1
2
‖g‖2 ‖B‖
≤ −1
2 ‖g‖ min
(
, ‖g‖
‖B‖
) ,
so (4.21) holds here too. In the remaining case, (4.22) does not hold, and therefore
gT Bg < ‖g‖3
. (4.23)