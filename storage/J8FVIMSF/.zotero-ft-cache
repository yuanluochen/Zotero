了解CV和RoboMaster视觉组
——你的最后一本计算机视觉入门手册
湖南大学 机器人学院 RoboMaster跃鹿战队 2022/23视觉组 ©
曾庆铖 neozng1@hnu.edu.cn
No. 1 / 352


版权声明
Copyrights 湖南大学RoboMaster跃鹿战队2022视觉组曾庆铖All Rights Reserved.
本文档采用CC协议,要求保留姓名标示(BY)。本仓库采用MIT License,因此你也可以将本文视作
HTML代码或说明文档。
只需要注明引用来源,就可以任意使用、复制、修改、合并、散布、出版/再版本文的部分或全部内容;
本文中引用的第三方内容版权归其著作者所有。
文中的有部分取自网络图片未能识别版权信息,若出现了您的图片被使用却尚未标注出处等的情
况,请通过邮箱/qq等方式联系笔者。
No. 2 / 352


湖大RoboMaster公众号
RM视觉交流群
关注我(们)
GitHub: https://github.com/NeoZng/vision_tutorial (本教程仓库,star/watch获取最新动态!)
战队Github&Gitee: https://github.com/HNUYueLuRM & https://gitee.com/HNUYueLuRM
CSDN: HNU跃鹿战队 https://blog.csdn.net/NeoZng
知乎:NeoAndrew https://www.zhihu.com/people/zengen-38 关注neozng谢谢喵
Bilibili:湖南大学跃鹿战队 https://space.bilibili.com/522795884
Bilibili: RoboMaster机甲大师官方账号 https://space.bilibili.com/20554233
微博: HNU跃鹿战队的微博 https://weibo.com/u/7503257970
RMCV 视觉开源数据站:https://rmcv.52pika.cn/#/rmcv
RM开源资料汇总:https://bbs.robomaster.com/forum.php?mod=viewthread&tid=6979(代码、教 程、RM圆桌、分享会)
RoboMaster机甲大师官方网站:https://www.robomaster.com/zh-CN
TODO:
1. KCF(实在有点难,很难讲得通俗易懂)
2. 自适应滤波在控制理论中的解释(某种最优准则下的观测器?)
No. 3 / 352


### 打广告
for RMer以及嵌入式开发者和初学者。
我们战队的超详细超规范超完整超细节的电控嵌入式机器人开发框架basic_framework开源啦!!!
basic_framework: Hey this is the basic frame work for robomaster standard infantry Robots! enjoy
using it, have fun developing Robot with us (gitee.com)
HNUYueLuRM/basic_framework: framework for developing RoboMaster Electric Control
programmes (github.com)
优点多多难度适中关键是教程贼详细,每个模块和底层bsp都可以独立拆分运行方便学习,代码格式规
范,还有不同兵种的示例,年轻人(年轻战队)的第一个电控框架就在这里等你。
还有对basic_framework用cpp重构的power_framework,采用了完全现代化的设计模式和开发工具
链,详情戳:powerful_framework: 基于basic_framework打造的C++进阶重构版本:
https://gitee.com/hnuyuelurm/basic_framework
### 打广告2
笔者正在编写关于控制理论与控制工程基础的新教程《线性控制系统——希尔伯特空间的视角》,教程
将从大家熟悉的线性代数引入,介绍以函数为分析对象构成的一种特殊的线性空间:希尔伯特空间。再
根据实际物理系统中最常见的常微分方程和偏微分方程为例,通过对这些函数的变换得到更好的基表
示,从而更好地研究系统地性质和表现。有了以上基础,我们就可以分析零极点、裕度这些生涩概念的
物理意义,经典控制从未如此简单。转而进入状态空间表示,相似变换和特征值分解在这里将充分发挥
作用,揭开所谓几何重数和代数重数的神秘面纱,了解线性反馈控制器、基于模型的控制以及这些方法
在传递函数建模中的体现。不吹不黑,这个系列可能会给你完全不同的颠覆性控制系统学习体验。
同样,还有对嵌入式开发的抽象层级介绍《嵌入式开发的抽象层次》,一步步教你从逻辑门开始构建时
序数字逻辑电路,进而打造以cpu为核心的经典冯诺依曼和哈佛架构,再引入arm32的cortex-m4内核以
及cmsis框架,找到硬件外设和软件编码的关系从而编写自己的标准库,完成中断管理,再封装更高级的
功能隐藏底层细节实现HAL;同时学习实时系统的搭建方式,了解RTOS和OS的异同,最后实现完整的嵌
入式系统。
感兴趣的同学持续关注我的知乎和github噢。
No. 4 / 352


-1.Catalogue
推荐使用支持Latex的markdown引擎作为本文的阅读工具,如typora、vscode markdown
addons、smark等。由于文中有一些动图,导出的PDF可能无法正常观看,可以到 Image_base 文
件夹下寻找对应动图,或打开markdown对应。
版权声明
关注我(们)
### 打广告
### 打广告2 -1.Catalogue
0.前言
1.摘要
2.视觉在各兵种中的作用
2.1.装甲板识别(步兵、英雄、哨兵、无人机)
2.2.能量机关(步兵)
2.3.哨兵 2.4.工程
2.5.雷达
2.6.飞镖系统 2.7.自动步兵
3.视觉组接触的软件
3.1.Ubuntu
3.2.IDE 3.3.Git
3.4.ROS
3.5.其他常用软件和小工具
4.视觉组接触的硬件 4.1.相机
4.1.1. 镜头
4.1.2. 相机参数调节 4.1.3. 硬件参数
4.2.运算平台
4.2.1. 运算平台的性能指标
4.2.2. 设备介绍以及benchmark 4.3.IMU
4.4.激光雷达
4.5.特殊相机 4.6.低速IO外设
4.5.1. 串行通信
4.5.2. CAN&总线通信
5.比赛中的CV算法
5.0.CV的常识性概念
5.1.OpenCV常用算法
5.1.1. imgproc 模块(image process) 5.1.2. imgcodecs 模块(image reading and writing)
5.1.3. videoio模块 (video input and output)
5.1.4. highgui模块(high level graphis user interface)
5.1.5. 其他模块 5.2.目标检测
5.2.0. 目标检测基础
5.2.1. 神经网络 5.2.2. 防止过拟合
5.2.3. 让你的网络学习得更好
No. 5 / 352


5.2.4. 梯度下降的进化
5.2.4.1. 进击的mini-batch 5.2.4.2. 尝试更高的阶数
5.2.5. 卷积神经网络
5.2.5.1. 卷积层
5.2.5.2. 池化层 5.2.5.3. 全连接层
5.2.5.4. CNN的反向传播
5.2.6. 目标检测
5.2.6.1. 目标检测中的常用术语
5.2.6.2. 经典检测网络之R-CNN家族
5.2.6.2.1. R-CNN
5.2.6.2.2. Fast R-CNN 5.2.6.2.3. Faster R-CNN
5.2.6.3. 经典检测网络之YOLO
5.2.6.4. 目标检测小结
5.2.6.5. 损失函数的设计与改进
5.2.6.5.1. MAE (mean average error)和RMSE(root mean square error)
5.2.6.5.2. CE/BCE(cross entropy/ binary cross entropy)
5.2.6.5.3. FL(focal loss) 5.2.6.5.4. GHM
5.2.6.5.5. OHEM系列
5.2.6.5.6. IOU loss family
5.2.6.5.7. GFL 5.2.6.5. 多尺度特征融合
5.2.6.5.1. SPPNet
5.2.6.5.2. FPN 5.2.6.5.3. PAN
5.2.6.5.4. Bi-FPN
5.2.6.6. CV中的注意力机制
5.2.6.6.1. 通道域注意力 5.2.6.6.2. 空间域+混合域
5.2.6.6.3. Transformer-liker
5.6.6.4. 降低注意力时间开销 5.2.6.7. Anchor-Free模型崛起
5.2.6.7.1. FCOS
5.2.6.7.2. CenterNet
5.2.6.7.3. CornerNet 5.2.6.7.4. ExtremeNet
5.2.6.7.5. FSAF
5.2.6.7.6. ATSS
5.2.6.7.7. 到底应该怎么确定正负样本
5.2.6.8. ViT(Vision Transformer)
5.3.滤波器、观测器、估计器和预测方法
5.3.1. 常用的几种信号滤波器 5.3.1.1 x通滤波器(频域)
5.3.1.2. 中值滤波器(时域)
5.3.1.3. Kalman Filter(卡尔曼滤波器,时域)
5.3.1.4. Particle Filter(粒子滤波器,时域) 5.3.1.5. Least Mean Square Error Filter(最小均方误差滤波器,时域)
5.3.1.6. 自适应滤波
5.3.2. 预测方法
5.3.2.1. 朴素方法:简单物理规律的应用
5.3.2.2. KF及其优化
5.3.2.3. 基于运动行为和环境交互
5.3.2.4. RNN
No. 6 / 352


5.3.3. 在图像处理中应用的滤波器:
5.3.3.1. 频域分析 5.3.3.2. 空域滤波
5.3.3.3. 形态学操作
5.4.目标跟踪
5.4.1. 基本实现方法和原理 5.4.2. 光流法(Optic Flow)
5.4.3. 以KCF为例的相关滤波(kernel correlation fitlering)
5.4.3.1. 相关滤波 5.4.4. NN-Based
5.4.4.1. 特征提取模块
5.4.4.2. 候选区域生成
5.4.4.3. 分类器/回归器(fine-tune) 5.4.4.4. 端到端方法
5.5.参数自适应方法和稳健特征
5.5.1. 阈值自适应与不变特征
5.5.1.1. 亮度自适应 5.5.1.2. 局部阈值自适应
5.5.1.3. 双边滤波
5.5.1.4. 形态不变特征 5.5.2. 统计特征和global-based方法
5.5.2.1. 基本模型
5.5.2.2. 直方图特征
5.5.2.3. gradient-based feature 5.5.2.4. PCA-face
5.5.3. local-distribution方法
5.5.3.1. gradient histogram 5.5.3.2. GIST
5.5.3.3. HOG
5.5.3.4. ConvNet没有不变性!?
5.5.3.4.1. 旋转不变性 5.5.3.4.2. 尺度不变性
5.5.3.4.4. 光照、对比度、颜色不变性
5.5.3.4.3. 平移不变性 5.5.4. 特征点选取与特征描述子
5.5.4.1. 特征点选取
5.5.4.1.1. Harris角点检测
5.5.4.1.2. 尺度空间及其上的极值检测 5.5.4.1.2.1. 检测目标:斑点
5.5.4.1.2.2. 尺度和分辨率的定义
5.5.4.1.2.3. 利用高斯核构建尺度空间 5.5.4.1.2.4. 通过DoG近似LoG
5.5.4.1.2.5. 筛选极值点
5.5.4.1.3. 极值检测的改进与积分图近似
5.5.4.1.3.1. 亚像素级特征定位 5.5.4.1.3.2. 二维前缀和:积分图
5.5.4.1.3.3. DoG的超级近似:更快速地快速计算LoG
5.5.4.1.4. 基于像素比对的FAST特征点检测
5.5.4.1.5. 图像金字塔和尺度空间的区别和联系 5.5.4.1.6. 再次检验invariant feature
5.5.4.2. 建立特征描述子
5.5.4.2.1. 如何保证各种不变性? 5.5.4.2.2. 梯度直方图
5.5.4.2.3. wavelet(SURF-liked)
5.5.4.2.4. 标定质心法确定主方向
5.5.4.2.5. LBP
No. 7 / 352


5.5.4.2.6. BRIEF
5.5.5. 特征点匹配方法 5.5.5.1. KNN/Cross Validation
5.5.5.2. FLANN
5.5.5.3. transformation/projection
5.5.5.4. RANSAC & PROSAC
5.5.6. 视觉词袋模型:基于特征点的物体分类/目标检测
5.5.7. 一个完整的特征点检测/匹配算法:以ORB为例
5.5.8. handcraft vs Deep feature:总结 5.6.三维视觉基础
5.6.1. 坐标系变换
5.6.2. 相机成像模型
5.6.3. 相机标定
5.6.4. 重投影和反向投影
5.6.4.1. 重投影:re-projection
5.6.4.2. 反向投影:back-projection
5.6.5. 旋转描述和四元数 5.6.6. 点云处理和PCL
5.7.初探SLAM系统
5.7.1. 状态估计问题 5.7.2. SLAM系统的一般构成
5.7.3. 为什么使用李群&李代数
5.7.4. 常用传感器配置及处理方法
5.7.4.1. 单目SLAM 5.7.4.2. 双目SLAM
5.7.4.3. 2D激光雷达
5.7.4.4. 3D激光雷达 5.7.4.5. IMU和轮式里程计
5.7.4.6. GNSS及其他绝对式测量
5.7.5. 多传感器融合
5.7.6. 滤波和优化&传感器耦合程度 5.7.7. 可能的进展
5.8.路径规划和轨迹生成与跟踪
5.9.自主决策 6.各算法具体应用
6.1.装甲板识别
6.1.1. 传统的灯条匹配+数字识别方法
6.1.1.1. 预处理
6.1.1.2. 寻找轮廓并筛选拟合出的对象
6.1.1.3. 数字识别
6.1.2.基于CNN的目标检测方法 6.1.2.1. 网络选择
6.1.2.2. 准备数据集
6.1.2.3. 开始训练吧
6.1.2.4. 评估效果 6.1.2.5. 存在的问题
6.1.3. 设计key-point detection方法的装甲板检测
6.1.3.1. Backbone
6.1.3.2. Neck 6.1.3.3. Head
6.1.3.4. Label Assignment
6.1.3.5. Loss 6.1.3.6. 其他
6.2.能量机关激活
6.2.1. 传统方法
6.2.2. 基于CNN的目标检测方法
No. 8 / 352


6.2.3. 圆拟合
6.2.4. 旋转函数拟合 6.2.4.1. 计算旋转角速度
6.2.4.2. 拟合旋转函数
6.3.测距与深度估计
6.3.1. 传统的定点解算方法 6.3.1.1. 比例法
6.3.1.2. PnP
6.3.2. 双目和深度相机
6.3.3. 激光雷达与相机联标
6.3.4. 深度学习方法
6.4.轨迹预测与跟踪
6.4.1. 运动学建模 6.4.2. 估计超前量
6.4.3. 跟踪器
6.5.弹道模型
6.6.反陀螺 6.6.1. 状态识别
6.6.2. 运动学建模
6.6.3. 击打策略 6.6.4. 观测器示例
6.7.自瞄算法框架
6.8.雷达站检测与小地图
6.8.1. 小地图构建和第一人称UI位置/血量展示 6.8.2. 英雄机器人狙击
6.8.3. 哨兵辅助识别
6.8.4. 和自动步兵联动 6.9.飞镖电视制导
7.视觉组成员需要掌握的知识
7.1.硬件相关
7.1.1. 计算机组成原理 7.1.2. 微处理器结构
7.2.软件相关
7.3.语言相关
7.4.计算机视觉相关
7.5.机械、硬件和电控
7.5.1. 伺服驱动单元
7.5.2. 电力电子硬件模块 7.5.3. 控制理论和姿态估计
7.5.4. 悬挂系统
7.6 知识体系和学习路线 7.6.0. 遇到问题怎么办
7.6.1. 数学
7.6.2. 计算机科学基础
7.6.3. 编程语言和软件框架 7.6.4. 数据结构和算法
7.6.5. OpenCV
7.6.6. PyTorch
7.6.7. 有用的网站和资源 8.心得体会和给新人的学习建议
8.1. 我的RM历程
8.2. 常见Q&A
8.3. 我对计算机视觉的一些理解
参考文献
No. 9 / 352


每个RoboMaster心中应该都有一个冠军梦,和一座金灿灿的奖杯
No. 10 / 352


0.前言
在阅读本文之前,你需要有计算机科学的基本知识并至少掌握一门编程语言,同时对 robomaster比赛
规则和过程有大致的了解。
若只是希望知道视觉组的基本工作,仅需要阅读第二部分、第五部分的第零节。
若你是计算机视觉的初学者,并不打算/已经过了参加 RoboMaster的年纪,请跳过第二、第六部分。
而你是/打算称为一名RMer尤其是算法/视觉组的队员,通篇阅读即可。
面向的主要读者:参加RoboMaster比赛的感知/视觉/算法组成员和有志于计算机视觉领域研究的
同学。
笔者希望在这本教程中向大家介绍视觉组的工作的基本概况和进入视觉组需要学习的知识,面向的对象
为热爱机器人的朋友、战队中的其他技术组或准备进入视觉组的同学。本文会尽量广泛、全面地向你介
绍视觉组的方方面面,同时可能涉及一些技术细节,提供尽可能直观的认识和必要的公式推导,但又不
涉及过多深奥的数学知识,让其他技术组也能够了解机器人视觉模块的运行机制,以便更好地协作开
发,防止出现各自为战的境况。同时让新人能更快地接触这些知识,明白视觉软件开发的过程,减少踩
坑的次数。本文不提供算法对应的代码,即没有手把手的实践环节。我们力求帮助你尽快扩展视觉开
发、机器人开发所需的知识面。因此,看完本教程并不意味着你已经掌握了所有知识,当且仅当你成功
把所学应用到实际之中,才可谓成功。
本文提供了大量的优质教程的链接,避免重复劳动,并给予读者更多选择。若已有外部优质资源,这里
将给出链接并以综述或总览的形式进行介绍。
最后也是记录一下笔者一年多来的成长,算是给自己的一个交待。限于笔者的水平,文中难免出现错误
和理解不当之处(小孩子不懂事写着玩的),请读者指出错误,多多包涵。可以直接提交issues或者
pr、通过邮箱或qq联系我。
23/10/12增补:现在回看当时编写的内容,很多知识理解并不是很恰当,有一些想当然,也有一些
对新人不够友好的地方。最要命的是没有做到详略得当,介绍了太多关于神经网络的东西。技术的
发展飞快,在大规模与训练当道、0代码训练基础模型的时代,这些东西多少有些无力。当然,所
谓尽信书不如无书,你已经是一个成熟的学生了,要学会自己辨别哪些内容值得学习。
No. 11 / 352


1.摘要
计算机视觉(computer vision)无疑是当今AI界最火热的研究领域之一,和自然语言处理、强化学习
并称三架马车。自然而然,在RoboMaster的赛场,视觉的软硬件开发也占有一席之地,视觉组便
和其他技术组一样应运而生了。
视觉组负责的机器人模块主要是传感器和数据处理,即通过对相机、激光雷达等传感器采集到的信
息进行处理从而让机器人在一定程度上具有”视觉”或“知觉”功能。
在比赛中,视觉组能够让机器人自动识别地方装甲板,实现“自瞄外挂”;也能让操作手轻松地击打
能量机关,使得全队获得增益;视觉组打造的感知系统更是哨兵机器人、自动步兵上的“大脑”,没
有视觉组的工作,这些机器人就完全失去了在场上的作用;视觉组同时还全权负责雷达这个兵种,
耳听八方眼观六路,可谓是战场指挥官。
在第二部分,主要介绍了视觉在每个兵种中的作用。
第三部分则是视觉组在开发时会使用到的软件,如Linux系统、一些IDE和小工具。随后将会简述视觉组
会使用的硬件如相机、各种运算平台等。
在第五部分是本教程的重头戏,介绍了比赛中会用到的算法,第五部分几乎覆盖了计算机视觉的所有基
础内容;紧接着在第六部分着重叙述比赛中对第五部分算法的工程应用,即如何让算法落地发挥作用。
这两个部分占据了全篇3/4以上的篇幅。
第七部分简要说明了在视觉组需要的开发技能和知识,同时也代表着你能在视觉组学到什么东西。如果
你刚入门,看完第二部分以后一定要先去第七部分的最后面看看视觉组的学习路线图和第八部分的常见
Q&A!
最后一个部分是笔者在视觉组练习时长两年半的心得体会,和对新人上手视觉工作的一些建议。
!另外,在每个部分中都会穿插地介绍一些视觉组在这些方面会接触到的知识!
计算机视觉学习之旅马上开始,祝你学习愉快,不要忘记实践噢!
No. 12 / 352


图源RoboMaster2022超级对抗赛规则手册;有删改
2.视觉在各兵种中的作用
看图说话,有图有真相!先看看RM比赛场地”战场“上和视觉组有关的场地道具和交互模块吧。记住这些
框框和注释,方便理解接下来出现的一些名词和概念,记不住也没关系,再回来看看。
2.1.装甲板识别(步兵、英雄、哨兵、无人机)
自动跟随装甲板效果
由于机器人上安装的图传模块到操作手看到的第一视角的传输延迟加上操作手反应速度的滞后,操
作手几乎很难手动瞄准高速运动的机器人上的装甲板。因此,视觉组在这三个兵种的研发上主要负
责装甲板的识别算法,通过处理图像找到相机视野范围内的装甲板(相机一般安装在云台上,和枪
口平行放置并指向同一个方向,类似瞄准镜),进而向下位机(STM32等用于控制的
MCU,microcontroller unit,单片机)发送此装甲板的相对枪口的角度数据,电控根据此数据控制电
机自动转向目标装甲板,实现装甲板的自动打击。
No. 13 / 352


因为图像处理、上位机和下位机的通信、电机的动作(即转向目标)、发射机构的动作(拨弹机构
旋转将弹丸推向摩擦轮+弹丸链路空隙),以及最耗时的弹丸在空中飞行,都会带来延迟,它们的
总时长最高可以达到0.5~0.7s。倘若将当前时刻识别到的装甲板相对于云台的位置原原本本地发送
给下位机进行控制,在识别到装甲板时刻之后的这段时间,对方机器人很可能已经移动到其他位置
了(以步兵机器人为例,最快移动速度甚至可以达到6m/s,平均移动速度也有2~3m/s),我们打
出的弹丸只会和目标装甲板擦身而过。因此,视觉算法还需要根据对方机器人在过去一段时间的运
动状态或掌握其他的先验知识,预测对手在当前时刻后一段时间的运动趋势(即上述所有会带来迟
滞的环节的时间总和),从而提前将云台转向对应位置,实现对目标运动的预测。玩过空战游戏的
同学可能很清楚,开火的时候都要给出一个提前量。通过理论来预测物体运动趋势最早可以追溯到
二战期间,控制论(cybernetics)之父维纳博士曾在美国空军基地研究自行防空火炮预测敌机运动
轨迹的问题,维纳滤波器(最小方差滤波器)也是在那时候被提出的。
处于小陀螺运动下的步兵机器人
随着自瞄算法的不断升级进步,RoboMaster的赛场上也出现了“反自瞄”,其中的代表之一就是“小
陀螺”。机器人在小陀螺模式下,云台和底盘处于分离状态,在底盘绕运动中心高速自旋的同时云台
保持稳定不动。在这种情况下,视觉识别的难度会大大上升,一是自瞄很难跟上装甲板的高速转
动,基本上我方机器人的云台运动会滞后于装甲板的运动;即使使用了前述的预测算法,由于装甲
板在很短的时间内就会在车身的一侧“消失“,这往往会导致预测失误,给出的提前量过大使得预测
值超出车身范围(瞄到空气);二是即使跟随到一个装甲板后,那个装甲板随着底盘的转动很快消
失在视野中,此时就要锁定另一块装甲板,使得云台在切换目标的过程中会来回转动,无法稳定,
导致弹丸命中率下降,且从操作手的第一界面看来,整个画面不断晃动带来晕眩感,体验极差(更
有甚者直接”晕车“)。其他单位尚且可以通过”火力覆盖“的方法凭借运气击中”小陀螺“,而开火冷却
时间长、弹丸价格高昂的英雄机器人可不敢随随便便对着小陀螺开火。
因此,反“小陀螺”算法出现了:通过设计合适的策略,识别对手的机器人处于小陀螺状态,然后让
云台对准敌方机器人的中心位置而不再跟随装甲板移动。由于机器人处于“小陀螺”状态时基本上是
匀速转动,这样就可在适当的时机开火(可以采用一些预测算法预测装甲板何时运动到云台所对准
的位置),提高命中率。也可以通过一段时间的检测,计算出对方机器人地盘转动的速度,从而预
测出每一块装甲板出现和消失的时刻,进行打击。
当然,随着反”小陀螺“算法的出现,赛场上又开始出现反‘反“小陀螺“ ’算法如变速小陀螺、超快小陀
螺(舵轮步兵)、随机转动等。哈尔滨理工大学(荣成校区)还研发出了偏心小陀螺,它的云台有
额外的平动自由度,使得转动的中心不在底盘的中心,戳这里看(精准空降)。快进到反反反反反小
陀螺(禁止套娃)因此,今后必定需要研发出鲁棒性(健壮性)和泛化性能更好的算法,才能应对
愈发“卷”的比赛啊~
No. 14 / 352


类似狙击枪/火炮的瞄准镜
哈工大I HITer战队的英雄机器人头顶一个ArUco定位标签,笔者猜测就是用于雷达定位的
英雄狙击点机制的加入让三维视觉又有了用武之地。英雄机器人在狙击点发射大弹丸将会返还10个
金币,并且狙击点处发射的大弹丸若击中对方的基地会获得2.5倍的攻击力增益!而传统英雄吊射
(狙击)都是采用量测射表的方式进行的:通过云台不同仰角的弹丸发射测试,获取不同距离的落
点信息,得到一张射表;然后在操作界面上利用自定义UI画出不同距离时需要对准的位置,从而实
现瞄准。
这种方法一方面对发射机构和云台的机械精度要求很高,另一方面还要保证摩擦轮的状态始终保持
一致(如果摩擦轮的硬度/磨损程度发生改变都会使得弹速变化),同时,操作手还得是“写轮眼”,
要能够精确对准对应距离的瞄准线。实际上算是一种半开环的控制过程,操作手只能根据每一发弹
丸的落点对云台姿态进行修正。诸多约束条件的限制使得英雄狙击难上加难,狙击成功率基本靠“抽
奖”
那么应该如何定量的确定狙击时需要的云台仰角,并准确地将炮口方向对准目标呢?假如我们能精
确定位英雄机器人的位置就好了!通过雷达系统,我们用某种方式计算出雷达相对英雄机器人的距
离,而雷达在绝对系下的坐标又是可知的,整个战场的各种尺寸也在规则中给出,由此以来就可以
利用准确的公式计算英雄机器人狙击时需要的瞄准参数了!
目前似乎没有队伍使用远距离视觉吊射(狙击基地)。存疑。
No. 15 / 352


2.2.能量机关(步兵)
2023赛季中能量机关的图样已经更改,旋转机制保持不变,但激活之后的增益有改动,击打的准
确度越高,增益越强力。详见规则手册。
在比赛场地的中央有一个风车形状的场地道具,就是能量机关。它也被称作”风车“、”神符系统“或”
大符“(大能量机关)”小符“(小能量机关)。能量机关的激活点是我方高地处的一个高台,距能量
机关7m。我们需要用小弹丸按顺序连续击中五片随机亮起的扇叶的末端装甲板才可以激活能量机
关。激活小能量机关能够为队伍带来50%的攻击力增益,大能量机关能为队伍提供100%的攻击力
增益和50%的防御增益。由于图传模块使用的是广角镜头,因此从操作手的第一视角看来五片扇叶
的间距非常小,再加上遥控的延迟,难以通过鼠标移动来进行打击,并且小能量机关处于匀速旋转
的状态,大能量机关更是以 0.785sin(1.884t)+1.305 的角速度旋转(在2022赛季其旋转角速度已经
修改为 ,上式的四个参数都是在一定范围内随机初始化的),操作手很难预测
其运动轨迹。两片扇叶之间的击打间隔必须小于2s,否则能量机关将会重置回到初始状态,之前已
经击打成功亮起的扇叶会直接熄灭。这便需要视觉组设计算法来识别未被击打过的末端装甲板并对
其实现自动瞄准,找准时机控制子弹的发射从而实现自动击打能量机关。
能量机关任务对视觉来说可以分为两个部分:扇叶识别和运动状态拟合。首先需要识别出待击打的
扇叶,然后利用包括能量机关的旋转速度、能量机关的几何尺寸等已知信息预测出一段时间后的目
标位置,进而发送给下位机实现击打。对于大能量机关,我们还需要辨识出被随机初始化的四个用
于确定旋转角速度的参数:幅值、角频率、初相和角速度常数。扇叶识别的任务和装甲板识别非常
相似,难度更是有过之而无不及,因为在激活点看向能量机关,背景处常有形式复杂多样的光线干
扰。运动状态拟合则需要综合信号处理、傅里叶分析、非线性优化等知识。
正在激活小能量机关的步兵机器人;图源RoboMaster2021内部技术交流
除了激活我方的能量机关,我们还可以在对手激活能量机关的过程中干扰对手。在能量机关的激活
的规则中规定:倘若击打了错误的扇叶(如已经击打过的扇叶或尚未亮起的扇叶),能量机关会将
进行重置,因此,我们可以进入敌方半场,通过识别对手已经激活的扇叶并自动瞄准它,发射子弹
击中错误的装甲板进而触发重置以干扰对手的激活过程。
No. 16 / 352


2.3.哨兵
哨兵机器人被悬挂在基地前方的导轨上往复运动,是场上的一个全自动机器人,其移动、目标搜
索、打击敌人的行为都依赖于其自主决策。它相当于基地的防御塔。编写一个优秀的感知程序和决
策程序,是发挥哨兵机器人威力的关键。哨兵机器人的云台会不断地转动使得上方安装的相机能够
扫描到它附近的每一个角落,一旦识别到敌方的机器人便能立即锁定对手,随后根据其决策算法判
断是否开火。也可以安装其他的传感器如激光雷达等,融合更多来源的数据以帮助更好地进行决
策。
我们还可以让在哨兵机器人遭到攻击时进入快速机动的规避状态,在导轨上进行随机地不规则动作
以躲避敌方的弹丸并干扰敌方机器人搭载的预测算法。
在前哨站尚未被击毁时,哨兵机器人处于无敌状态,这时我们可以让哨兵机器人保持固定以提高自
己的命中率,一旦前哨战被摧毁,立即启动哨兵机器人的底盘,进入巡逻状态。
巡逻中的哨兵机器人;图源RoboMaster2021内部技术交流
规则中允许机器人间通过裁判系统进行通信,因此操作手可以通过官方规定的串行接口向哨兵发送
指令,以改变哨兵的状态或完成简单的控制。
在2023赛季中,哨兵已经成为自动步兵的替身,拥有1000hp和2个发射机构以及750的发弹量,底盘功
率限制120w,在己方前哨战被摧毁之前处于无敌状态。若前哨战被摧毁且巡逻区的场地交互模块连续
15s没有检测到哨兵机器人存在,那么基地护甲将会展开。
哨兵机器人在基地前方的巡逻区出发,是场上的全自动机器人,官方常称之为AI机器人,其移动、
目标搜索和打击点人主要依赖于其算法的感知和自主决策。2023赛季中哨兵的地位从以往的基地守
护者转为具有重要战略意义的自动机器人。
若自下而上剖析哨兵机器人的技术栈,可以分为如下几个层面:
1. 运动控制层,一般由电控实现,即能够让底盘或云台按照上层指令(传统机器人为操作手通过
遥控器发送或算法平台发送),达到特定位置或跟随指定轨迹。注意,此处的闭环对象是电机
等执行机构。例如马上要介绍的在2.中的例子,当机器人与目标轨迹存在偏差时,轨迹跟踪层
会给运动控制层发来机器人底盘运动的期望速度和加速度等信息,若当前使用麦克纳姆轮底
盘,我们根据底盘运动的期望信息,通过运动学和动力学分析解算出每个轮子应当输出的扭矩
和速度等,并让电机对这些控制目标进行闭环。
2. 轨迹跟踪层,由算法实现。其上的路径规划算法会给出一条预定轨迹,区别于运动控制层,这
里的闭环对象一般是机器人底盘等,例如底盘当前距离目标轨迹的偏差为150mm。该值会发
往运动控制层,交由电机pid计算出具体需要的控制量以通过电子调速器完成最终闭环。轨迹
跟踪层拥有当前的机器人位置和其他状态信息,这些信息是通过同步定位与建图技术
(SLAM)获取的。
No. 17 / 352


3. 路径规划层,由算法实现。给定由建图与定位层提供的一个导航地图上的位置,路径规划层
使用A*、D*等算法规划出一条可行的路径,该路径往往由一系列路径点组成。通过对路径点
进行样条曲线插值并避免碰撞,生成的轨迹会交由轨迹跟踪层处理。
4. 定位与建图层,由算法实现,一般可以看作最高层的算法。通过机载的激光雷达、IMU、RGB
相机、超声波雷达、毫米波雷达等的组合,感知周围的环境并和预先建立的全局地图比较,以
确定机器人在场地所处的位置。这里的全局地图特指类似导航软件和地图软件中使用的导航地
图,这种地图一般只需要标注机器人的可通行区域而不需要详细的建筑或物品标注。最常见的
导航地图是占栅格地图。
5. 感知与决策层,和定位与建图同级,虽然SLAM层也有感知环节,但感知与决策层的感知一般
指的是对特定的具有明确含义的目标及其状态的感知,如对手的步兵机器人的方位、对方机器
人的运动状态等。例如安装在哨兵顶部的360°鱼眼相机采集回的图片,在其上运行目标检测算
法,发现12点和4点方向各有一辆对方的步兵机器人,而3点钟方向有一台我方的英雄机器
人;同时通过裁判系统读取的数据得知12点方向的敌人血量较少,但3点钟方向有一台我方机
器人,可以配合其对4点方向的对方机器人完成围杀,故做出决策:向4点方向移动+发射机构
转向4点方向+准许该云台在瞄准目标之后开火的指令集合。向4点移动的指令会由路径规划层
接收,根据定位与建图层给出的导航地图规划一条不会发生碰撞的路径给下层进行进一步操
作,发射机构转向4点方向的任务则直接交给运动控制层执行。常见的自主决策方法包括决策
树、状态机和基于强化学习的深度策略。
云台手能以最高3s/次的频率给哨兵机器人发送地图标点或指令,因此,云台手一定程度上可以“操
作”哨兵前往特定位置或执行封装好的简单战术。如开局移动哨兵至对方英雄狙击点、通过按键发送
执行围堵对方飞坡等战术。
若有一台强大的能够自主决策行动的哨兵机器人,我们可以展开诸多战术:配合英雄速推前哨战、
追击对方英雄机器人、“堵泉虐杀”、速下对方工程机器人等。
2.4.工程
在2023赛季中,取消了工程机器人刷卡救援的功能,并且所有机器人战亡之后会自动复活,只不
过需要返回补给区解锁发射机构。花费金币立即复活的机器人不需要解锁发射机构。
工程机器人在本赛季的任务主要有:抓取矿石、兑换矿石、移动障碍块以及救援阵亡机器人。这里
的每一步都可以利用视觉识别以完成自动化。
金矿和银矿
在抓取矿石的时候,可以在工程机器人的机械爪上安装可见光相机、测距传感器等,再编写相关的
算法来识别矿石,实现自动对位和自动夹取。在技术交流中,上交、东大等惊艳全场的“空接矿石”
依赖的就是自动识别矿石的算法。
No. 18 / 352


在兑换矿石时,当且仅当矿石下方二维码处的RFID被兑换站扫描到之后再推入,才视为兑换成功。
而工程机器人在抓取矿石的时候无法保证矿石的朝向始终正确,因此可以通过识别矿石的姿态来自
动翻转矿石。同时,根据兑换站上的一些图像特征,还可以定位扫描矿石窗口的位置,来快速地完
成自动化兑换,不需要操作手手动控制机器人以提高效率。
矿石下方的二维码,内嵌一个射频模组;二维码一样能够当作特征作为检测对象
2023赛季中,兑换站矿石存储的机构提供了不同级别的兑换难度,难度每提高一级,其变动的自由
度就增加一个,同时兑换获取的金币数量也会增多。当兑换金币达到一定数量之后,工程机器人会
顺次升级为“白银矿工”、“黄金矿工”,之后兑换的矿石会获得额外金币倍率。
障碍块;其定位标签和矿石相似,因此识别算法基本可以复用
障碍块夹取算法的设计与矿石识别算法如出一辙。通过移动障碍块,我们可以改变战场的地形来阻
挡对手的推进或保护我方机器人。
在我方机器人阵亡后,可以通过两种方式复活阵亡机器人:工程机器人将其拖回基地旁的补血点
(被称作”回家复活“)或是让工程机器人所携带的复活卡(RFID射频卡)和其他机器人上的场地交
互模块接触(被称作“刷卡复活”)。在这两种情况下,可以通过编写视觉算法来实现快速准确地救
援。可以在工程机器人的救援机构(夹爪、电磁铁)旁安装相机,并在其他机器人上黏贴二维码、
ArUco等或利用已有的特征,通过算法识别对应特征从而进行对位,将工程车的救援机构(夹爪、
电磁铁等)对准待施救的阵亡机器人上的救援结构(如环、柱、磁铁等),让工程机器人自动套牢
阵亡的机器人,随后就可以把阵亡机器人抬走了(或者原地刷卡复活)。
No. 19 / 352


正在夹取矿石的工程机器人(哈工大 I HITer战队);图源RoboMaster2021内部技术交流
2.5.雷达
雷达是本赛季新增的兵种,被放置在场边的一处高地上,拥有全局视野。利用目标检测算法和三维
重建、反向投影等,可以定位敌方机器人在场上的位置。随后,我们能利用这些信息为我方制作一
张实时更新的“小地图”,掌握对方机器人的动向,以帮助我方操作手进行战术决策,做到知己知彼
而百战不殆。
雷达在将全局数据处理后,还能通过多机通信功能和己方的哨兵机器人进行通信,相当于为它们开
了一双“天眼”。这无疑是极大地增强了这些自动单位的感知能力和决策能力(我愿称之为云计算!),
通过机间通信使得机器人不再受到边缘计算平台计算能力潺弱的限制,让算法火力全开,也让我们
离全自动机器人战队又更近了一步。
可以参考上海交通大学的雷达站,基本功能都已经实现,API封装的很好,通信功能也很完善,适合
在其上进一步开发。
No. 20 / 352


两侧各有一枚相机,中间被罩子盖住的是激光雷达;图源沈航-TUP战队
2.6.飞镖系统
飞镖系统也是2020赛季新增的兵种,相当于战争中的”导弹“。其目标是且仅是比赛中的建筑物:前
哨战和基地。云台手可以控制飞镖闸门的开启,每局最多能开启两次,每局最多发射四枚。一旦命
中,可以造成巨量伤害(前哨站750点,基地1000点),以及对对方全体机器人长达5秒的视障效
果(遮挡操作界面,相当于fps游戏中的“闪光弹”,A1高闪来1个!)。最关键的是,发射飞镖不需
要消耗金币!
No. 21 / 352


飞镖的本体,前方带“R“标的是击打触发装置,前哨战和基地可以检测其上发出的红外光以确认是否
击中
飞镖发射架;以通过包胶轮摩擦、挤压飞镖以提供动力
高收益却不意味着高成本,那就意味着高技术难度。对于机械和控制组的同学来说,空气动力学在
本科阶段是较为复杂的领域,镖体设计、发射方式等都需要经过精心地设计,由于依赖经验和试
错,大量的实验是不可避免的。对于视觉组的同学来说,为了实现主动制导,我们需要给飞镖一个
打击的目标。前哨战和基地顶部各有一个特殊的装甲板以及一枚绿色引导灯,这可以作为我们的参
考对象。由于规则(制作规范手册)对飞镖镖体的体积和重量都有限制,我们只能选用一些片上系
No. 22 / 352


前哨站和基地顶部的装甲板和飞镖引导灯
统作为运算平台,并且尽可能地精简算法以提高识别帧率,毕竟飞镖的速度最快可以达到18m/s,
而发射站距离前哨战只有16.3m左右的距离!
现阶段大部分学校采用的都是惯性开环方案,通过反复实验,开环确实可以达到相当的精度,如四
川大学和广州城市理工(原华南理工广州学院)的飞镖就非常的精准(命中集锦戳这里和这里)。
但是,如果能添加闭环,则会大大提高其灵活性和对扰动、误差的容忍性,这将极大程度地减低对
飞镖发射架机械精度的要求和对镖体的一致性以及两者的时不变性的严苛限制。更何况,我们又怎
么能保证不会出现反导系统呢?(近防炮,开火!ARMA3玩家集结)据笔者所知,由学校已经正在
开发哨兵反导功能了。
2.7.自动步兵
自动步兵机器人的属性
自动步兵同样是本赛季新增的兵种。当不为某台步兵机器人配备操作手时,可以将此机器人配置为
自动步兵。自动步兵的所有属性都高于普通步兵机器人,其底盘功率、枪口热量上限、冷却速度、
血量上限、弹丸射速都相当于同级的步兵机器人选择了所有类型的升级加点,甚至还要更多,是当
之无愧的“六边形战士”。
超高的属性值便意味着极大的开发难度。由于没有操作手,机器人进行的所有移动、攻击等动作都
需要自主决策。虽然弱AI(在一个特定的问题上能够取得比人类更好的成绩)在特定领域已经击败
人类,但是强AI(拥有各方面的智能)的诞生也许还为之过早。自动步兵便算是向强AI探索的一个
尝试。
为了知道自己“在哪里”,自动步兵需要搭载SLAM系统(Simultaneous Localization And Mapping)
以帮助自己构建整个地图的信息;为了知道要“往哪走”,自动步兵要能够进行路径规划;为了能够
自己决定“怎么做”,自动步兵要配备自主决策系统以确定当下应该执行的动作...... 总之,这是一个
大有可为,上限极高的研发方向。
No. 23 / 352


由DJI承办的RMUA(RoboMaster Uniersity AI Challenge)赛事便是一项关于全自动机器人对
抗的比赛,自动步兵的规则引入也是由此而来的。若要了解更多,请访问RoboMaster ICRA
RMUA。RMUA2021年决赛视频请看这里,2022在这里。哈尔滨工业大学(深圳)在今年5月
卫冕成功。
2023赛季中,自动步兵机器人已经被删除,其功能和属性大部分转移到全新设计的哨兵机器人上。
RMUA大赛也处于无限期停办状态,归期未有。
No. 24 / 352


3.视觉组接触的软件
进行视觉开发会用到各种各样的软件、开发环境、辅助工具等,所以很有必要了解一些相关的快捷
键、命令、使用技巧。选择一款适合自己的IDE能够提高开发效率,方便版本管理。
3.1.Ubuntu
为什么使用Ubuntu
Ubuntu是一个Debian系分支的第一大系统,是当前用户量最大的linux发行版。因此,遇到
任何问题一般都能够在用户社区askubuntu中得到解答。它的安装也非常的方便,并且在更新
到20.04后,ubuntu的桌面美观性也有提升。同时,ROS是在Ubuntu之下开发的。如果要使
用ROS,Ubuntu是你的不二之选。
Linux下开发C++程序相比Windows有无与伦比的优势,可以方便的配置各种第三方库和依
赖。常言道python好用是因为有大量开箱即用的第三方库,可以轻松通过pip安装,而Linux
下通过yum/apt/pacman等包安装软件管理的软件包,实际上就是C/C++隐藏的库安装/管理利
器!apt是Debian系发行版用于管理第三方库的一个软件,负责管理系统中安装的各类软件
包,开发包,依赖库。可以通过apt轻松地安装各种软件(可执行文件)、开发库(头文件
headers,.so动态链接库等。如果你曾经在Visual Studio中为项目配置繁杂的头文件、链接库
路径等依赖,你一定会爱上Linux下用cmake管理c++环境的开发方式。
Linux的内核和上层系统都比Windows更加精简,故在运行时占用的各类资源都要小于
Windows。在不打开任何应用的情况下,笔者的电脑在运行Windows10时占用的内存为
4.2G,cpu占用率在10-20%左右,而运行Ubuntu20.04LTS的时候,只使用了2.2G的内存,
cpu占用率只有10%不到。这样,在运行我们的视觉算法程序时,可以更充分地利用系统资
源,最大程度压榨电脑的性能。(甚至可以在测试结束后实际运行时关闭图形界面,只保留终
端!这样,系统内核作为唯一需要运行的基础程序,大概能将cpu占用率缩小到1-5%)
截止本文编写到此处,Ubuntu已经发布22.04 LTS
Linux对于深度学习的支持比Widnows更加友好,经常有sh脚本能够一键配置开发环境。此外
Linux对一些设备驱动的支持也更完善,我们可以选择挂载自己需要的驱动和IO,并且精简属
于自己的内核,增加实时性补丁等。
想要安装Ubutnu,可以参阅这篇教程:Ubutnu/Windows双系统的安装-排除各种问题!
NeoZng,当然,学习时使用虚拟机也是不错的选择,这能给你更大的试错空间,不用担心把系统
搞奔溃。
No. 25 / 352


这篇文章就是在Ubutnu下使用markdown编辑器完成的
现在,最方便的linux发行版是windows wsl(windows subsystem linux),这是一个由微
软维护的和windows高度集成与优化的linux发行版,wsl2本质上和虚拟机无异,但性能和便
捷性相比一般的virtualbox和vmwares等都由很大提升。wsl可以在命令行或Microsoft商店直 接安装。目前已经支持直接打开linux下的GUI界面。最佳实践在wsl内运行code server,在
你的windows下通过ssh连接到wsl。
不建议新人使用这种方式,双系统或一般的虚拟机更适合你。wsl2在连接usb和网络设备上目
前仍然存在一些复杂的步骤。
提到Linux就不得不提到命令行的使用,在Linux上进行开发常会使用到命令行,有些软件甚至只有
命令行界面的版本。在一些时候,直接在命令行中用键盘操作可能要比数不清的鼠标点击快得多。
你需要学习:
cd、ls 、pwd、mv、cp、touch、diff、rm、cat、mkdir、rmdir、echo、tar等文件系统的
基本操作,grep、find 查找文件和目录
帮助手册man和-help参数。
sudo、su、chmod等权限相关的操作。
ping、ifconfig、wget等网络相关的操作。
一定要亲手熟悉命令行的基本命令,切忌只看不动手!学习以上命令,戳这里Linux
Commands,简版的教程推荐这个:Linux Commands | Harry's Blog
至少掌握一个无GUI的文本编辑器的基本使用,如vi,vim,nano等。这能够帮助你在系统出现问
题的时候快速修改一些配置文件,或是在使用ssh连接的时候简单地编写一些程序。当然,笔者不
推荐你将这些文本编辑器作为主力IDE使用(即使是安装了各种各样的插件!)虽然一个熟练使用
vim的程序员和一个熟练使用eclipse的程序员拥有相同的开发效率,但是vim的学习成本可不知道
比eclipse高多少!
Linux的设计哲学是“一切皆文件”。它将所有的IO设备如网络接口、usb接口、显示屏、相机、键盘
鼠标、应用都视为文件,和这些“文件”的交互就是以规定的方式进行读写。因此,有必要了解Linux
下的基本目录和文件组织方式,其目录结构请参考:Linux文件目录结构一览表。对于文件读写权限
的管理也非常重要的,这决定了用户/程序对某些文件是否有访问权限。要是对文件系统有一些了
解,那便更好不过了。
在使用系统的时候,建议大家有良好的文件分类习惯,把代码库、软件、开发环境分开存放,避免
出现home目录乱糟糟的情况。
No. 26 / 352


3.2.IDE
想要编写代码,光靠文本编辑器+gcc+gdb可不行,我们要充分利用技术进步带来的便利,谁不喜欢做懒
人呢。这里推荐几款Linux下编写C++程序使用的IDE:
VSCode:微软的小儿子,啥系统都能用。丰富的插件生态只有你想不到没有你找不到,配置完之
后使用起来非常方便,比如C++就有一个C++ extensions pack。官方文档也很详细,毕竟是微软
主推的下一代编辑器。关键是好看啊!在使用了snippets和Visual Sutdio Intellicode这两个插件之
后,智能提示也足够智能。想要写其他的语言也能够一条龙配齐,总之,上手容易且可定制化程度
极高。
安装了wsl后可以在windows上利用vscode server可以获得与ubuntu无缝衔接的体验。
我们建议c++开发者使用clangd或c/c++以及cmake插件以实现代码语法高亮和智能提示等功能。
Clion:JetBrain家的IDE,界面很美观,智能提示也很智能。以前用过PyCharm或者其他jb系的IDE
的同学可以继续使用。统一用cmake管理项目,对cmake扩展的支持非常到位。也提供了大量的可
选插件。用.edu后缀的学校邮箱可以免费申请教育资格,就可以免费用了。
JB家为了对标VSC,推出了自家的轻量级编辑器Fleet,喜欢jb设计风格的同学可以尝试。同时,
Clion也更新了UI设计,选用新版ui更加简洁方便。
Qt:Qt也是一款跨平台的C/C++ IDE,在Qt上编写的GUI程序能够在所有平台上运行。用Qt可以方
便地编写一些图形化的程序,比如串口调试助手、调参助手等。他的整体界面也算是比较清爽,适
合新人使用。
这里需要特别提及的是CMakeLists的编写。Linux没有Visual Studio这样保姆级的IDE,并不存在
一款能够自动为你生成makefile的软件。所以至少要学习qmake和cmake中的一种工具。这里推荐
cmake,虽然比qmake的语法稍微复杂一些,但是cmake的功能非常强大,拥有非常优良的跨平台
支持。学习cmake还能帮助你进一步了解程序的编译、链接过程。关于程序是怎么从源代码到机器
代码最后在电脑上运行起来和cmake的基本使用,请参考《程序的生前死后-Cmake-noob
comein》-NeoZng这篇文章。
对于那些不太复杂的项目,你还可以使用语法规则更简单的xmake,这是一款由国人研发的基于
Lua的跨平台构建工具。
No. 27 / 352


git的标志性图标,分岔的icon表示强大的分支功能
萝卜青菜各有所爱。虽然IDE把工具链都集成在了一起,极大地方便了我们的使用,但笔者还是推荐你学
习一下GNU工具链的使用和基本原理,至少熟悉编译、汇编、链接的过程。这样可以更深入的了解软件
的运行,以便在开发过程中出现问题的时候,快速定位问题所在并找到解决方法。
3.3.Git
团队协作开发需要一款优秀的代码管理工具,那Git就是不二之选,大家肯定都听过GitHub这个最大的提
供gitlab服务的同性交友平台,它便是一个基于Git的代码托管平台。这里有个小故事,Git是Linux的元老
Linus因为Linux社区被禁止使用BitKeepter这款版本控制软件后,一怒之下在一周之内用C写出来的程序
哦。
我们实验室开始的时候都是用u盘拷贝程序,有时候在某个人的电脑上写一点有时候又在minipc上写一
点,虽然在文件夹上标准了时间和版本号,然而这并没有什么用,这导致一次合并代码的时候有十多个
版本的代码,根本不知道哪个能用哪个不能用,那时候又还不知道diff这个工具,弄得眼睛都快无了。
要学习Git,推荐这几个网站:廖雪峰的git教程 git简易指南-no deep shits! GitHub Guides
在学习Git的时候,一定要动手跟着一起实践,切忌光看不动!俗话说熟能生巧。
只要学会创建分支、合并冲突、建立远端仓库、合并分支和版本回退等基本操作即可,过于复杂的功能
需要时查阅man和help,没必要强行记忆。
3.4.ROS
ROS是所谓的机器人操作系统。但实际上他并不是运行在硬件上的内核或操作系统,而是开发机器人所
用的一套丰富完整的中间件,可以看作是完整的机器人框架和开发抽象层。
使用ROS可以免去许多线程间信息交互、传感器和执行器配置的烦恼,ROS已经提供了开箱即用的库,
以及大量方便调试与可视化的工具如RViz和rqt等,还有精心设计的日志系统和配套的仿真环境gazebo。
学习ROS你只需要学习ROS的几个基本编程理念,如节点、服务、发布订阅的概念,以及ros的包管理和
构建工具,之后就可以愉快的使用ROS了。
由于ROS1在实时性和安全性以及部分功能的易用性方面考虑得不是那么周到,在2017年ROS开始重构,
就有了新的ROS2。与Python3和Python2的关系一样,你可以把ROS1和ROS2看作两套完全不同的系
统,它们的功能大部分不兼容,即不能使用对方的库,若一定要这样做必须使用一个名为rosbridge的兼
容库。但它们的设计理念是类似的,从ROS1迁移到ROS2只会让你感觉无比轻松方便。
No. 28 / 352


若你对ROS不熟悉,请直接开始ROS2的学习。
建议安装Ubuntu20.04,因为它原生支持ROS1与ROS2,现在还有大量的软件基于ROS1实现,所以有时
难免使用ROS1的包。
通过ROS2,我们可以方便地构建视觉算法和建图定位、导航算法的框架,轻松接入各种传感器。
推荐华南师范大学的rm-vision和四川大学的rmoss这两套开源代码作为你的rm视觉入门框架。
3.5.其他常用软件和小工具
Microsoft Edge DEV for Linux :Edge浏览器Linux版,可以方便同步windows下的收藏夹、设
置、插件等。集锦的功能非常好用。
SimpleScreenRecorder :一款录制屏幕的软件
VLC:一款多媒体播放器,方便录制调试视频后进行观看。若安装系统的时候选择最小安装,则不
会预装媒体软件,因此需要自己安装。
qv4l2:linux下相机驱动的图形界面,在Ubuntu软件商店可以找到,方便调节普通USB相机的参
数。
Meld:一款diff软件的图形界面,方便对比文件的不同,在Git使用merge或pull的时候可能会用上,
在Ubuntu软件商店可以找到。虽然vscode也预置了此功能(只需要安装git便会激活),但是只能
在被git管理的repo文件夹里面启用。
Fsearch:和Windows下的everything类似,提供超快速的文件检索功能。
No. 29 / 352


Typora:好看好用的markdown编辑器,本文就是使用typora编写的。使用markdown编写代码
的说明文档是一个很好的习惯,这可以降低其他人阅读你编写的代码的难度,也有利于代码分享和
代码的传承。同时,你的也可以使用markdown来记录自己的学习历程、一次艰难的问题解决之
路。使用markdown可以提高你的记录效率。vscode内也有相关插件提供对markdown支持。现在
似乎要收费了,可以在官网下载beta history版本,版本号为0.x的公测版仍然免费。
TigerVNC:一款局域网内可用的远程桌面软件,VNCViewer也可以作为替代。强烈推荐使用远程桌
面调车!电控都有无线调试器,我们怎么能跪在地上呢(气抖冷)。在把运算平台安装到机器人上
之后,我也曾经拿着一块小屏幕和键鼠,蹲在地上和机器人进行亲密交流,这不仅加深了我和机器
人的感情,也加重了我的颈椎病和腰椎键盘突出。(最恐怖的是车车的云台或者底盘疯了的时候,
线全部缠到机器人上!!机器人甚至有可能对你造成伤害!!都是电控的锅,你云台怎么又疯了)
使用了vnc后,只要将minipc和你的笔记本连接到同一个局域网,你就可以优雅地拿着笔记本调车
了。如果校园网的带宽不够,建议买一个路由器,或者和搭建裁判系统的路由器公用也可以。
其他远程桌面如Xrdp(分辨率和画质最好)、NoMachine(最流畅,画质次之,可以实现局域网
内IP自动搜索,强烈推荐!)也是很好的选择。
配置Xrdp的步骤稍微有一点多,参考:Ubuntu利用xrdp实现远程桌面连接;在此过程中可以
熟悉linux系统的环境变量配置以及简单的计算机网络知识。
上交的同学更是把这件事做到了极致,他们直接通过网页来修改机器人的各种参数并得到反
馈信息,能做到不需要任何远程桌面就能实时调参,此想法以为妙绝!华南师范大学使用
foxglove进行网页可视化,也是一种选择。不过这些都需要了解包括动态网页的构建在内的
一些基本前端知识。
当然,ROS/2已经提供了大量方便可用的工具用于调试与可视化,这也是我们推荐ROS的原
因!不要再在裸机上开发vanilla程序,自己写调试工具辣!
SSH:在外面不需要图形界面的时候,能够直接连接终端就是一件很方便的事情。并且在代码真正
部署上车时通常我们为了最大程度降低额外开销会选择关闭图形界面,终端连接就成了不二之选。
使用过SSH连接服务器的同学应该对此不陌生了,有兴趣的同学可以了解一下非对称加密的原理。
linux下可以直接使用终端作为ssh的客户端,windows下Microsoft全新推出的windows terminal
也十分美观。其他的如PuTTy等也可以尝试。
VScode也有SSH connection插件,可以将连接端的文件夹映射到本地,同时还可以直接在code里
重用端口开启多个终端!同时,通过宿主机安装的Code Server,所有的插件也支持在远端运行。
关于SSH的原理及其在windows/linxu下的简单配置,戳这里。
Docker:标志性的小蓝鲸logo。Docker可以把开发环境和依赖打包在一起并于Docker engine上运
行,运行时完全和系统的环境隔离。就像一个轻量级的虚拟机。Docker能够将应用程序与基础架构
(操作系统、开发环境、依赖等)分开,使用docker engine提供的抽象大大提高兼容性,不需要
关心底层和外部复杂的关联从而专注于软件开发。相信很多同学都遇到开发的时候配环境时醉生梦
死的情况,使用Docker镜像则可以免除这一切烦恼!华南师范大学PIONEER战队的开源代码就提供
了Docker的部署方案。这里也非常推荐他们开源的rm-vision视觉框架,这是一套基于ROS2构建的
自瞄程序,代码层次分明易读,关键部分附有注释和文档,调试工具好看易用。
No. 30 / 352


Docker的架构
有了docker,你甚至可以在windows下运行代码(实际上对宿主机的系统没有要求),虽然
不推荐这样做。
No. 31 / 352


4.视觉组接触的硬件
虽然别人总觉得视觉组就是整天对着屏幕臭敲代码的程序员,实际上我们也会接触很多的底层硬件
与传感器,在使用硬件的同时很可能还需要综合运用其他学科的知识。
4.1.相机
相机是机器人的眼睛。和人眼的成像原理一样,相机通过镜头汇聚光束使他们聚集在一块半导体感光元
件上(相当于视网膜)从而产生可供读取的数据。随后图像随着数据线传如miniPC等运算平台(视网膜
刺激视神经传到神经冲动到大脑)。时下的感光单元主要分为两种:CMOS和CCD。电类的工科生或是
摄影爱好者对此应该不会感到陌生。
CMOS(complementary metal-oxide semiconductor)传感器是由金属氧化物半导体排成的阵列,
和发光半导体相反,其上的pn结受光照时会产生电荷存储在电容中,通过和内存一样的结构采用行
选和列选开关,为每一行/每个像素点配备放大器和AD(Analog-to-Digital converter,模拟-数字信
号转换器),选中位置的信号会通过OP和AD,从而将电荷转换为数字信号,最后生成图像。优点
是成本低,图像帧率高,但是固有不可消除的采集噪声(每个采集单元的参数不可能完全一致)会
影响成像的质量。
CCD(charged couple device)传感器是由最简单的MOS电容器阵列构成的,与CMOS不同,CCD的
每一行像素只配有一个信号处理器,利用时钟脉冲驱动产生的差压,这一行电容器采集到的电荷会
以串行的方式依次通过每一行末端的信号处理器从而产生图像。因此CCD能达到更高的像素密度
(CMOS上的op和ad都要占用空间),其优点是成像的一致性好,噪声很小,由于不能同时处理所
有信号,其帧率一般不高。
想要获得关于感光单元的更多信息,请参阅cmos和ccd。
这是我们实验室的一枚MV工业相机,可以看到正中间有一块小小的CMOS传感器晶片
系统地学习成像原理,参见国防科技大学的MOOC:《第十五讲:CCD图像传感器》
相机的快门还有全局曝光和卷帘曝光之分。全局曝光时一次性采集所有感光单元的信息或为每个感光单
元配备一个寄存器暂存电荷,而卷帘曝光则是分时逐行采集信息(根据其名“卷帘”就能看出是一排一排地
采集数据)。若经费充足建议购买全局曝光的相机,其所有单元在同一时刻采样,曝光时间和我们设置
的曝光时间才是真正一致的。而卷帘曝光的相机第一行首先完成曝光,接着是第二行、第三行...到最后一
行完成曝光时,中间大概有数百ns甚至ms级的时间,这就导致整个画面其实不是在同一时间采样的,在
面对高速运动的物体是会出现“果冻效应”,即图像的不同部分出现倾斜、断层等现象,极大地影响了成
像质量和识别的效果。CCD相机一般使用全局快门,而CMOS相机使用全局快门的制造成本很高,需要
为每个感光单元配置一个寄存器。
No. 32 / 352


这两幅风扇的图像是在相同时刻分别用全局快门和卷帘快门的相机拍摄的
从透镜到小孔的近似过程,截取自知乎-龚健男的回答-凸透镜和小孔成像的原理与联系
4.1.1. 镜头
就像人的晶状体,不同的是,人的晶状体是可以改变焦距但像距固定,而一般使用的工业相机和USB相
机配套的镜头是定焦但可以调节像距的。不过在小孔成像模型中,我们把透镜看作被压缩成一个点的大
小,因此在这种情况下我们认为焦距和像距等同。
我们都知道镜头是一块凸透镜,在他的两边各有一个焦点,焦点即所有平行进入透镜的光线的汇聚
点。不同焦距的镜头其视距和视野范围不同,一般来说,视距大(看的远)的镜头,其视野范围小
(可视角小);而视距短(看的近些)的镜头,视野范围大,典型的例子是广角镜头。拿生活中的
例子来说,现在的手机的摄影系统都是由多个镜头组成的,每个镜头的焦距一般不同,从而适应不
同焦段和视野的摄影需求。高级的镜头通常可以调节光圈大小,从而改变镜头的进光量。
No. 33 / 352


在比赛中,我们一般给步兵机器人配置广角镜头(适用近战,可视角广)或是6mm的镜头(中庸的
选择,兼顾长短)。打击能量机关的步兵机器人会选择8mm、12mm的长焦镜头来获得更好的远距
离成像效果。哨兵机器人可能会配置两个相机,分别搭载广角镜头和中短焦镜头,广角用于“广撒
网”,对敌方目标进行大致的定位,之后交由另外一个相机进行精确的定位。有些打得准(弹道精度
高)的队伍甚至为所有机器人都搭载了两枚镜头或选用变焦镜头,让机器人成为各个距离都能实施
自动打击的多面手。海康机器人的官网提供了一个镜头选型工具:海康机器人-镜头选型工具,输入
参数后可以自动计算需要的视场角和靶面尺寸、焦距等,不过它推荐的肯定是自家的产品,只需要
拿着相应的参数自行联系经销商或去tb上找平替款即可。
若想要更有针对性地选择镜头的焦距,可以根据相机成像模型进行视野要求和焦距之间相关关系的
计算。
USB工业相机上几种不同焦距的M12镜头,依次是超广角,4mm,6mm,8mm,12mm
M12镜头的12指的是相机上的接口直径为12mm
由于凸透镜本身的性质和镜头制造的工艺问题,使得光线在通过镜头时无法保持物体在空间中原本
的位置关系而发生畸变,好在这些畸变都能够通过数学建模并由反向解算而得到还原。这需要我们
通过相机标定来去除这种畸变以便还原图像中物体的真实位置。畸变主要分为切向畸变和桶型畸
变,我们可以利用标定板和畸变的数学关系来进行相机标定,OpenCV中也有相关的函数可供调
用。关于成像模型、畸变和标定,可以参阅相机标定和OpenCV官方文档中的
CameraCaliberation。在 5.1、6.3 中我们还会提到这一点,并且给出了提供标定流程的参考文
章。
光圈就是镜头前面可以开闭的小扇叶(M12镜头一般不可以调节光圈,但板级工业相机一般有一个
“auto aperture”的选项,可以根据环境亮度调节“虚拟光圈”,毕竟上面没有机械光圈)。不同光圈
大小代表不同的镜头开度,其影响的是镜头的进光量。一般用f值刻画光圈的开合程度:
No. 34 / 352


f/x,x代表的是开合程度,镜头上此数值越小说明通光孔的直径越大
光圈越大,景深越小,反之景深更大
光圈还和成像的景深有关系,越大的光圈得到的景深越小,即成清晰像的范围越小:
因此,在调大光圈提高进光量的同时,能够成清晰像的距离范围就缩小了,我们需要在两者之间进
行权衡。不过没有关系,为了提高画面的亮度和可视性,我们还可以调节一些其他参数:在下一个
小节会介绍曝光时间和增益,以及gamma这三个参数。
此外,不可追求景深而将光圈缩得太小。一方面是进光量大大下降(平方反比级),另一方面是光
在通过小孔的时候会有强烈的衍射现象,导致像的边缘模糊,边界不够锐利。
No. 35 / 352


4.1.2. 相机参数调节
相机在使用过程中,除了硬件参数我们还可以调节许多采集参数,之所以很多队伍(可以说是大部分队
伍)会选用工业相机,就是因为其参数调节是高度定制化的,厂商提供了SDK以方便我们进行二次发
开。这里列出一些相机的主要的可调参数:
曝光:每一帧图像的感光时间,其值愈大则画面的整体亮度越大,曝光时间过长过短都可能会出现
宽容度不够的情况(一片雪白或是漆黑无比),选择正确的曝光是算法能否奏效的关键。如传统的
灯条特征选取算法就要求恰当的低曝光以保证灯条不会出现过曝而显现白色但同时又要能够看清装
甲板中间的数字以便进行模板匹配、svm分类或其他数字识别;运用卷积神经网络的目标检测算法
则需要相机采集到的画面能尽量接近数据集中图片的情况,一般来说需要高一些的曝光。
帧率:相机每秒钟能够获取的图像数。一般来说,如果你的图像处理算法的速度够快,那么帧率越
高越好,这能够保证你处理结果的实时性。一些相机提供了硬件触发 功能,这样能够让相机以固定
的时间间隔触发采样,保证两帧之间的时间相同,以便于和机器人的控制进行时间线同步。视觉算
法处理、数据传输、电控算法处理、再到控制执行机构动作,最后子弹在空中飞行——这几个过程
中都有时间延迟,累加之后算是非常可观。高速的算法和确定的延迟时间是打造预测算法的基础。
白平衡:设定白色是怎么样的“白”,本意是不管在任何光源下都让原本呈现白色的物体通过增加偏
置而还原为白色。涉及到色温和颜色空间的概念,调节此参数即调整RGB三原色的混合比例。
图像保存格式:图像的编码方法,如JPEG, RGB, YUYV, YUY2等,不同的编码格式保存的信息量可能
有差别。我们会在 5.0 节进一步了解相关信息。
分辨率:一张图像的像素数,常见的有1920x1080,1280x720,640x480,一般来说,分辨率越
高则图像保留的细节就越多,但同时相机处理的数据量变大,会降低采集帧率和每秒传输的图像数
量。在之后的图像处理中,同样意味着更大的开销和处理速度下降。
增益:调节感光单元在进行电荷信号放大时的增益(gain,一般是用dB表示的,这需要你注意数量
级),对于图像的亮度和各颜色信息的保存都有影响。在低曝光的时候可以有效提高成像质量,但
同时也可能提高噪声(不规则噪声信号也会被放大)。
对比度:图像中明暗区域最亮的白和最暗的黑之间不同亮度层级的测量,差异范围越大代表对比越
大,在高动态范围和高宽容度的时候,提高对比度可以凸显图像中亮度不同部分的区别,相当于用一
把刻度更精细的尺子去测量物体能够得到更精细度量信息。某些情况下,提高对比度所指的则是直
接增加亮暗之间的差别,让亮部更亮,暗部更暗。
饱和度:是HSV色彩空间中的概念,代表了一种颜色的纯度(Saturation)。
这里推荐使用qv4l2这款软件,可以方便的给相机调参并实时显示效果。
No. 36 / 352


这是软件qv4l2的截图,v4l2是linux自带的相机驱动程序,可以看到有许多可供我们调整的设置
工业相机的配置则需要使用厂商提供的sdk,OpenCV也提供了一些修改相机参数的函数
No. 37 / 352


4.1.3. 硬件参数
这些参数都是在选型的时候需要注意的,衡量了一个相机在可调参数上有多大的调节空间。
靶面尺寸:该参数即感光元件的面积大小,值越大表明面积越大,面积越大进光量就越大,信噪比
自然会相应提高,对于暗光环境会有更好的成像效果,还有其他种种优势,这也是所谓的底大一级
压死人。使用靶面尺寸这个看起来无厘头的metric其实是有历史渊源的。
在CCD出现之前,摄像机是一直使用光导摄像管的成像器件感光成像的,其直径的大小,直
接决定了其成像面积的大小。因此,后来大家就用光导摄像管的直径尺寸来表示不同感光面
积的产品型号。直到CCD出现之后,也就自然而然沿用了光导摄像管的尺寸表示方法,进而
扩展到所有类型的图像传感器的尺寸表示方法上。
例如,型号为“1/1.8”的CCD或CMOS,就表示其成像面积与一根直径为1/1.8英寸的光导摄像
管的成像靶面面积近似。光导摄像管的直径与CCD/CMOS成像靶面面积之间没有固定的换算
公式,从实际情况来说,CCD/CMOS成像靶面的对角线长度大约相当于光导摄像管直径长度
的2/3。
常见的靶面尺寸其和实际面积的对应关系如下:
宽容度/动态范围:大家常常会问一个问题:为什么我的相机拍出来的画面很暗,但是灯条中间仍然
发白?这是相机的硬件特性动态范围或宽容度(这两个词汇在摄影领域常常被混淆,但是对于数字
相机而言这两个词汇代表的意思几乎一致)所决定的。动态范围即感光元件能够记录的光强从最低
到最高亮度的范围。最低即感光元件刚好有输出(或是当没有任何光线进入时产生的响应,即暗电
流),会根据光线变化而产生电荷变化的阈值;最高亮度范围则是相机能够捕捉的最大累积光强,就
像放大器一样,超过了其输出上限就会产生截断,此时即使继续提高曝光时间或外界光强增大,转
换原件的输出也不会变得更大。因此上面的问题就出在相机的宽容度不足上。以高级的摄影器材为
例,它们通常拥有超高的宽容度,下方是RM官方制作的数据集ROCO中的一张图片,可以发现画面
整体明亮,但是红色装甲板表现地非常“红”,蓝色也很“蓝”,没有出现“过曝”的情况。
No. 38 / 352


图源ROCO数据集,建议在新标签页中打开放大观看
上方图片采用了6种曝光下采样的数据进行融合,左下角是融合的最终结果;图源
http://bbs.a9vg.com/thread-5113593-1-1.html
动态范围在硬件实现上体现为单个CMOS/CCD单元能够容纳电荷的数量,光电耦合单元的容量越大
就能保证曝光时间增大时不会发生“溢出”。摄影技术中常用的一种从软件层面提高动态范围的方法
称作HDR(high dynamic range),这是一种一次性拍摄多张不同曝光的图片并利用算法对多帧图
像进行对齐以及融合的技术。例如在逆光拍摄的时候光线来源区域往往会过曝,而背光处的物体常
常“黑成一团”,这时候开启HDR,相机会采用两种曝光参数分别采样数次,高曝光图像能够捕获更
多背光区域的细节,低曝光图像能够防止光线来源区域的感光单元溢出。通过自适应阈值技术自动
为不同区域选择合适的图像进行填充即可得到一张完美的图像。
因此在选购相机的时候,特别是对于传统算法,应该选择动态范围大的相机。动态范围一般以dB为
单位,表明最大输入和最小输入之间的倍率关系。
No. 39 / 352


只选择深色的像素进行成像,白色像素不工作
像元尺寸:顾名思义就是一个像素的实际面积有多大。在靶面尺寸相同的情况下,自然是拥有更大
像元尺寸的传感器的感光能力更强了(合并后的像素可以采集更多的光线)。不过相应的,这自然
会造成分辨率的下降。而相反,较小体积更小的像元尺寸可以做到更大的分辨率,不过由于单个像
素的面积减小,CMOS/CCD单元保存电荷的能力也会下降导致动态范围无法提高,安放AD的位置
也会缩小。
一般工业相机的SDK都支持合并数个像元(通常是水平合并 以及竖直合并 ,或者叫Binning),让
用户可以合并相邻像素获得更高的画面亮度(binning也分为模拟binning[硬件支持?]和数字
binning[利用算法进行插值],模拟binning由于DSP需要处理的数据变少了/CMOS转换的行列帧数
减少,可以提高采集帧率)。还有另外一种降低分辨率(一般是为了提高采样速度)的方法是下采
样,这种方法和像素合并不同,直接看下图就很好理解:
特别注意,若选用相机的ROI模式或者直接修改分辨率,则会在cmos上裁剪一块区域用于成像,剩
下的区域不会工作。这会提高相机的工作帧率,但是显然会减小成像的视场角,因为有效成像面积
下降了。下采样则在分辨率下降的时候保持相同的视场角。
信噪比和动态范围的对比,图源https://www.znjtech.cn
信噪比:SNR(signal noise ratio)是真正的由光线引起的动作信号和噪声(电磁干扰、杂光、暗
电流和散射光等)的比值,一般也是用dB来衡量。注意区分此参数和动态范围此参数越大,在增大
增益的时候,噪点引起的干扰会越小。
像素格式:或者叫图像保存的格式/采样格式。常见的有这些:
No. 40 / 352


最常见的RGGB排列方式
Mono 8/10/12,黑白
Bayer RG 8/10/10Packed/12/12Packed
YUV422Packed,YUV422_YUYV_Packed
RGB 8,BGR 8 ;BGR 8是最友好的,采集后可以直接用 cv::Mat 处理,不需要额外的转换开
销
对于工业相机采集得到的原始数据,OpenCV可以直接处理第一种和第四种,而Bayer格式和YUV系
列的则需要经过手动转换或者使用相机自带的SDK进行格式转换,才能为OpenCV所处理(OpenCV
似乎也提供了转换的API)。实际上大部分相机在拍摄时的格式都是Bayer RG,这是因为像素的实
际排列并不是规则的RGB三种像素(一个像素单元实际上只能感受一种波长的光):
既然绿色像素比红蓝像素多,其转换的时候必然要进行权重的矫正。对于没有对应颜色像素存在的
地方,转换的时候肯定要进行插值,选择的插值方式(线性、双线性、泰勒)对于转换速度/转换质
量的影响是比较大的。
如果能够直接让相机采集某种你需要的格式的图片,那么直接对相机进行设置,而不是采集到图像
之后再于程序中对图像进行编解码转换等。相机内部一般有asic或fpga芯片,可以快速完成这些工
作;而对于你的cpu来说,这是一个不小的负担。
当初华为手机在宣传其摄影功能的时候就提到它使用的是RYYB的排列以最大程度提高感光能
力,不过缺少绿色像素,其成像即使经过算法矫正也通常偏黄。
上方为USB3.0,左下方为数字IO
链接方式:即相机连接到运算平台使用的硬件定义和通信协议。对于工业相机而言,我们会用到
的、最常见的有USB3和工业网口GigE,工业场合历史遗留或标准遗留的包括CamerLink和
IEEE1394,由于通用性和可扩展性不佳正在被逐渐淘汰。CoaXPress和CameralLink这种超高速的
接口,由于通用性受限、价格昂贵,我们一般用不到。现代工业相机一般还支持自定义的可编程数
字IO接口,即下图中的圆形6-pin数字IO:
No. 41 / 352


6-pin接口一般包括光耦输入和输出以及光耦地(这两路信号都通过光耦二极管进行电气隔离形成保
护),电源和电源地,还有一路可配置GPIO(可以配置成输入或输出)。额外电源一般在USB接口
供电能力不足或高负载工作的时候才需要连接,GPIO以及光耦输入输出三条线可以用于硬件触发、
采集触发等功能,对于与其他设备同步采集时间戳是一个非常有效的解决方案。
USB连接线在相机一端一定要选用有固定装置的接头(相机上都有由于固定的螺孔),防止松动导
致连接异常,损坏接口甚至由于不明原因的异常导致相机内部DSP损坏。USB和GigE虽然都支持热
拔插,但还是尽量停止取流并断开相机后再行拔插,笔者使用海康威视工业相机的时候就常因为连
接不稳数次后相机亮红灯,而一些机械结构的设计又让重新拔插分外困难。连接到运算平台的一
侧,在确认机械结构不再改动后可以打上热熔胶或使用3D打印件进行固定防止松动。
4.2.运算平台
常见的运算平台有这几种:定制的minipc、Intel NUC、jetson系列、DJI manifold2(有cpu版本和gpu
版本,分别相当于同配置的i7-8265u的minipc和jetson tx2,不过manifold的体积很小)、工控主板/工
控机、OpenMV、k210、k510等。其实选型的空间并不大,不过需要大家根据预算平衡一下性能和价
格。
4.2.1. 运算平台的性能指标
那么如何评价一个运算平台的性能呢,这里要提一下CPU和GPU运行的概念。CPU的时钟频率高,但是
内部的运算单元(ALU、FPU等)数量有限,是为通用计算和程序控制所设计的,其实并不擅长进行大规
模的并行运算,比较擅长单线程的流水线处理。GPU则相反,GPU有大量的低速运算单元,但是能够一
次性处理巨量并行数据,因此尤其适合图形计算(相当于每一个运算单元计算一个像素的相关数据)。
视频解码、单线进行的程序适合在CPU上运行;图形处理、显示渲染则适合用GPU进行。这也是为什么
大家都说玩3A大作这些大型游戏需要一张好的显卡(GPU)。
No. 42 / 352


GPU和CPU的对比;图源https://zhuanlan.zhihu.com/p/156171120
当然,也有专门用于各种运算的TPU、NPU、APU等等等等,他们都属于ASIC(application specific
integrated circuit),直接将一些特殊运算(比如神经网络中的激活函数、矩阵乘法、非线性运算)或
者软件功能固化在硬件中,以提高处理速度。
常听说的DSP(digital signal processor )其实也可以看作一种ASIC,如相机中用于采集cmos信
号或专能于图像处理的芯片。
一般用于评价性能的算力指标有GOPS(Giga Operations Per Second),MOPS(Million
Operation Per Second),TOPS(Tera Operations Per Second)。若有F如TFLOPS,则是衡 量浮点运算能力。英伟达官方也有一套衡量N家显卡机器学习算力的标准,参见CUDA GPU |
NVIDIA Developer。若显卡的compute ability大于5,一般来说就比较适合进行机器学习的训练。
(但是貌似官方没有给出定量的计算方法,可能是根据自家cuda内核数、tensor单元数和主频等参
数通过一些加权方法得到的分数)
注意,不同类型的数据运算速度也不同,因为CPU、GPU等有专门的ALU或针对某些运算优化的指
令集。常见的类型有INT8/16/32(8/16/32位整型)和FP16/32(16和32位浮点),也有较为少见
的FP8以及一些”奇怪“的混合精度计算。
在RM比赛中的识别算法中,传统的灯条匹配算法是更依赖CPU的,但是因为涉及到矩阵运算,
OpenCV支持的一些图形库能够利用电脑GPU的能力进行加速(intel自家的ippcv以及OpenCV
Contrib的cuda库)。而新兴的基于卷积神经网络的目标检测算法则是非常依赖电脑的GPU性能。
不过,Intel推出的OpenVINO部署平台(仅仅支持Intel的cpu)和腾讯优图开源的NCNN(适合
arm架构)推理框架都能够通过优化CPU的运算来提高神经网络的性能。英伟达的TensorRT推理框
架则是只支持自家的GPU(jetson平台上也可以部署),能进一步提高神经网络的推理速度。
No. 43 / 352


4.2.2. 设备介绍以及benchmark
为了提供性能的大致参考指标,这里介绍一下下文所说的Nanodet(我们使用的是Nanodet-m,
320x320的输入,backbone使用的是shuffleNet V2)和传统算法。Nanodet是一款超轻量化的目标检 测模型,是基于卷积神经网络的目标检测算法的代表之一,可以以此作为其他目标检测算法的基准
(benchmark);下文提到的传统算法则是基于特征提取和灯条匹配完成的。我们在第五部分、第六部
分会分别详尽地介绍这两个算法。至于传统算法,以i7-8265为例,按照一般的处理流程可以运行
150+fps,若加入ROI则可以跑到300fps以上。而i7-1165G7则能实现1ms左右波动的单帧处理时间。
以下分别介绍一下各个运算平台:
minipc:搭载i7-8265u, i7-8565u的minipc较为常见,在tb上能够找到各种大小的minipc,至于那
些搭载intel J系列和N系列的Genimi Lake架构的工控机、软路由、电脑棒、计算卡之类的玩意,是
根本带不动神经网络的,如果是J1900甚至解码视频流都会出现卡顿。我们实验室曾经为了缩小体
积购买了一台40x40x40大小的N4100 cpu的minipc,结果就是神经网络的目标检测大约10
12fps,传统的灯条匹配在不进行数字识别、环境光线简单的情况下也只有50-60fps。所以,从性
能的角度考虑,至少是标准架构的i5-7代之后的cpu才能胜任RoboMaster赛场的视觉任务。
更新:现在有AMD锐龙的minipc下场了,参考配置为R7-4800H和R5-5600U,性价比都还可
以,不过外壳都做的比较大,参考价格2000RMB。关键的是,Zen2+在完善了AVX512指令集
后似乎也兼容OpenVINO,笔者的电脑(amd r5-5600H)可以正常使用OpenVINO
21.04LTS。
Intel NUC:8代和9代的nuc和上面提到的miniPC并没有什么区别,价格还要贵一些。而nuc10在
nuc11登场后就成为了一个性价比比较高的选择,最近NUC11的价格也非常的不错。nuc11建议购
买i5以上的配置,intel在第十一代cpu中挤了很多牙膏,用锐炬Xe显卡替代了万年不变的
UHD630,这让nuc上运行神经网络成为可能。我们实验室同样配置了一台nuc11猎豹峡谷,cpu为
i5-1135G7,运行传统算法能够达到250-350fps(全图检测),加入ROI检测后甚至处理一张图片
只需要1ms左右的时间,速度不可谓不快。并且配合Intel OpenVINO推理框架,即使使用
OpenVINO推理神经网络也能达到很好的效果。我们实测运行nanodet网络,使用
Vulkan+NCNN/MNN/LibTorch对Xe显卡加速后能够达到50-60fps的速度,若运行OpenVINO推理
框架则拥有约150fps的速度。选择i7系列的nuc11,并且配置更高频率的内存条(核显没有显存使
用的是内存因此需要更高频的内存来提供访存带宽),应该能够有进一步的提升。11代最高支持
3200MHz的内存。
参考算力:i7-1185G7的核显拥有1.9FLOPS的fp32算力。FP32推理YOLOX-n拥有50fps的速度。
推荐购买薄款,厚款其实就是增加了一个2.5inch的机械硬盘位,当然也可以购买厚款然后把外壳拆
了让机械组的同学帮忙设计一个亚克力外壳或3d打印一个外壳。
工控主板:现有派勤intel 11代的工控主板,相当于NUC的翻版,只不过砍掉了雷电接口和无线网
卡,但是价格同样直接腰斩!(单单主板2000以内)散热能力稍逊于NUC11,其他方面持平。而
且拥有板载串口、USBo1/2,自带2个网口,非常方便扩展。只需要自己用3d打印或板材拼接制作
外壳即可,是替代NUC的不二之选。若后续推出AMD的工控主板,性价比应该更高。其他品牌也有
推出NUC的平替,体积一般和NUC的厚款相似,可以拆卸外壳制作亚克力,tb上搜索有一大把,不
再赘述。
Jetson系列:使用jetson系列主要是看重其GPU性能。目前能够使用神经网络检测达到实时性要求
的设备,应该只有jetson tx 2(有些吃力)、jetson Xavier NX和jetson Xavier AGX。若使用
Tensorrt框架部署后,Xavier AGX的int8算力接近1080ti,浮点算力和1070持平,是当下最强力的 边缘运算平台。Xavier NX的算力是AGX的2/3~3/5左右,tx2就要更劣一筹。其大小相对minipc和
No. 44 / 352


NUC来说有比较大的优势,哈工深的同学自己定制了一块Xavier NX载板,进一步缩小了运算平台
的体积。不过由于jetson搭载的是ARM64架构的cpu,可能在某些软件的兼容性上有一些问题,如
conda等,好在大多数可以在网上找到解决方案或替代方案。GPU类型设备适合推理标准卷积架
构。不过Jetson系列的CPU稍显孱弱(相比x86大核)。
不推荐使用jetson nano、jetson tx1。
更新:Jetson nx以下的开发板现在应该是非常吃力!!Nano和TX2强烈不推荐,而且现在溢
价非常严重。
更新:新版的Jetson NX orin和Jetson Xavier orin在2022的第三和第四季度马上要推出了, 算力爆炸(当然价格也可能爆炸)。
更新:Jetson Xavier AGXorin已经上市,int8算力是Jetson Xavier AGX的八倍不止...... 目前经 销商能拿到的价格是16999。
DJI Manifold:主要优势就是体积和重量,若不差钱且机械对体积重量要求高则可以选择这款。但
是使用改造载板的Jetson Xavier NX应该在价格体积重量性能上完胜Manifold G系列了(没有催更
DJI的意思,反正就算更新了也买不起)。
极度不推荐使用 Manifold1
更新:不再推荐使用DJI Manifold,刷机过于复杂,而且串口外设容易出现一些奇怪的bug。
manifold2则已经停产,二级市场价格直接翻倍。求求快出Manifold3吧。
华硕旗下的研扬工控仍然在推出和manifold2C大小相仿,装载laptop级cpu的卡片式工控电脑。价
格和妙算也类似。如果有极致需求的队伍可以考虑购买。
OpenMV:一般指基于stm32f7和h7系列,刷入openmv固件的嵌入式设备。32位架构的处理器算
力在复杂的视觉处理场景明显不足,这里提到openmv主要是在飞镖系统上可能会用到这款运算平
台,其扩展能力可以同时兼顾运动控制。openmv相对其他的运算平台优点就是体积非常小非常
轻,并且编写其程序使用micropython,开发的速度很快。openmv也可以为机器人增加辅助视觉
功能,提供一些不那么复杂的算法处理工作。
k210/k510:勘智科技推出的人工智能处理芯片。特点是体积小,非常小!现在市面上能买到用
micro-python开发的maixpy出品的k210芯片。AI算算力在同价位和大小属于独一档。特别提一下
最新推出的k510芯片,算力到达了2.5TFOPS,超过了jetson tx 2的2.1 TFPLOS!如果队伍内的电
控组或硬件组有能力的话,可以让组内的同学根据datasheet制作pcb板,进一步缩小运算平台的体
积。nihui大神已经成功把ncnn移植到RISC-V上(即k210和k510的处理器架构),如果后续能移植
linux mint等微型linux系统到k510上(机械狂喜ohhhhhhhhh),将会大大便利在这些soc上的开 发。并且这些芯片同时提供了低速IO外设,从这里也可以看出它们是专为嵌入式边缘应用打造的。
k210适合做飞镖制导,或作为机器人的辅助视觉单元,而k510的超强算力(同体积重量来说)甚至
能够胜任整个机器人的控制!不过由于其cpu相比其他平台还是稍显孱弱,除非能在视频解码上进
行优化,否则帧率可能会受限。
更新:K510已经上市,参考价格为999RMB,目前只有勘智官方自己做的开发板,体积比较
大。
No. 45 / 352


FPGA:全称为Field Programmable Gate Array,是可编程逻辑器件(PLD)的进化产物。要通过
CPU这种固化的数字逻辑电路实现不同功能,就产生了编程语言和代码,利用复杂的软件实现对单
一硬件的多功能扩展(ALU只支持那些在设计之初被固化在硬件中的操作,如加法器,乘法器
等)。而FPGA则是让你拥有直接对硬件编程的能力,你需要什么样的运算,就能使用硬件描述语言
(HDL,hardware description language)直接构建对应的运算逻辑器件。使用FPGA的时候,你
就是芯片的设计师!当然,现代FPGA为了方便使用,常常在外部挂载一些通用性非常强的外设如串
口、CAN、SPI等。FPGA相比CPU、GPU等通用运算芯片,最凸出的优势就是硬件级的大规模并行
和几乎零额外开销的算法实现。HDL的编程范式也和软件编程语言有很大的不同。
笔者没用过FPGA,之前的课设都是找同学帮忙做的。如果你对FPGA比较了解,可以为本小
节贡献内容,提出一些选型建议!沈阳理工大学开源了一种飞镖视觉制导的FPGA方案,参
见:飞镖视觉1.0开源基于FPGA和传统视觉的光点追踪。(笔者的FPGA课设仿真激励文件就
是这位大佬帮忙写的,感谢万能的群友哈哈哈~)
各个运算平台大小的对比,从左起依次是k210、openmv、某intel N4100的minipc、Jetson
nano、jetson Xavier NX、i7-8265u的minipc、intel NUC11猎豹峡谷、intel NUC7 Bean
canyon(豆子峡谷)
各种Linux/Android开发板:例如全志和瑞芯推出的各类arm64架构的开发板,火爆的RK系列,以
及荔枝派、香橙派各种派等树莓派的平替。arm64对于ncnn、mnn等推理框架的支持较好,适合推
理深度可分离卷积的模型。不过目前似乎没有队伍尝试使用此类运算平台。
有些经费比较宽裕的队伍可能会专门搭建一台用于队伍网络服务的服务器+深度学习训练机+雷达站
运算端。若预算不足,则可以选择在一些提供云服务器训练的商家购买GPU核时进行训练,价格也
不算太贵。现在的显卡价格实在过高,不适合自己额外购置显卡,如果有钱当我没说。(挖矿的都
给爷爬)
若要搭建雷达站又苦于显卡太贵,这里推荐雷达可以使用搭载较好显卡的游戏本或者intel NUC 11
幻影峡谷:i7-1185G7+RTX2060的配置进行前向推理还是没有任何问题的,平时若有需要还可以用
于训练小模型。
22/Aug更新:30系显卡已经跌破发售价,刚需的小伙伴可以入手了,从此炼丹再也不用求人
了(没事的时候还可以试试RTX ON)。
再更新:以太坊解算制度修改,比特币算力翻番,现在大家都用专用asic矿机挖矿,显卡终于
自由。Pytorch也在最近释出ROCm支持的正式版,锐龙显卡
之前在RM视觉交流群 就有小伙伴提出大家可以利用各自手里有的运算平台测试各种网络和传
统算法,制作一个modle-zoo,如果有想法的同学或者已经测得数据的小伙伴可以戳我,让
我们一起完善这个表格哦!
2023增补:现在小主机突然火爆,利好机器人领域!小米、华硕、机械革命、零刻等纷纷推出自己
的小型主机(Intel退出中止NUC业务,交由华硕维护)。
No. 46 / 352


4.3.IMU
IMU对于RoboMaster比赛十分重要,是电控实现“小陀螺”、控制平衡车的基础,也是视觉算法预
测是否准确的根基。我们需要云台和机器人的姿态数据,以实现对目标在绝对系下的准确位姿测
量。更多关于这部分的内容在第六章击打预测和弹道模型将会介绍。
全称为Inertia measurement unit,惯性测量单元。一般包括加速度计和陀螺仪,气压计、磁力计等也
可能被一起集成。这里主要介绍MEMS IMU的原理和对其输出数据的处理。
MEMS:micro-electronic-machine system,微机电系统。它是集微传感器、微执行器、微机械
结构、微电源微能源、信号处理和控制电路、高性能电子集成器件、接口、通信等于一体的微型器
件或系统。下面随笔者一起看看介绍MEMS IMU的结构,你就能理解上述定义了。
工作原理
先看看微机电陀螺仪的实现。我们知道加速度的来源是力,因此需要通过某种物理效应将力转化为方便
测量的量,对于现代传感器来说,大部分选用电作为信号载体,因为相关的信号处理技术已经非常成熟
(而新的发展方向是利用光,其功率更低载波频率更高)。考虑下面的一个质量块弹簧阻尼系统:
质量块M在受到力F的作用的时候,将发生运动;由于阻尼器的存在,质量块最终会在新的位置停下来:
这个位置满足 。学习过自动控制原理/电路的你应该知道,可以用微分方程/传递函数描述这
个标准的二阶系统。只要K是已知的,我们就能根据位移量x确定力的大小。相信此时你已经有了不少方
案,最简单的一种也许是在质量块上搭载滑动变阻器抽头根据电阻分压确定位移。不过,为了让我们的
传感器足够小,小到能够放进一块芯片当中,并且对外部的加速度有足够的敏感性,我们采用如下的方
案:
No. 47 / 352


MEMS加速度计的原理,实际上有3组这样的器件,分别置于三个两两垂直的方向;图源
HowToMechatronics.com
上方为惯性系视角,下方为旋转系视角(红点不动)
绿色的小条是电容的极,一个质量块被固定在两个微小的弹簧上。当受到加速度的时候,质量块由于惯
性将会受到反向的“惯性力”,此力的大小就等于加速度乘上它的质量;在短暂的过渡过程结束后,质量块
将停止在某个位置上,此时其位移就满足我们前面得到的公式。我们选用检测电容的方式确定质量块的
位移,质量块两边延伸出的长条是导体材质的,将和绿色小条形成电容。根据电容和极板距离的关系:
求得距离的变化量,再根据 得到力的大小,最后由 求得加速度大小。
之所以要“两片夹一片”的电容构造,是因为使用了传感器中常用的提高灵敏度、减小非线性误差的
方法:差分感知。学习过传感器相关课程的同学肯定不陌生。实际上MEMS加速度计中包含了数十
组这样的差分电容片,通过求平均的方法降低随机误差。
再看看MEMS陀螺仪,高中的时候就学过物体在旋转时会受到向心加速度,可以据此原理进行检测。但
是在设计上太难实现,而且很难区分旋转产生的向心力和物体受到的其它力。因此,我们通过间接的方
式对旋转进行测量:科里奥利力。如下图所示,考虑一个光滑的竖直圆盘和一个自由下落的小球,由于
参考系的选取问题,若一个物体在转动参考系上发生径向运动,转动参考系上的观察者会发现物体实际
上走过一道弧形。因此在旋转系下看,物体受到了某种虚拟的惯性力作用。我们就可以利用这个虚拟的
力对旋转进行检测。由公式推导可以得到这个惯性力的大小为 , 为旋转系的角速度,m是
物体质量,V则是物体在绝对系的速度。
那我们的设计思路就是,让一个质量已知的质量块以某个速度运动,并以适当的方式检测出惯性力的大
小,从而推得旋转速度。像上图小球那样一去不返的运动显然是不可取的,我们通过对质量块施以周期
性的力使其在限定范围内往复运动,它的速度是可以获知的。然后,在其内部再添加一组运动方向和前
者垂直的质量块(内部的结构和加速度计相同,即让惯性力 反映为位移大小),一旦发生旋转,内部
的质量块就会受到科氏力从而发生位移,通过电容的变化将检测出位移大小,从而推算惯性力的大小,
再由 得到物体旋转的角速度。
No. 48 / 352


MEMS陀螺仪的原理,这里同样只展示了一个轴;图源亚诺德半导体
这是MPU-6050的晶片显微镜视图,依稀可以看见两个传感器的排布
可以发现,上图中内部用于检测科氏力的加速度计装置和外层框通过弹簧进行了隔离,这就能在很大程
度上防止装置在受到纵向的加速度时产生误动作。
关于两个传感器工作原理的详细介绍,可以参考MEMS 陀螺仪工作原理和MEMS 加速度计工作原
理,都是Analog公司的科普作品,非常详细。
若集成了气压计,IMU还可以检测海拔高度从而与加速度计在z轴方向上的数据融合,提高精度。引入磁
力计则可以通过检测地磁的方向以确定IMU的位姿,与陀螺仪融合。
RoboMaster 开发板C型(C板)上集成了一颗博世生产的六轴IMU(即三轴加速度,三周角速度)BMI
088传感器,并内置了恒温加热电路和减震海绵垫。同时,C板旁还有一块iSentek生成的三轴磁力计芯片
IST8310。
No. 49 / 352


数据处理
温度常常是影响各种传感器精度的一大因素,因为材料的物理化学性质基本都会随着温度发生变化。C板
上集成的加热电阻可以很好地控制温度保证温差的影响被降到最低。可以使用PWM+PID对电阻发热进行
控制。虽然陀螺仪通过科氏力检测角速度已经最大程度降低了加速度对检测结果的影响,但难免还是会
有检测误差。陀螺仪最常见的误差类型就是零漂,即IMU在禁止状态下仍然会产生一个很小的角速度输
出。因为我们一般通过对陀螺仪的输出进行积分以获取位姿,因此零漂将持续引入误差,最终累计到不
可接受的程度。加速度计也有量程的限制,并且对阶跃输入、撞击很敏感,在外力频率极高或幅值大的
情况下会输出奇怪的数据,同样也需要利用某些手段进行矫正。
实际上加速度计也存在零飘,以及两种传感器都有随机游走的噪声。已经有一些开源的工具包可以
对imu进行动态和静态标定。一种有效的分析噪声和误差的方法被称作Allan方差标定,感兴趣的同
学可以自行搜索。不过在电控场景下,标定陀螺仪的零飘和imu的放缩系数一般来说已经足够。而
对SLAM场景,一般需要与其他传感器进行更高精度的联合标定。
对于六轴姿态融合解算,一些经典的算法包括Complementary(互补滤波)、Mahony、Madgwick。
它们都利用加速度计的数据对陀螺仪的数据进行修正(因为总有一个重力加速度,因此加速度计能较好
地估计静止状态下的姿态)。要学习位姿解算,你首先要拥有坐标变换和旋转描述的知识,我们在5.6.1
提供了最基础的必要工具,并在5.6.5增加了四元数和欧拉角相关的扩展以供选用。
这里我们暂时不考虑磁力计和气压计,这是因为机器人附近的电磁干扰太大:电机、超级电容等都
会产生较强的磁场,导致引入磁力计后姿态解算效果反而更差。对于地面上工作的机器人,高度几
乎不怎么改变,气压计谓之鸡肋。
当然,有很多开源库已经实现了大量方法。AHRS库包含了常用了各种位姿解算算法,可以轻松通过pip
安装Attitude and Heading Reference Systems,它的文档非常详细,对于算法原理的介绍与模型调
用、接口定义等的描述一应俱全(遗憾的一点是只有python的实现,不过理解原理之后你就可以用高性
能的语言对其进行转写了)。Fusion是Mahony解算的官方实现,加入了磁力计(你也可以选择去除)
融合,提供了C/C++的接口,不过也可以使用python调用。RoboMaster C型开发板的姿态计算例程,使
用的就是Mahony算法(但听说好像有些地方存在小bug?)。
状态估计/控制算法实践这个专栏较为详细地介绍了姿态解算中用到的知识和Mahony解算,以及作者自
己实现的EKF四元数融合算法,在有了卡尔曼滤波器、PID算法和四元数的基础之后,推荐阅读。
4.4.激光雷达
雷达(Radar,RAdio Detection And Ranging)是一种通过发射电磁波并根据回波强度和时间判断物体
距离的一种传感器。那么激光雷达的工作媒介就是激光(故称LIDar,LIght,实际上激光也是电磁波)。
在自动驾驶大热的今天,激光雷达、毫米波雷达、IMU、相机和GNSS系统可以算是Auto-driving界的五
虎上将(有时候还包括超声雷达)。根据工作原理的不同,可以将雷达分为TOF(time of flight,飞行时
间法)和FMCW(frequency modulated continuous wave,调频连续波)两种;根据工作范围也分为
点式、线阵和面阵三种。还有其他的分类方法如根据扫描方式和发射/接受方式分为机械旋转、MEMS、
转镜、棱镜、Flash等...... 我们主要介绍工作原理的分类,并分析在RM比赛场景下激光雷达可能的用途。
No. 50 / 352


Livox MID-70激光雷达;参赛队伍可以以优惠价格购买供开发使用
工作原理
TOF的原理非常简单,雷达发出光脉冲并检测反射受到脉冲所用的时间,乘以光速便得到距离。为了保
证收到的反射是自身发出的,一般激光雷达会发出特定编码的脉冲序列(相当于某种“通信协议”),接收
时要检测到该序列才视为对准。(这也是不同厂家、不同批次的雷达防止“串号”的原理)
FMCW的原理稍微复杂一些,雷达通过发送和接收连续某种频段激光束,让回光和本地光发生干涉,并
利用混频探测技术来测量发送和接收的频率差异,由发送和接收波的频率差得到距离。此外,FMCW雷
达还可以测速:利用多普勒原理,激光束击中目标物后被反射,而反射会影响光的频率——如果目标物
向车辆走来,频率会升高;如果目标物和车辆同方向行走,则频率会降低。当反射光返回到探测器,与
发射时的频率相比,就能测量两种频率之间的差值,从而计算出物体的距离信息。更详细的介绍参见:
详细分析FMCW雷达测距/测速原理,需要有信号与系统/数字信号处理的基础,或者了解调频和调幅的原
理。
参数
这里以Livox MID-70(棱镜扫描式)为例激光雷达的一些关键参数包括量程、近处盲区、FOV(field of
view)、测量误差、光束差角等。量程一般以回波强度至少为100klx记,对于不同反射率的物体有着不
同的量程。由于当激光脉冲出射的时,碰到镜头将发生反射,这时候回光接收器将检测到此反射波从而
出现误动作。为了避免误动作,一般加入一个不响应的死区时间。如果障碍物距离太近,接收器仍处在
死区内就会使得近距离物体的脉冲回波无法被探测到。激光雷达的盲区范围基本都小于0.4m,因此对于
大部分应用不用太过担心。FOV是面阵雷达的常用参数,和相机一样表示可视范围。测量误差主要是随
机误差,一般小于5cm。光束差角则是衡量雷达的(角)分辨率的参数,即发出的光线的致密程度。Livox
官方对其雷达的具体原理和所用技术进行了科普性的说明,讲得很不错,戳这里:解密Livox激光雷达原
理系列(高低要吹吹牛对吧)
数据处理
从激光雷达的工作原理我们很容易得知,采集得到的结果一定是一堆点,每个点都记载着和传感器的距
离和角度(回忆FOV的概念,接收器会接收不同角度射入的回波),而在实际中为了方便处理一般转换
成以雷达为原点的笛卡尔坐标。数量众多的点就形成了点云,因此数据处理方法主要式点云处理算法。
我们在5.6.6会大致介绍一下点云处理,前提是有二维图像处理的知识(或你已经有相关背景)。Livox官
方也提供了开发SDK和一些开源的算法实现,包括点云去畸、配准、雷达自动标定、相机联合标定、车
道检测、点云目标检测等,戳这里:Livox-SDK。
No. 51 / 352


配合IMU,激光雷达在城市道路上生成的点云图;不同的颜色代表采集该点云时与激光雷达的距离
赛场应用
和IMU配合工作,激光雷达可以搭载在机器人、汽车上移动并实时获取点云信息,根据位姿的变换将测
量结果投影至三维地图中,完成三维重建、同步定位与建图(SLAM)等。和相机配合工作,激光雷达与
相机进行联标后,可以确定RGB图像上每个像素在点云中的对应点,从而获取其深度信息以进行精准测
量。因此,对于自动步兵来说,激光雷达肯定是不可缺少的部分了,若将面阵雷达置于云台上,则可以
替代线阵雷达并且可以建立三维地图;同时,还可以将激光雷达和云台相机联合标定,又能够实现距离
测算以进行枪口补偿,一举三得!而雷达系统(此处指的是兵种)中使用激光雷达也是非常不错的选
择,毕竟LiDAr可以提供相机视觉系统无可比拟的测量精度。另外,对于哨兵机器人的自动运行来说,激
光雷达几乎是必不可少的。
在SLAM部分的介绍中,我们会进一步了解和激光雷达相关的算法,以及如何标定激光雷达与其他传感器
等。
大赛组委会还为激光雷达专门设立了创新开源奖,如果你能想出更多有趣的应用,在取得出色效果
的同时,说不定还能拿下额外的奖金!
4.5.特殊相机
双目相机
双目相机应该是最早出现的也是最成熟的立体视觉方案。利用两个相机的视差和他们固定的相对位
置信息,通过3d数学的几何解算方法计算对应点之间的位置偏差,得到物体的深度信息。关于双目
相机的测距原理,请参考《机器人视觉测量与控制》这本书。这篇教程较为详细地讲述了双目相机
测距的基本原理和用法。
No. 52 / 352


双目相机
OpenCV也提供了双目标定的例程和大致的原理说明。同步双目相机(两个相机在一块pcb上,共用
一个相机号,在Linux系统下只有一个文件描述符)的标定要比两个单独的相机更容易标定,若使用
两个单独相机也需要尽量让参数保持一致以便得到更简单的解算模型。
双目相机通过视差由相似三角形得到的测距精度显然是要比单目高的,不过解算的开销要远大于单
目。但是提升的精度对于我们补偿算法的影响其实并不大,而且高精度也带来对误差敏感性的提
升。单目相机在目前的RMUC赛场是应该是足够满足我们的需求了。不过,了解双目标定和解算的
原理对于理解3维重建、学习3d数学与线性代数是有很大的帮助的。我们会在 6.3 中详细介绍距离
解算和三维重建的内容。
西北工业大学WMJ战队在2021赛季采用了双目视觉,可以看到他们的哨兵云台拥有一个像
《机器人总动员》中WALL·E一样的长条形大脑袋。不知道他们会不会开源他们的双目视觉解
算方案呢~
双光相机
可以采用NIR(近红外)+RGB的双目相机组合,这在目标检测领域这是一个新兴的方向。红外成像
可以弥补低可见光照条件下的噪声影响和细节信息的丢失,若采用这种组合则可以进行优势互补。
机器人的电机、超级电容、电源管理模块发热量都较大,在近红外相机上会有明显的响应;而荧光
充能模块的发热量更是惊人。
最简单的利用方法是直接把NIR采集的图像追加到rgb之后,将神经网络的输入变为四个通道,让后
续的层自动抽取特征。我们也可以手动添加先验,使用类似transformer的方法给予RGB抽取出的
feature map上NIR通道有高响应的区域以更高的权重(乘以一个系数)。
对于传统的handcrafted方法和装甲板检测,红外通道应该帮不上忙。而且这种相机很难用于测
距,因为两个相机采集到的特征不同,我们无法在RGB相机上寻找NIR相机的特征点(关于这部分
可参见本教程的双目相机和3D视觉部分)。
遥感卫星和航拍无人机上会搭载一些高光谱相机,拥有多个不同波段的镜头,可以同时获取各种光
谱信息进行分析,在农业、防灾和测绘领域有广泛应用。
深度相机
生产深度相机的规模化厂家目前主要有4个:ZED、intel Realsense、奥比中光、Kinect(还有和
OpenCV合作,最近主推的Oak)。大部分厂商采用的解决方案都是双目➕TOF/结构光的方案,相
机内部的DSP芯片会融合多个信息来源的图像进而结算得到深度图。使用深度相机的时候,厂家一
般都是对其进行了彻底的封装,我们对于其原理只需要有大致的了解即可。
在第五章三维视觉基础的介绍中,我们会更详细的讲解不同方案深度相机的深度估计原理。
No. 53 / 352


intel深度相机拍摄得到的深度图像,像素的不同颜色表示距相机的距离不同
Intel D435深感相机,采用usb3.0-typec连接
在上图中由紫色到红色表示距离的远近,黑色则是因为遮挡或相差导致双目相机无法解算生成的深
度空洞,深度相机一般也有一个空洞率的参数,越低越好。
相机提供了相应的SDK供我们进行二次开发,调用对应的函数接口就可以得到一幅深度图,每个像
素的value即为相机视野范围内对应点和相机的距离。不过商用产品的闭源特性让我们很难进行深度
DIY(深度相机还是不够有深度呀~)。由于需要内置DSP对图像信息进行实时解算,限于体积和功
耗,目前市面上的深度相机帧率多少都有些不足,最高的为intel D435,能够在800x640的分辨率
下达到90fps的速度(深度图90Hz,而RGB光学相机的采样频率是60Hz故同步时也仅有60Hz)。
No. 54 / 352


笔者所在学校的实验室居然还有一个kinect Xbox360体感相机,确定不是买来玩游戏的吗...
深度相机由于开发方便,赛场场地小,经常被用作视觉SLAM中的一环,有些深度相机同时集成了
IMU(惯性测量单元),使得机器人的位姿估计可以轻松进行。在工程机器人兑换矿石的任务中,
很多学校设计了基于视觉(或结合了其他传感器方案)的自定义控制器,如南方科技大学ARTINX和
南航长空御风的控制器就是用了intel T265这款直接输出相机位姿的传感器。bilibili平台搜索他们的
战队可以观看相应的视频。
事件相机
事件相机是一种新兴的仿生传感器。与一般的曝光成像相机原理不同,事件相机的像素只有3种输
出:-1,0,1且每个像素都是独立工作的。0表示这个像素的亮度和之前相比没有发生变化,-1表示
这个像素之前是被激发的而当前变成了未激发,1则和-1相反。由于其只检测像素的亮度的变化,每
个像素需要的曝光时间都非常短,因此有动态范围极高、事件无运动模糊等特点。其简要工作原理
如下:
No. 55 / 352


这里C2的作用是和C1一起调节放大器的增益
(a)中的具体电路设计则是:
左侧的光感受器能够将光电流大小按对数比例转化成电压 ,中间的差分电路用于计算两次采样的
电压差,输出到右侧的比较器电路,当压差超过阈值则完成一次触发,触发方向取决于压差的方
向,是相比之前升高还是下降。单个单元的工作的流程如下,当一次事件触发之后,差分电路的
reset开关会闭合,使得C2和放大器被短路,导致C1接地(上图没有画出,前面的图(a)中的reset则
连接了地),其两端的电压保存为上一次触发的电压 ;随后reset断开,新的一个周期光电流产
生电压 ,由于C1没有放电通路,C1两端的电压应该保持不变,此时C1左侧电压变为 ,为
了保证电压不变C1C2之间的结点电压则要变为 ,从而使得C1的压差仍然维持
在 。这样,两次采样的电压差被反向放大器放大之后输入比较器,以判断是否超过正/负阈
值。C2的作用是和C1一起形成差分电容放大电路,用于控制在C2右侧输出的电压即输出电压和
C1C2之间节点电压即输入电压的增益,若你学过模电便知道这是一个经典的反馈放大器,可以自己
列式计算增益。因为不同单元的放大器的增益不可能完全一致,因此通过设置 的值,可以放大倍
率一致,防止出现不同像素触发门限不同的情况(when in trouble, use feedback!)
相比之前介绍过的RGB相机(或其他expose-based camera),事件相机单个像素的结构要复杂许
多,因此分辨率相比其他相机有较大的劣势,目前主流的事件相机分辨率在3MP以下,超过该值的
品类价格则非常恐怖。
感谢Dr.yyy对事件相机电路的分析。
No. 56 / 352


当触发事件时,事件相机后续的逻辑模块会记录这个像素的坐标、激发方向(-1or1)和激发时间。
每次激发,事件相机都会立刻将数据反馈给上位机(当然也有累积一段时间的事件然后发送整个事
件包的,和激光雷达非常类似)。由于亮度没有发生变化的那些像素不会产生输出,因此最佳的观
察事件相机生成的“图像”的角度是把时间加上,从 的三维视角去分析其产生的数据。
事件相机常用于高速物体运动估计、动作捕捉,暗光环境的动态物体观测,以及一些新兴场景的
SLAM。
4.6.低速IO外设
在处理完图像并得到装甲板、能量机关扇叶相对相机中心的角度或找到救援目标和矿石之后,我们要设
法和电控通信,让下位机能够接收到数据,告诉机器人目标在哪个方向、敌人处于什么状态,进而控制
电机、气缸等来执行我们的任务。同时我们也需要从电控获取一些信息,如敌我的颜色、当前工作模式
(自瞄、激活能量机关还是手动)、裁判系统读取到的各种状态等。以下介绍两种通信方法和对应的协
议。CS专业、了解过计算机网络或学习过嵌入式开发的同学对此应该不陌生。
4.5.1. 串行通信
常用下位机通信方法之一串口(Serial Port)。为了方便简单起见,我们使用的都是异步串行通信,通过
一个usb转串口芯片将解算好的数据发送给电控,电控经过一定处理后再控制电机动作。
串口通信的基本知识:在最简单的串口使用中,通信需要两条线路,一条用于发送,一条用于接收
(其实还需要一根共地连接线)。顾名思义,串口通信就是将数据像“串”一样一位一位的顺着一条
线发送出去。任何通信方法都需要制定通信协议,串口通信也不例外。异步串口通信中需要通信的
双方提前约定好波特率、数据位、停止位、奇偶校验位。因为使用的是最简单的两线通信,需要关
闭硬件流控制。关于串口通信的更多基本信息,请参考What is Serial Communication and How it
works?
串口在Linux上作为一个设备,同样会以文件的形式挂载在 /dev 分区,使用时需要赋予串口权限:
若不想每次使用都输入一次,希望永久赋予串口权限,参考这个教程:永久打开串口权限
串口通信协议:通信发送开始时,先发送一位0表示开始发送,紧接着是8位的数据(相当于一次发
送一个byte,低位在前),然后是一位奇偶校验位(如果开启此功能),最后是一位停止位1(可
以设置不同的位数1.5、2)。以上便是串口通信的一个数据包。显然单单通过这样简单的协议(一
个数据包传送8位数据,最多可以表示256种状态),我们无法完成复杂的通信功能。因此,在此基
础上我们定制一套自己的通信协议以完成数据包更大、数据类型更复杂的通信,形成一个简单的协
议栈。
一个简单的协议需要包含帧头(标明数据包的开始)、数据内容(需要传送的数据)和帧尾
(表明一个数据包结束)。
在数据包中可以增加用于数据校验的校验码,通信中常使用的有CRC(cyclic redundant
check)、奇偶校验、和校验、哈希校验等。
sudo chmod 777 /dev/ttyUSB0
#当电脑只连接了一个串口时,将挂载为ttyUSB0,对于一切皆文件的LINUX,其实和赋予文件的读写权
限差不多
No. 57 / 352


高层的通信协议(在这里为我们自己制定的通信协议)是以底层协议为基础(这里为串行通信
协议),如典型的、当下最复杂的网络——计算机网络,就采用了5层通信协议栈(也有7层的
说法)。
这是一个USB转TTL串口模块,由seasky-lw设计
刚刚入门没有看懂或是想要更具体的实例,请参考这篇文章:在串口通信的基础上定制一个简单的
通信协议-NeoZng
4.5.2. CAN&总线通信
和直接的点对点通信不同,还有一种常用的工业通信方式的分类为总线通信,即多个设备挂载在一条线
上。大家最熟悉的总线莫过于USB总线(universal serial bus)。但USB的协议过于复杂,包括了物理到
消息包的多层协议栈,这里我们简要介绍最常用的CAN和485。总线分为两种,无主机和有主机,区别在
于有主机总线上的通信只能由主机设备发起。
被广泛使用的无主机方法是CAN(Controller Area Network)通信,CAN是一种由博世开发的通信规范
和协议,是一种应用广泛的现场总线,在工业测控和工业自动化等领域有很大的应用。其最大的特点就
是稳定性(使用了差分通信的方法)和灵活性(设备只要挂载在总线上就可以使用,不需要额外的连
接)。我们在比赛中大量使用了DJI生产的电机,这些电机都非常的智能,通过集成了mcu的电子调速
器,我们可以跳过下位机器,直接通过CAN与电调上的微控制器进行通信,将控制信息直接发送给电调
从而控制电机的转动。当然也可以把信息通过can发送给电控再由下位机对执行单元进行控制。
想要让运算平台支持CAN通信,有两种方法:
Jetson(nano除外)、树莓派等主板上自带 40 pin接口,只需要一个CAN收发器即可实现
CAN通信。
购买一个USB转CAN转接器,也可以实现CAN通信。
希望学习具体的例子,在Linux上利用CAN进行通信,请看这里:在Jetson Xavier NX上实现CAN通
信并控制3508电机-NeoZng
CAN通信的基本知识:和串口一样,必然也需要一个通信协议。不同的是,其信号是通过其总线上
电平的差值来表示的,这样能够有效抑制共模信号(因为噪声对两条线的影响通常是一样的,相减
之后噪声的影响便被消除了),因此一般采用两条通信线。不过也最好连接地线,共地可以最大程
度降低干扰。还有4-pin的CAN线,额外的一条线用于独立供电,适用于对信号质量特别高的场合,
此电源专门为CAN收发器和信号电平供电。
由于采用了总线结构,数据的传输只能是分时复用地,其通信机制更为复杂一些,故不在此赘述,
想要详细了解其硬件构成和通信协议,请参阅一篇易懂的CAN通讯协议指南。
No. 58 / 352


一个CAN收发器
而RS485是一种常用的串行总线,总线上有一个主机和若干从机,当主机发起通信后,从机会检测总线
上的电平,若通信id和该从机匹配,便会开始和主机的数据传输。RS485属于半双工总线,由于通信使用
差分信号,同一时刻只有一个发送者和一个接收者。在机器人领域中有些电机会使用RS485作为通信手
段。
注意CAN的带宽有限,常用的有250k、500k和高速的1M,不要在一条总线上挂载过多的大流量设
备,否则会出现丢帧、乱码等情况。
每个结点的ID必须独立,CAN协议中的仲裁顺序和send-ack机制就决定了总线上的不同设备需要设
定不同的id。新人很容易犯这样的错误:两个结点(常常是想要同步转动的电机)设置相同id,这
样就能发一次信息让两者同时接收。
还有通过网口连接的工业标准总线EtherCAT,对TCP/IP协议改良之后满足了IEEE和ISO的一些工业总线
标准,常用于机械臂和流水产线的通信。近年来也逐渐应用到机器人上。相比CAN和485,EtherCAT的
功能多,扩展能力强,带宽远超传统的低速总线。
对于其他的总线通信方法如SPI、I2C等,其实都大同小异,只要了解了通信的基本原理,都能够很快上
手对应的API。感兴趣的同学可以自行搜索学习具体的软硬件实现。
说到和电控方面的通信,这里不得不提一下广东工业大学的视觉控制一体解决方案:rm-control,
他们机器人只使用了一个运算平台,将上位机下位机合二为一,通过CAN控制电机。这消除了
minipc到单片机通信产生的延迟,并且有良好的兼容性和复用性,维护起来也很方便。并且他们还
研发了一款应该是市面上体积最小的usb转can模块。很佩服第一个有这样的想法同时又将其付诸
实践的队伍(廖佬nb!)。
No. 59 / 352


5.比赛中的CV算法
讲了这么多,视觉组的重头戏——算法终于来了。
在大部分时候我们都不需要设计底层的算法,而是直接调用封装好的API,设计更具体的应用于特
定问题的算法。当然,有必要了解一下造轮子(底层算法的实现)的过程,这能够让我们深入理解
算法内部的构造,从而更好地使用这些算法,出错的时候也能更快定位问题。如果只是调用API而
不了解原理,那么只是简单的缝合+搭积木,对于提升自我的思考能力和逻辑思维没有任何帮助。
应当要有“使用科技的黑箱会使我惶惶不安” 的觉悟。
我们最常用的OpenCV和一些神经网络模型都是开源的,它们都有优秀的注释和说明文档,尤其是
OpenCV的Documentation和Tutorial十分详细,全是使用doxygen生成的标准文档系统。通过阅
读这些材料,很快就能上手。在GitHub社区你可以提出Issues,和其他开发者一起讨论问题。
5.0.CV的常识性概念
计算机视觉是让机器拥有视觉同时让机器能够理解所看到的东西并对其进行一定的分析和处理的研究领
域。目前主要分为图像识别、图像分割、图像生成、目标检测、目标追踪、视频处理 等,因为有着共通
的根基和大量知识交叠,其实很难将他们分得太开。此部分就主要介绍最基本的概念:
图像的构成
一张皮卡丘的图片,由一个个像素点构成。
像素:像素是构成图像的基本单元,像素通过行列组合成矩阵就形成了图像。在计算机中一般
都以矩阵的形式存储图像。若仔细观察你的手机或电脑屏幕,应该能看到微小的由红绿蓝三色
组成的发光单元。当像素密度足够高,人眼就会认为一张图片是连续的了。在图像处理的过程
中,我们常把图像视作一个二元函数(如果是灰度图的话),在两个坐标轴上,亮度随坐标而
变化。当图片是彩图时,下面会介绍颜色空间的概念。
像素深度:自然届中的颜色固然是连续的,但是在计算机中存储的数据是离散的。存储一个像
素所使用的位(bit)数叫做像素深度,可以看作图像在某一点取值的值域。常听说的8位宽颜色
就是用8位数据表示一种颜色,存储一个像素使用的位数越多,其能保存的信息就越丰富,主
要表现在能显示的色彩的数量和对比度上。
No. 60 / 352


各中位深度图像的对比,显然24位深度的图像最能还原真实的场景,逼近连续的情况。不过你
的显示器未必支持高位色深(一般来说10位已经很大),这里只是展示效果
图片源自csdn-[丁香树下丁香花开](https://blog.csdn.net/csdn66_2016),侵删
一张以灰度图形式展现的英雄机器人
通道数和颜色空间:当一张图像只有一个通道的时候,他只能表示一个维度的信息,比如这张
灰度图,在唯一的一个亮度(灰度)通道中保存。
当一张图片想要以彩图的形式保存,它至少需要三个通道,即Red Green Blue(RGB),每
个通道都是一个矩阵,矩阵中的每一个元素保存着0~255的值,对应不同的颜色分量。仔细
观察下图就会发现,原图中呈现蓝色的部分在蓝色通道中比其他通道要更亮,比如机器人后轮
下方的一片蓝光,在红色通道中就几乎没有分量存在。
No. 61 / 352


图片被拆分为三个通道
RGB空间的坐标轴-来自百度百科
HSV空间的坐标轴(柱坐标系)
除了RGB空间,还有其他不同的颜色空间如HSV、YUYV、LAB等,他们都是把图片投影到不同
的空间中,图片在这些空间中的每一个坐标轴的投影,就是它在这个方向上的分量(和线性代
数中概念的具象)。
图像的压缩编码方式:为了达到减少空间占用的目的,我们会通过某种算法将图像进行压缩。
存储图片时格式一般有bmp,jpg,png,tif,gif等。不同格式图片的解码速度和占用空间大小不
同,有时候甚至是算法时间占用中的关键一环。
No. 62 / 352


视频
视频就是连续的图像集合,从另一个角度来看,可以把时间当作除x、y外的另一个坐标轴,看作是
一个三维的函数。如果选取一个确定时间,则视频退化为图片。最简单的视频保存格式和图像的压
缩编码相同,而高级一些的压缩算法会根据两帧或多帧内容的相关性,找到关键帧和相似相同部
分,进一步压缩空间。
典型的任务
图像分类的例子
图像识别:给定一张图片,通过算法确定这张图像的分类,又叫图像分类。比如提供一张含有
猫的图片给计算机,计算机应当认出:这张图里有一只猫咪。图像识别的输出是整张图片的标
记,是图片(若把一张宽高分别为w、h的图片看成一个长度为w*h的向量,则图像识别是找
到一个从w*h的空间到图像分类标记的映射(函数)。
与图像识别相近的任务还有单帧场景理解、描述生成等,这些任务的输入仍然是一幅图片,而
输出是若干个描述图像内容的句子或单词,也可以是图像中物体之间的关系。
图像生成:这部分的内容比较复杂,现在一般是通过神经网络训练一个生成模型,它可以根据
你给予的标签(如猫咪),根据学习的数据生成一张对应的图片,因此又被成为“画家AI”。
VAE、flow model、自回归模型和GAN是生成模型领域的始祖。若感兴趣可以参考生成模型之
PixelRNN、VAE与GAN三种算法浅解。
生成模型和判别模型的差别就在于一个是学习联合分布然后由贝叶斯公式求后验分布,
即学习 ,给定一个输入 ,由 求出给定 时最有可能的
;一个是直接拟合条件分布 ,输入一个 就给出最有可能的标签 。
GPT作为第一个超大的语言模型带给人们无限的遐想,其后继者GPT2、GPT3以及
Google为了对标OpenAI退出的BETR等更是在训练参数量上翻了几番,Transformer在
多模态学习中展现威力后,MoCo、CLIP、DALL-E,以及基于基于diffusion model的模
型等直接把语言-图像模型推向新的高度,对超大预训练模型的进化和细节感兴趣的同学
可以参考跟李沐学AI:论文精读系列的相关视频。
No. 63 / 352


生成模型一般会一次性生成大量输出,上图展示的是概率最高的几张
通过对抗学习,给一张熊猫的图片增加了一些噪声
下面的这几张图片是由DALL-E2生成的,给定的输入是Bitter Cocoa Brioche And butter,模
型就真的输出了栩栩如生的奶油可可卷和黄油,光影效果处理的非常好甚至对背景进行了虚
化!更多由它生成的图片可以在Dalle-2 | Image Database (dalle2.app)浏览,你也可以申请 模型使用权(虽然几乎没法排上队)创建一些有趣的作品。
在RM比赛中,我们可以利用生成模型创造出一个会让对手的自瞄算法认为是装甲板的图片,
用它来作为机器人的涂装,以此干扰敌方的识别(一般对特定的神经网络有效,对传统算法无
效)。这样的涂装看起来和装甲板毫不相干,但是检测算法却会认为它是一个装甲板!这种方
法叫做对抗样本生成,下图给出了示例。
虽然在和一张噪声图片叠加之后的熊猫看起来和原来别无二致,但是目标检测算法却将它认作
一只黑猩猩!当然,我们不能直接在装甲板上添加这样的噪声,这显然无法实现,但我们可以
在周围的涂装上使用对抗样本,让神经网络认为贴在涂装上的喷涂样式是一个装甲板。是不是
觉得这和迷彩服、隐身战斗机有相似之处呢?
目标检测:这是Robomater赛场上最常用的算法。目标检测和图像识别在一些方面有些类
似,图像识别主要是对图像进行分类,让计算机判断这张图“有什么”或者”是什么“,而目标检
测不仅要判断图片中是否有对应的物体,还要输出关于这些物体”在哪儿“的信息。和目标检测
相似的“目标定位”的任务则是对图像中的一个特定物体进行定位,而目标检测算法中,图像内
含有的对象种类和数量都是未知的。目标检测的输出是目标物体的位置和类别。
No. 64 / 352


目标检测算法不仅对图像中的对象给出了分类,还用一个Box把他们框出
上图展示的是多目标检测算法的检测结果,基于神经网络的目标检测算法能将一套框架运用到
所有目标对象的检测问题上,在训练过程中习得待检测对象的特征。而基于灯条匹配、扇叶识
别的算法则是专门针对装甲板识别和能量机关识别的,相当于我们手动设计需要检测对象的特
征。他们各有优劣,我们会在 5.2、6.1、6.2 中对他们进行更详细的介绍。
时下效果好、速度快的目标检测算法几乎都是基于神经网络构建的,常见的有R-CNN系列、
YOLO、SSD等,我们也会在 5.2 中分别解读这几个算法。可以参阅这个系列的文章来进一步
了解目标检测:目标检测入门。
图像分割:根据图像的特征把图片分为几个有确定性质的区域,并寻找我们感兴趣的区域。图
像分割算法可以目标检测的基础上进一步解析图像,其输出可以是对每个像素的像素级别的描
述,如下图中灰粉色的区域就代表“运动员”。常见的算法有阈值分割、边缘分割、聚类、基于
神经网络的语义分割等。想要了解更多可以参考图像分割传统方法整理。同样,最新的效果最
好速度最快的算法也是基于神经网络的。这类算法目前在雷达站、自动步兵上可能会使用到。
No. 65 / 352


图中的运动员和他的自行车被算法从背景中提取了出来
图源知乎-芝芝-https://zhuanlan.zhihu.com/p/143261645 ,侵删
上文所述的几种任务的直观区别,a为图像分类,b为目标检测,c为语义分割,d为实例分割
目标跟踪:视觉目标(单目标)跟踪任务就是在给定某视频序列初始帧的目标大小与位置的情
况下,预测后续帧中该目标的大小与位置。有同学可能会疑问,明明目标检测算法能对每一帧
图像进行处理确定出目标的位置,为什么还需要目标跟踪?这是因为目标检测算法需要对整张
图片进行处理,其消耗的运算资源很大,而目标跟踪不仅运算量以数量级的优势比前者小,还
有简单准确,适用面广,抗噪性好的特点。因此在检测出目标之后,可以使用目标跟踪算法来
进行后续的处理,同样能识别到装甲板等物体。在 5.3 中我们会更具体地介绍这个算法。
视频理解:在以上的几个任务中,大多以单张图片作为任务对象,视频处理则是将一段时间内
的所有帧都作为任务输入。一个RM赛场的例子就是,我们可以保存在一秒钟内相机拍摄的所
有图片,将它们堆叠成一个张量,送入卷积神经网络的输入层(卷核的维度也要进行相应的改
变)。此时,我们便不单单可以处理定格在图像中的信息了,还能够对包含时间的数据如机器
人处于小陀螺运动这个状态进行检测,以启用反小陀螺算法。利用了历史数据的图像处理算法
都可以看作是视频处理算法。
这里只介绍了一些和机器人与比赛相关度较高的视觉图像/视频处理和分析算法,实际场景中
还有大量不同的计算机视觉任务,感兴趣的同学可以到paperwithcode和arXiv上看看时下热
门的研究问题有哪些。
No. 66 / 352


5.1.OpenCV常用算法
OpenCV 是一个软件工具包,用于处理实时图像和视频,并提供分析和机器学习功能。使用这些标
准化的软件包可以极大提高我们的开发效率,并且这些工具包对一些算法运行速度有特别的优化,
能够使得这些算法在拥有GPU或支持多线程的电脑上得到加速。掌握OpenCV中的基本数据类型和
常用函数是视觉组迈出开发的第一步,同时也能学习大量的相关知识。
这是OpenCV的官方网站,可以在这里的社区和其他开发者交流或查阅说明文档和例程。
首先你需要安装OpenCV,可以参考Ubuntu下OpenCV+contirb模块完全安装指南-NeoZng。
基本数据类型
Mat:矩阵类型,能够保存图像。
Point:一个像素点,或者任何类型的“点”。
Scalar:一个四维点类,是许多函数的参数,常用于描述颜色。
Size:同样是一对数据构成的组,一般表示一块区域或图像的宽高,有些时候可以和point互
换。
Rect:rectangle,矩形类,拥有Point和Size成员,用于表示一块矩形的区域。
RotatedRect:同上,不过有额外的成员angle用于表示角度。注意这个类的角度系统有些独
特,务必阅读:OpenCV中旋转矩形的角度。
具体的说明请参阅OpenCV的说明文档,或在IDE内选择switch to declaration,便能转到注释处。
5.1.1. imgproc 模块(image process)
是我们使用OpenCV时最重要的模块之一。主要是一些像素级的操作,通过图像滤波、形态学操作、阈
值操作、通道处理、图像变换、轮廓查找等功能来凸显图像特征或滤除噪声。还可以通过一些简单的绘
图函数在图片上作画、输出文本。下面列出一些常用函数:
画图
颜色空间转换
阈值
滤波与平滑
circle() //画出一个颜色、大小、粗细可调的圆,一般用于标记角点等特殊位置
line() //在两点之间画出一条直线,用于框出目标或作为参考。
//用于标记装甲板、能量机关的角点,框出候选的目标
cvtColor() //将图片从一个颜色空间转换到另一个颜色空间
split() //把图片的不同通道进行拆分,放入不同的Mat
subtract() //将两张矩阵的每个元素相减
//这在自瞄中将用于RGB到GRAY和HSV等空间的转换和颜色通道的分离。
threshold() //阈值操作,对一个特定的分量与阈值进行比较,大于阈值则全部设为某个值,小于阈值
设为另一个值
inRange() //进阶版本,可以确定一个分量是否在一个区间内
//我们使用这两个函数来筛选特征,对拆分后的颜色空间进行操作以屏蔽不感兴趣的部分
No. 67 / 352


形态学操作
其他图像算子
滤波、平滑、形态学操作等都属于使用图像算子对图片进行卷积操作,学习过数字图像处理或信号与系
统的同学应该对此熟悉。在OpenCV中,你可以使用 getStructuringElement() 来构建独特的卷积核,随后
使用 filer2D() 来对图像进行卷积运算。
寻找/画出轮廓 + 矩形/椭圆拟合
仿射、投影变换
要区分仿射变换和投影变换,请记住仿射变换是线性变换,而投影变换不单单是线性变换。仿射变换会
保持对象的相似关系和平行关系,而投影变换可能会改变这种关系,添加了非线性的因素(仿射变换的
维度比投影少1,投影是一个商空间)。
blur() //加权模糊图像
GaussianBlur() //高斯加权模糊
medianBlur() //中值滤波
bilateralFilter() //双边滤波
//用于对图像进行降噪处理,或是抹去小光斑等
erode() //腐蚀操作,二值图的边缘或收缩
dilate() //膨胀操作,二值图的边缘会扩张
getStructuringElement() //获得结构元素(核)
morphologyEx() //更多的形态学操作,包括Opening,Closing,Morphological
Gradient,Top Hat,Black Hat等
//用于增强图像的某些特征
Sobel() //微分运算,检测边缘,微分会使得图像中像素强度(某个分量)变化最大的部分为极值
Laplacian() //二阶微分,检测边缘,二阶微分会使得图像中像素强度变化最剧烈的部分为零
//寻找图像中的边缘
floodFill() //漫水法,常用于寻找轮廓的预处理操作,和“画图”软件中“油漆桶”工具有相同的效果
findContour() //寻找二值图中的轮廓,并保存为一组点,算法类似于漫水法,遍历所有像素并查找相邻
像素
drawContour() //根据一组点画出轮廓
//承接上面的各种预处理,用于找出图像中的轮廓并进行下一步操作
minAreaRect() //通过轮廓点,拟合出最小面积的RotatedRect
boundingRect() //通过轮廓点,找到其外接矩形Rect(水平)
fitEcllipse() //通过轮廓点,用最小二乘法拟合出一个外接椭圆,函数会返回椭圆的内接旋转矩形
RotatedRect
minEnclosingCircle() //通过轮廓点,找到最小面积的包含圆(注意不是外接圆)
//将轮廓点转换为更容易处理的形状对象
remap() //根据给定的映射(函数)改变图像中每个像素点的位置
warpPerspective() //进行透视变换
getPerspectiveTransform() //获得透视变换所需的矩阵(4个点)
warpAffine() //进行仿射变换
getAffineTransform() //获得仿射变换所需的矩阵(3个点,为什么比透视少一个点?)
//一般用于把图像根据变换关系转化成正视图以便进行模板匹配、SVM匹配等操作
No. 68 / 352


5.1.2. imgcodecs 模块(image reading and writing)
5.1.3. videoio模块 (video input and output)
5.1.4. highgui模块(high level graphis user interface)
除了第一个 imshow(),在使用highgui模块时需要你了解一些响应式编程的方法(有些类似于中断编
程),不同于以往的的控制流命令式(面向过程)编程,响应式的程序在运行的时候会监听并响应异步
数据流(Event Stream),可以时时和用户交互。我们使用的操作系统图形界面几乎都采用了响应式编程。
5.1.5. 其他模块
ML:machine learning模块,内有封装完全的多层感知机、基于Dtree决策树的boost集成算法、
EM(expectation Maximization)、逻辑回归、朴素贝叶斯分类器、模拟退火优化、支持向量机等
经典的机器学习算法和一个能够提供各种功能的内置的数据集包。对于自动步兵、哨兵和雷达的开
发有很大的帮助,可能也能帮助我们构建其他需要这些算法的模块如:装甲板分类(Bayes或
SVM)。
imread() //根据路径读取一张图片
imwrite() //向对应路径写入一张图像
imreadmulti() //一次读取多张图片
//读取、保存测试用的图片或者自己制作的卷积核、用作模板匹配的图片模板等```
class VideoCapture() //构建一个视频捕获类,捕获的视频可以以每一帧图像的形式保存到Mat
中
//VideoCapture cap(0);
//Mat frame;
//cap>>frame; //这样就把一帧图片保存到frame内部了
//这个类能够通过get(),set()方法获取和设置一些相机参数
class VideoWriter() //构建一个视频保存类,能够方便地保存视频,并且提供各种格式
//在实验室时无法模拟赛场的光线环境,常常在比赛时录制相机第一视角的视频,以供之后测试使用
//也可以把检测完的每一帧图片连成视频,保存下来,之后根据这个视频来查找问题、改进算法
imshow()/*在指定名称的窗口中显示一张图片,注意和waitKey()配合使用否则可能导致异常,用于查看一
些算法处理后的结果,waitKey()的参数为图片显示的时间*/
//以下这个组合可以极大地方便参数调试,在程序运行的过程中通过回调函数,可以实时修改参数值
//这种根据用户或客户端的请求进行动作的编程范式被称为“响应式编程”,在GUI、OS和应答服务中很常用
nameWindow() //新建一个空窗口
createTrackBar() //创建一个拖条,传入相关的参数可以实现参数调节
getTrackBarPos() //返回拖条所在的位置
//这个组合能够通过键盘和鼠标向程序传递参数,改变程序的状态,调试的时候非常好用
setMouseCallback() //设置鼠标的回调函数
waitKeyEx() //从键盘读取输入
No. 69 / 352


DNN:deep neural network模块,随着深度网络的流行OpenCV自然也提供了对包括onnx、
tensorflow、pytorhc、caffe等模型推理的支持(不支持训练)。基本的api封装的非常简洁,以分
类网络为例,分别运行 readnet() , blobFromImage() , setInput() , forward() 就可以得到
结果。对于检测网络和分割网络,还要进行后处理,不过 cv::DNN 也提供包括 NMS() 等简单的后处
理。如果要在程序中嵌入一些轻量的网络,使用DNN模块足堪使用。若有复杂的pipeline和前后处
理,建议还使用特定的推理框架(OpenVINO、TensorRT、NCNN、MNN等)。
calib3d:Camera Calibration and 3D Reconstruction模块,包含了相机标定的算法和一些三维重
建方法。我们在得到装甲板的位置后,需要解算装甲板到相机的距离,这就会用到这个模块内的
solvePnP()函数。在前面提到过,为了正确的反映物体在原本的位置关系,我们需要对相机进行标
定,也是利用这个模块中的函数,幸好OpenCV官方已经为我们编写了一个标定例程,我们可以在
OpenCV编译目录下的 /opencv/samples/cpp/tutorial_code/calib3d 处找到它,在修改 in_VID5.xml
和VID5.xml 内的参数后就可以开始标定了。
如果你想要更具体的实例和操作,请参考在OpenCV中用例程标定相机-NeoZng这篇文章。
video analysis:包括各种版本的Kalman Filter、光流估计等。OpenCV提供了封装好的标准KF,
我们可以通过修改状态转移矩阵、控制矩阵和测量矩阵从而将其升级为Extended版本(扩展卡尔曼
滤波,4.5之后似乎自带EKF和UKF以及AUKF)。可以方便地调用此模块完成基础的运动轨迹预测功
能,同时减少噪声。
extra_module
ccalib:Custom Calibration Pattern for 3D reconstruction,双目相机的标定和双目测距可 以利用这里的函数进行。
tracking:目标追踪模块,内有几个经典的跟踪器如KCF、HAAR、HOG、GOTURN等。可以
用于装甲板追踪和雷达站的目标跟踪。
videostab:视频稳定模块,提供了一些提升视频稳定性的工具,如防抖、插帧、消除模糊
等。但由于处理速度稍慢,缺乏实时性,难以用于自动瞄准算法。在雷达站上可以运用,也可
以用于分析制作测试视频。
cudaXXX:以cuda开头的这些模块都是可以利用英伟达的通用并行计算平台(CUDA)进行
加速的(如果你的显卡是英伟达生产的)。
若要使用extra_module中的模块,需要和opencv-contrib交叉编译,前文的安装教程中提及了这一
点。
这篇博客:RM 教程3-OpenCV 传统视觉介绍了OpenCV中基本数据类型、基本api的简单使用并辅
以代码示例,推荐阅读。对应的视频教程推荐东南大学的OpenCV入门教程。学完上述内容你就基
本具备了完成传统装甲板识别算法的能力。
attention:5.2、5.3、5.4对于新人来说可能有一定难度。
若是新人或刚入门的 RMer,可以由此直接跳转道第六部分继续阅读,第六部分看完后再回来这里继续
~~
No. 70 / 352


5.2.目标检测
时下RM赛场上的自瞄算法分为两个流派:传统特征提取和神经网络。前一个部分已经介绍了和比
赛相关的OpenCV函数,因此为了保证行文的连贯性和整体性又不重复叙述,这个部分主要介绍新
兴的基于神经网络的目标检测算法。在第六部分当中则会介绍传统算法的全部流程。
神经网络是时下最火热的ML的子领域,也就是我们常听说的“深度学习”、“深度神经网络”。而基于
卷及神经网络的目标检测算法又是这个领域的超级明星。基于机器人学习的目标检测几乎在各方面
都替代了传统算法:传统算法需要手工设计检测目标特征,并且很难分离前景和背景。而前者则是
通过学习算法来习得检测目标的特征,并且在速度上随着各种trick的加入已经对算本身的改进,已
经达到了实时性。虽然缺乏一些可解释性,但也不能阻止其在CV方面大放异彩、在各种顶会顶刊上
乱杀的势头。这个部分会介绍卷积神经网络的基本原理和几个热门的目标检测算法的相关知识。检
测装甲板、机器人、能量机关这些都只是冰山一角,这些目标检测算法的检测对象几乎是没有限制
的,因此基本只需要更改训练集和很少的超参数就可以将算法部署到其他问题上。
5.2.0. 目标检测基础
目标检测算法经过近十年的发展已经成长为一颗参天大树,基于CNN的目标检测现在大致可以分为
两类:1-stage和2-stage。也可以分为anchor-based和anchor-free,再加上使用transformer 的后起之秀这三类。
若要入门深度学习(神经网络),首先要学习机器学习的基础知识(神经网络算是机器学习的一个
分支)这里推荐斯坦福大学吴恩达教授的系列课程机器学习经典名课,深受大家欢迎,一般在看深
度学习课程之前将此课程学完会有更好的理解,当然最好是能先看完。而机器学习又是以多元微积
分、概率论和线性代数为基础的,这三门基础课一般会在大二上学期之前结束。额外学习一些凸优
化的内容将有助于机器学习的理解。高数相关的内容推荐国防科技大学朱建民教授的一系列
mooc,线性代数推荐MIT Gilbert Strang教授的18.06。
吴恩达教授的深度学习系列课程相信是不少同学入门DL的选择,深入浅出贴近实践并且在coursera
上有相关的练习。务必至少看完这个系列的前四部分视频再学习目标检测的相关内容(第五部分序
列模型是Transformer的基础,但是我们暂时不需要,有时间的同学可以一并学完)。若能学完这
两个系列的课程,在AI方面就算是入门啦。
深度学习方面广为流传的教程还包括李沐的《动手学深度学习》和台大李宏毅的深度学习课程。
《动手学深度学习》在任意搜索引擎搜索d2l即可,拥有配套的jupyter notebook,非常方便,
bilibili上有作者本人的教学视频,他出品的面向真实应用场景的实用机器学习也非常受欢迎。台湾
大学李宏毅教授的深度学习课程也是广为流传,上课的时候很多梗也很多二次元,PPT精美可视化
很多,寓教于乐且紧跟学术前沿。
我们首先介绍目标检测的基础——神经网络。
当开始阅读 5.2 之后,笔者默认读者已经拥有了微积分、线性代数、机器学习的基础。
5.2.1. 神经网络
和传统的统计学习方法如SVM、PCA、LDA等都是需要经过训练的。以最简单的单层感知机为例,
其实我们可以将所有的神经网络看作是一个多维特征空间到另一个我们需要的得到的属性的映射。
对神经网络的训练,就是用一对对标记好的数据(已知输入的特征向量和输出的属性,即数据标
签)对这个“函数黑箱”的建模过程。其中的每个神经元的w、b即是我们需要得到函数的许多构成参
No. 71 / 352


app神经网络的截图,内部还提供了著名的网络如shuffleNet、语言模型、TextCNN、自编码器等
数之一。然后我们根据BP(back propagation)算法等对网络进行最优化(让网络学会这个我们
需要的函数,即学习过程),使得这个函数能够逼近我们需要的理想的映射。
分类问题就是在回归问题的基础上增加后处理,如加入SVM和softmax函数等。对于基于卷积神经
网络的猫狗图像分类器,假设其输入是320x320的图像,那么就是有320x320=102400个特征(每
个像素作为一个特征,单通道位图),其输出是一个三维向量[cat dog none],每个元素智能取0和
1并且最多只能有一个元素为1。那么,某个元素的值为1时说明此图像是对应的那种物体,none为
1则为说明图像中没有猫也没有狗。那么,训练出来的函数就是一个102400元函数,其输出为3维
的离散序列,此函数通过映射将一张图片映射到上述的三维离散空间中。
在此网站:A visual proof that neural nets can compute any function,有对神经网络是如何拟合
出任意函数的形象的、直观的解释。手机上还有一款就叫做神经网络的App,你可以手动设置隐藏
层的数量、各个神经元的w、b和激活函数等,然后得到一张清晰的拟合出的函数的图片(当然只有
3维空间以下的结果能够被可视化地输出)。
对于深度学习或机器学习模型而言,我们不仅要求它对训练数据集有很好的拟合(训练误差小),
同时也希望它可以对未知数据集(评价集、测试集、实际应用)有很好的拟合结果(泛化能力强,
即训练生成的函数可以很好的根据输入预测输出且基本没有错误),所产生的测试误差被称为泛化
误差。度量泛化能力的好坏,最直观的表现就是模型的过拟合(overfitting)和欠拟合
(underfitting)。过拟合和欠拟合是用于描述模型在训练过程中的两种状态。这两个问题的都会导
致神经网络的泛化能力下降,欠拟合一般是通过增大训练数据量和模型复杂度解决,而过拟合则是
当前科研面临的主要问题,解决方法也多种多样。
No. 72 / 352


训练模型得到的几种状态的图解,以回归分析为例
因此,在训练神经网络时,我们需要用一些trick来改善它的性能或加速训练过程,在此过程中就涉
及到需要人为确定的许多“超参数”。这些超参数设定的数学依据较少甚至没有,很大部分都是启发
式的(拍脑袋想出来的),而不是有一套严密优雅的数学方法来对神经网络的表达能力和学习有效
性进行分析。这也是神经网络被传统的统计学习学派诟病为“炼丹”的原因:需要在实践中不断调整
超参数的值和网络结构,根据训练结果才能改进网络的结构和超参数的值(就如控制领域的pid算法
一样,虽然三板斧调参用起来猛,你却很难说出个所以然来)。
不过它们确实有效,所以我们还是来看看他们的作用吧.
5.2.2. 防止过拟合
正则化(Regularization)
个人认为这种翻译的有些生硬也不够直观,按照笔者的理解,应该被称做“规范化”或“约束化”。通过
对损失函数加入额外的先验知识,防止w在训练中变化过于剧烈,进而在很大的程度上改善过拟合
的问题。在一些教程中,常常用一个两参数的单元来可视化这种方法:
No. 73 / 352


摘自周志华教授的《机器学习》,L1正则化和L2正则化的形象图解
通过对w添加额外的限制,倘若w在训练过程中变化过于迅速(这是过拟合的一个主要原因,对训
练集数据过于“亲近”,以至于学习到了部分分布当中的一些偏差与噪声,而不是整体分布的信
息),额外的惩罚项会约束w的增减,降低其变化速度。详细的解释,在Andrew Ng的深度学习课
程第二部分p5.
通过翻转和旋转、裁切等操作,扩充了猫咪数据集
数据增强(Data Augmentation)
以图像分类为例,通过将训练集中的图像进行翻转、反转、裁切、旋转、变形和添加色彩偏差等方
法来扩充数据集,是最廉价最简单的防止overfit的方法。虽然其效果不如收集更多的图片来的得
好,可架不住成本低、几乎无开销的特点。在小规模训集上,数据增强是最有效的方法之一。
高级一些的数据增强如MixUp、Mosaic等复合了以上几种中的若干种,有兴趣的同学可以自行查阅
资料。别小看数据增强的方法,一个优秀的DA甚至能提高几个点(mAP)。
Dropout正则化(随机丢弃)
初看dropout方法的同学一般都会认为它深度学习中显得更加玄乎。其原理大概是在某个隐藏层中
随机使某个神经元失活(使其输出为零),这样拟合出来的函数参数量下降,不容易拟合复杂的函
数,就更不容易出现过拟合的现象。然而dropout会在反向传播中出现难以解决的问题,由于其丢
弃的结点是随机的,无法在每一轮反向传播中确定要更新的参数,因而无法明确的定义损失函数,
No. 74 / 352


dropout图示,使神经元失活即置零其输出
除非每一轮训练只使用一个样本(想想为什么?)。
从数学角度理解,似乎dropout能够产生L2正则化的效果,随机丢弃某一层中的一个结点,其结果
就是该结点不会在本次反向传播对损失函数的计算产生影响。若该结点在之前的训练中变化过于剧
烈,那么使其失活就有机会让其他结点的参数得到训练从而获得变化,而非将所有的赌注放在那个
结点上(put all of its bets on)。因为每个结点失活的概率是相同的,那么他们应该会得到均等的
训练机会。详细的解释参见深度学习课程第二部分p7。
训练集上过度优化会导致过拟合
Eearly Stopping(提前停止、早停法)
非常容易极致简约的方法,如果训练的轮数过多导致模型过拟合了训练数据,那就减少训练轮数。
不过我们不用真的用不同的轮数训练多次,而是如一次训练300个epouch(将所有样本都循环300
次),然后根据其在测试集上的表现,选取测试误差最小的那一轮,保存其权重作为最后使用的模
型。
5.2.3. 让你的网络学习得更好
归一化和标准化(Normalization and Standalization)
这两个操作常常被混淆,在一些文献中也表示相同的意思,他们都是特征缩放的方法。因此本文将
他们视作同一种类的不同操作。常见的归一化操作有minmax缩放、零均值化、方差归一化等。也
可以把归一化理解成“单位化”、“统一化”。minmax缩放将所有数据通过线性映射把值投影到一个区
间内。零均值归一化操作会将数据特征的均值化为零,方差归一化则是将特征的方差变为1。后二
者常结合在一起使用。
No. 75 / 352


使用了零均值化和方差归一化后的数据,截取自深度学习系列课程
使用前后的损失函数对比,下方为损失函数的等梯度线(标量)
通过这些尺度缩放的操作,可以缩小存储数据需要的空间,同时可以让损失函数变得更加“圆润”。
下图给出了未归一化和归一化后的损失函数的对比。显然通下降法,归一化后的数据能够更快地使
网络收敛,因为在圆周上的任意一点梯度方向都是垂直于“等高线“的,梯度是函数变化最快的方向
(多元微积分知识)。
防止陷入局部最优解
当损失函数可能不是一个凸函数,或者它有多个极值点(大多数情况下都是这样),那么,使用梯
度下降方法可能会使神经网络拟合出来的函数陷入局部最优解(向着不是最小/大的极值点不断迭
代)
多种初始化值(从头训练)
在初始化网络参数权重时,一次选取多组参数,相当于一次训练多个有相同结构的网络。这
样,我们能够从参数空间中的不同位置开始运行梯度下降。即使部分解陷入了局部最优,我们
还有其他的解。可以把这种方法看成”广撒网“式的训练。最后我们会测试这多个网络的性能,
选取其中最好的一个。虽然这种方法的效果最好,但是其缺点也十分明显:极大的训练开销。
No. 76 / 352


从参数空间的不同位置(蓝色点)开始梯度下降,即使有部分陷入了局部最优解,也可能有一些
会收敛到全局最优,至少也是比较好的极值
对局部最优解加入扰动后,成功找到了更好的解
局部扰动
使用局部扰动有一定的概率跳出局部最优解:在网络的Loss function不再减小或缓慢振荡的时
候(也许此时陷入了局部最优解),随机地给网络参数进行微小的扰动。若在有限步迭代(人
为设置地超参数)内能够往其他方向移动并且进一步缩小了Loss function,那么说明我们找到
了一个比原来的解更好的解,此时再继续运行梯度下降。若迭代次数超出了我们设置地扰动次
数上限,则返回原来的位置,认为函数已经收敛到比较好的极值点了,得到了较优的结果。如
此,就有概率跳出局部最优解。
局部扰动和模拟退火算法在有某些相似之处。也有研究人员提出,可以利用模拟退火算法来在
小规模网络上进行训练,收敛速度十分迅速。
mini-batch(小批量训练)
No. 77 / 352


不需要遍历整个数据集就进行一次剃度下降,选取部分样本作为一个批次来计算其loss。从另一个
角度来理解,就是加快了迈步的频率。朴素的梯度下降是遍历整个数据集后再计算loss,对参数进
行一次更新。minibatch则是把数据集分为32/64/128/256...这样的“小训练集“,在一个小训练集遍
历后就进行更新。其优点很明显,就是可以提高训练速度。但它也同样存在一个小缺点:只使用部
分训练集进行训练可能产生局部的过拟合或是欠拟合(小部分的数据集无法反映整体数据分布),
还好在实践中对模型的影响似乎并不是很大。在数据集稍大的情况下,我们一般都会选择使用
minibatch进行训练,否则收敛速度过慢且空间开销过大。
mini-batch的极端就是每个batch只有一个样本,即SGD(stochastic gradient descend)。经过
调优后的SGD在训练的时候也有很好的效果,这一般需要有经验的炼丹师对学习率、lr schedule和
初始化参数等进行调节,新手还是乖乖用Adam吧(这些名词在下面都会进行介绍)。
Batch-Norm(批量归一化)
batch-norm字如其名,对每一小批数据进行标准化。它会将一批数据的每一个隐藏层的输出
(可以看作编码后的特征)都进行归一化。我们在前面已经了解到了Normalization的好处,
于是,有研究人员就想到为什么不能对神经网络中的每一层输出都进行类似的操作呢?于是,
我们采用的Batch-Norm会将数据集的数据划分为一个个batch,并对每个batch中的数据在每
一层的输出都进行归一化。(思考:那为什么不对所有数据即整个训练集的数据在每一个输出
层都进行normalization?)
在训练结束后进行推理时,一般都是单个数据的输入(特别是实时视频序列的处理),这时候
要注意的问题就是每一个输出层的归一化使用的均值与方差的确定,显然对单个数据没有均值
和方差可言。因此在训练过程中,我们会用指数加权平均的方式记录每一批norm的均值和方
差,并以此作为推理时的参数(根据大数定律,这个值最终会近似实际的分布)。
从第三层开始,可以将其看作整个网络的”子“网络。每一层的参数都试图从前一层的输出中习
得这些数据到训练标签y的一个映射
Batch-Norm为什么奏效呢?在前面的归一化中我们已经了解过,这些操作能够让参数搜索空
间中的loss function改变成“碗状”从而获得更好的梯度下降效果。但是既然数据分布已经完成
了这件事,在中间的隐藏层为何还要继续使用normalization?这里就要涉及到一个叫做
covariate shift的问题。想要理解covariate shift,请看下面这张图:
这里只是以第三层为例,其实可以没去掉一层(计算一层)就把它看作是一组数据到y hat的
映射。那么自然每一层的输出向量都可以看作是一个特征向量,则我们就对这一批“特征向量”
进行归一化。Covariate Shift就是一个向量经过前层的计算,数据原来的分布已经改变了(每
一层网络都是非线性映射,从极端的角度来看每一个隐藏层都是一个网络,单层感知机)。那
么显然,如果数据的分布一直在改变,网络是很难习得一个映射的(数据分布随着参数更新始
终发生变化,就像解方程我们需要一些确定的信心,而在这里这个信息却会一直发生变换表现
得像未知数)。使用batch-norm则可以把每一层的输出都标准化为方差为1均值为0的分布,
虽然它的具体形式还是未知,但至少我们可以线只它的均值和方差达到限制前层参数的更新对
输出数据分布影响的目的。
No. 78 / 352


同时由于batch-norm只对每一个batch的数据作用,显然一个batch不能体现整体数据的分
布,因此会引入统计偏差和噪声,有时候这反而是有利的,其效果类似dropout,能够产生一
定的正则化效果,避免网络拟合出来的映射过分依赖于某个神经元。
关于Batch-Norm的更多信息,可以参阅论文原文。文章:re-thinking batch in batchNorm 也是一篇
很好的参考文章,介绍了更多关于统计和数学原理的知识
不同学习率对网络收敛的影响,对比蓝色曲线和绿色曲线,我们可以取二者的精华
学习率衰减
学习率作为最重要的超参数之一,于我们来说有很大的操作空间。训练的开始阶段加速学习迈大脚
步,采用较大的学习率;在接近最优解时、Loss function下降减慢时减小学习率,防止超调和振
荡。
有些训练策略还会在初始阶段进行warmup,先选择较小的学习率防止随机初始化的权重使得模型
大幅振荡。Warmup也有两种:一种是常量warmup,在热身时选择一个固定的学习率。它的不足
之处在于从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。另一种是渐
进warmup,即从最初的小学习率开始,每个step增大一点点,直到达到最初设置的比较大的学习
率,再在之后的学习中逐渐减小学习率。还有warmup-restart、cycling、cosine,每隔若干epoch
就会重新启动一次warmup、让学习率从大到小以正弦规律往复变化等等learning rate schedule方
法。需要更深入的了解可以自行查阅相关资料。
No. 79 / 352


5.2.4. 梯度下降的进化
5.2.4.1. 进击的mini-batch
一个二维的搜索空间,红点表示Loss的最小值
竖直方向上的分量随着迭代减小,水平方向不断增大
GDM(Gradient Descent with Momentum)
动量梯度下降法的收敛速度总是要快于普通的梯度下降法,这得益于它的”动量累积“思想(Andrew
Ng说物理好的人容易理解这个算法)。假设在一个网络的训练中你遇到的参数搜索空间如下图所
示,其等梯度线呈椭圆排布,那么在训练的过程中就会遇到比较大的振荡(梯度总是沿着等梯度线
的切线的径向),即蓝线所示的学习路线。一旦你增大学习率,就会得到紫色的学习轨迹,产生过
大的振荡甚至出现不能收敛的问题。我们希望在这个学习问题中能够让竖直方向的学习速度下降,
水平方向上的学习速度增加(蓝字所示)。
观察蓝线会发现其竖直方向上的振荡均值始终为零,走了很多没必要的路,是否可以通过某种方式
让竖直方向上的学习率降低?(注意此处说的竖直方向和水平方向的都是以二维情形为例,显然我
们的搜索空间没有这么小,在高维空间里就对应多个方向)GDM就是通过观察学习的特点得到的一
种优化方法。
在每一个batch的迭代中,会记录下当前的dw和db,并且以指数加权平均的方式对dw、db进行累
积,把得到的累积值用来更新参数。这样,GDM就会记录那些振荡项并把方向不同的dw、db分量
抵消,而那些方向相同的分量则会随着迭代的进行不断增大,就好似GDM能够聪明地找到最快下降
的方向,顺着“山坡”(最速降线)不断累积“动量”,迅速到达最小值点。
如果拿具象的例子类比,就是将一个小球置于碗中,并且给它一个切向的初速度(相当于一步迭
代)。对于普通的GD,每次小球的切向速度为零时就让小球停下(把径向速度也归零,即重新计算
dw),然后再次释放小球;而对于GMD,就只给小球一个切向速度,之后不去干扰小球(每次计
算完成之后,dw都会加权累积),那么切向速度会随着往复运动而逐渐减小(dw的反方向分量会
抵消缩小),径向由于有重力加速度(方向相同,累积后不断增大),动量会持续累积,以更快的
速度到达碗底。
RMprop (Root Mean propagation)
RMprop和GDM异曲同工之处,同样会在迭代中保存每一次的dw值并对其进行均方加权平均。在更
新参数的时候会使用当前一次迭代的dw除以前述得到的值即前几次迭代中dw的平方的加权均值
(想想为什么用平方),这样,当dw变化很大的时候,该累计值也会很大,经过相除这个值就会相
对变小;而dw小的时候,累计值也相对小,那么相除后用于用于更新的参数将会相对变大。这样的
操作让每次更新都更加保守,不会像GDM一样速度越来越快而是始终稳步前进(此速度可以通过加
权平均的系数修改)。这种特性也允许在训练时使用更大的学习率。
No. 80 / 352


绿色的箭头就是是用了RMprop后的效果,可以看到学习的步伐非常稳健
Adam方法“稳中求进”
Adam (Adaptive Moment Estimatation)
Adams融合了上面两种方法,最后得到的参数更新公式如下:
Vdw是GDM的累计项,Sdw是RMprop的累积项,在计算结束后对Vdw和Sdw都进行了指数平均修
正,确保在刚开始的时候指数加权平均不至于过小。 是一个很小的数(一般为10e-6)防止出现除
零得到NAN的情况,这也是在设计网络中,若出现除法操作常用的训练技巧。
5.2.4.2. 尝试更高的阶数
这部分内容主要是作为了解,因为pytorch、tensorflow、mindspore等流行的框架都只支持自动微分,
并不支持更高阶的优化方法。当然,如果你要为自己的某个项目的优化问题编写求解方法,可能会用得
上。
牛顿迭代法
拟牛顿法和其他高阶方法
这些常用的非线性优化方法实际上是一门系统的课程/科学,一般被称作运筹与优化理论。实际上,
研究和工程上的很多问题都可以转化为优化问题。即如神经网络一般确定一个目标函数,并寻求合
理的求解工具,如简单的初高中就学过的线性规划,微积分所学的KKT问题,以及更深入一些的凸
优化和半正定优化方法,当然还有神经网络中最常用的梯度下降法(一节优化方法),使得目标函
数能够最小/最大化。
No. 81 / 352


了解了神经网络的构成和基本原理后就可以开始进一步学习卷积神经网络了。这里也再次提醒读者应该
至少至少先学习吴恩达深度学习课程的第一个和第四个系列视频再来了解相关方面的知识。拥有基本的
了解和学习背景对于探索新领域是非常有必要的,否则你只会在n个不同的超链接里面反复横跳(这是
啥?查一下,诶,这又是啥?再查一下......wft?怎么又回来了??)。若有信号与系统或数字信号处理的
前置知识,对于CNN的学习大有脾益。
5.2.5. 卷积神经网络
在5.2.1中我们提到,可以把图像resize成一个1xn的特征向量当作输入投入一个网络当中进行训练
和预测,但是这样做会出现很多的问题:首先是参数量过大,拿一张320x320的rgb图像举例,它
是一个拥有320x320x3个维度的向量,若隐藏层也采用这样巨大的神经元规模,其参数量是不可想
象的(利用你的排列组合知识计算一下)。另外,将图像resize成一维向量,在一定程度上丢失了
两个维度之间的相关性(在映射的过程中有信息被压缩到原向量空间的零空间中了)因此,卷积神
经网络就诞生了(LeNet5,1994)。但是由于算力的限制,直到到近年GPU的快速发展才得到广
泛的应用。
卷积神经网络的设计思想是参数共享、空间信息的保存与合理的参数量下降。
如何用网络设计一个图片分类器?根据前面学习的知识,可以把图像看成一个width*height维的特征向
量并把它输入到一个超大规模的神经网络中。在引言中提到这样做的参数量是随着分辨率的上升以阶乘
级上涨的,我们显然无法处理这种规模的参数,并且在 5.2.1 也提到了过大的参数量容易让模型在训练过
程中过拟合。
那还是看看造物主的杰作吧:计算机科学家从人类的视觉机制出发设计出了卷积神经网络。从认知心理
学的角度来分析,人类的并不是一眼就能判断物体是什么,而是从其纹理、形状与轮廓这种低级特征开
始鉴别一个物体,再上升到局部特征,最后通过组合局部特征得到对整体的判断。也就是说,我们没必
要在“第一眼 ”就关注所有像素。
根据这个思想,我们改进了神经网络的运算方法、参数传递和激活方式:利用卷积运算替代全连接。即
不必对每一层都进行全连接,全连接代表每个像素对之后的每一个神经元都有贡献,然而左上角的像素
和右下角的像素可以说几乎没有任何相关性(其他大部分像素也是这样)。因此我们在刚开始只需要关
注纹理和边缘,然后是纹理和边缘组成的局部特征,最后才是整体。这样,我们可以依据某种规则只让
神经元之间进行部分连接,最后部分再进行全连接以充分利用高层次特征。
那要怎么样确定这种规则呢?接下来分别介绍CNN中最常见的三个构成模块:卷积层、池化层、全连接
层。
5.2.5.1. 卷积层
引子部分提到,如果把图像resize成一个一维向量,我们很可能就在此映射过程中把原来处于相邻像素之
间的相关性给丢失了。想要保存这种相关性怎么办?这就要利用到“卷积”操作了。
No. 82 / 352


一个3x3的图片,每个像素和周围的像素显然是有位置的相关关系的
把上图resize成1x9的向量,可以看到这里就丢失了原来的位置关系
学过信号与系统的同学应该已经对一维卷积熟悉,时域的卷积操作可以得到系统某个时刻由连续输入叠
加产生地相应(一维情形),那对于图像信号该怎么做?显然图像在两个方向上延伸,图像信号就是坐
标的函数(二元函数),我们直接把卷积的积分变量由时间改为坐标分量并进行离散化,那么二维卷积
操作的结果就是系统在卷积核范围内的输出响应(即计算这个位置是否有相应的特征)。因为只需要采
集局部的信息,我们不用把卷积核设置的太大(如果考虑最极端的情况,把卷积核半径设置得和图像一
样大,那么就相当于采集了整幅图像的信息,和全连接无异了,只不过保留了位置信息)。来看几个动
图:
No. 83 / 352


二维卷积
No. 84 / 352


大小有限的卷积核与一维情形卷积函数的对比
如果没学过信号与系统或者还是不怎么理解卷积,可以参考知乎的这个问题:如何通俗易懂地解释卷积 。
若你学习过信号与系统,可以用我们所熟悉的一维卷积作为类比,二维情形一个大小有限的卷积核
就相当于一个一维的只在某个区间有定义的、在其他地方的值为零的卷积函数(脉冲响应函数)。
增大卷积核即增加感受野(在 5.2.3 中介绍),对比一维卷积函数来说就是扩大了使函数不为零的
定义域定义域。
No. 85 / 352


对一维离散序列的卷积
图源知乎-palet
用于提取垂直边缘的卷积核,当某处出现下图所示垂直边缘的时候,卷积操作会在对应位置得到较大的
值
一张图片的像素表示,和他的显示效果(下图)
未学过信号与系统的同学可以忽略这个类比理解。
那么卷积又是如何得到这些纹理等特征的?让我们看看提取图像中垂直边缘的例子就好了。以提取竖直
边缘的特征为例,我们需要一个这样的卷积核:
No. 86 / 352


提取水平边缘的卷积核其实就是竖直边缘旋转90度,角点则是两个边缘的交点
全连接神经网络和卷积神经网络参数(特征)的对比
再回想一下卷积的含义,是某个系统在受到连续输入时响应的叠加,这里的“连续输入”指的是随着时间变
化的输入信号。很显然图像信号是坐标的函数,那么在拍摄的那一刻所有信息就定格下来,这里的连续
输入就变成了不同坐标的像素输入(x,y两个维度)。显然,在有垂直特征出现的时候,该区域的卷积结
果就会很大,如果没有相应的特征出现,那么结果显然就会比较小。想要提取其他特征同理,这里给出
提取水平边缘、角点的卷积核:
因此,有怎么样的卷积核就会提取出怎么样的特征,不过到现在为止,我们还只是手动设计了物体的特
征,比如上面提到的几个特征提取器(垂直、水平、角点),如何让机器自己学习特征?和标准的神经
网络对比,这里的卷积核参数(即应该要学习到的特征,卷积核上每一个位置对应的参数值)其实也就
是神经网络的权重参数w和b(隐藏层变成了卷积层)。这样,我们不必再自己设计特征(自己设计的特
征不能涵盖所有情况并且特异性也不够好,学习得到的特征更能反映物体的本质),而是通过反向传播
来让网络自己学习特征。
No. 87 / 352


步长为2的卷积示例
现在,我们便有能替代全连接并且保持图像中像素的位置关系的同时还能提取特征的数学工具:卷积操
作。通过网络的反向传播和梯度下降,我们便可以提取各种各样的特征。
此外我们还可以修改卷积的步长step(上面的例子中都是每隔一个像素都进行一次卷积计算),修改
step就会改变每次卷积核移动的像素数,这样能够降低计算量、扩大感受野,对于大物体和高分辨率的
图像常常使用3、5、7等大小的步长。
对于图像的边缘,常使用“Padding”的技术来充分利用角落和边缘的像素(上面的例子中我们可以发现,
每次卷积操作[step=1]得到的feature map都会比原图更小,那么我们可以通过在图像周围填充0或是对
图像进行简单的复制,使得卷积核在运算时可以让中心保持对齐,这样在一些有特定需求的场景(比如
对精度要求高、希望提升准确度)就可以生成和原图大小相同的feature map从而充分保留、突出图像的
特征了。还有一些后面会介绍到的特殊的结构如残差连接、密集连接等(其实在现代神经网络也属于标
准设计了),也要求输入输出前后的feature map大小相同。
No. 88 / 352


padding示例
补零padding示例,复制padding则是通过一定的规则例如取边缘平均等方法填充外面一圈的值
是不是已经迫不及待想要训练一个CNN了呢?那么卷积神经网路的反向传播和普通神经网络有什么区
别?我们在讲完池化层和全连接层后再来细说,先把前向传播的过程看完吧!
No. 89 / 352


最大池化操作会保留feature map上最显著的特征
平均池化则是更加中庸,对一个区域内的特征进行加权平均,一般加权系数相同
5.2.5.2. 池化层
现在我们已经通过了一层卷积并且使用的是竖直边缘提取核,把有竖直特征的区域都保存了下来,那么
这张特征图(feature map)里面数值大的点就代表着此区域很有可能有竖直的边缘存在。聪明的你大概
已经想到我们可以通过设置阈值的方式来筛选这些特征,得到竖直边缘的强度大的区域(“够不够竖”)。
不过这里有另一种方法,既能得到最“竖”的那一个特征,又能减少参数量——Pooling。
池化其实也是一种降采样的过程,早期对其的翻译也是只表意不表义非常生涩,笔者个人认为叫“汇集层”
或“采集层”会更好理解一些,池化操作会提取feature map上最重要和显著的特征。常用的池化操作有最
大池化和平均池化(平均池化其实用得非常少了),下图直观地展示了池化操作的过程:
显然我们可以继续用卷积操作来提取feature map上的特征,但是池化不需要进行乘法和加法(最大池
化不需要),在速度上远远超过了卷积。并且在用卷积提取了图像的特征之后,利用池化可以让被提取
出的特征更有显著性、更凸出,以便之后的卷积操作会更有针对性,并且大大减少计算量。
使用池化相对于卷积来说还能提供额外的一些好处(当然是必须要配合卷积使用,只有卷积能提取特
征,池化是让特征更明显,或者说是卷积+池化操作对比单纯卷积的好处):
抑制特征噪声,降低重复信息
当以两个邻近的像素为中心分别进行卷积操作的时候,得到的结果很可能非常接近,这样就带来了
信息地冗余并且在之后的对此feature map进行的卷积操作(卷积核必然会同时覆盖到这两个像
素)可能会产生不利的影响(过大或过小的结果)。利用最大池化或平均池化,就会把当前特征图
上邻近的特征进行融合、取极值以降低冗余度。
提升模型的尺度不变性、旋转不变形、平移不变性
No. 90 / 352


当一张图像发生轻微的旋转、平移和尺度缩放,池化操作对这些变化并不敏感,即使部分特征变换
了位置或大小,也不会影响池化提取的结果或影响很小(从池化操作是如何进行的来理解)。
一定程度上防止过拟合
其实和第一个特点有相似之处,因为在进行池化操作的时候,随机噪声会被抑制,而局部样本不太
明显的特征显然不如数据分布的相同特征那么显著,池化更加关注全局的特征而不是局部出现的特
征,这在另一方面来说是对参数空间的降维,以防模型学习到一些不重要的特征维度上的信息。因
此能够防止网络的过拟合。
所以,我们构建CNN的时候一般都是用1~3个卷积层+1个池化层作为一个基本的block(conv
pooling-block,CPB),然后用多个block进行串联得到网络的基本结构,最后加上全连接层完成整个网
络的构建。
5.2.5.3. 全连接层
在经过数个基本的block(conv+pooling)后,是不是感觉特征提取的差不多,网络已经知道这些特征对
应的标签是什么了呢?这时候我们会选择把最后的feature map,resize成一个一维向量(最后的
feature map上不同点的相关性其实已经很差,并且每一个点在此时都代表着某种局部特征,对于图像识
别任务来说即使丢失位置信息了也不会对最终的结果产生太大的影)或是使用一个和特征图一样大的卷
积核进行卷积或1x1卷积(也就是第二种方法,一些情况下也被称作全卷积Full Convolution
Network),这样便会得到一个1xn的张量,本质上就是一个一维向量了(和resize得到的结果相同了)。
一般在最后添加1~3个全连接的隐藏层并在输出时连接到一个神经元,对其使用softmax分类(二分类
的推广),就可以得到一张图像的类别了。
No. 91 / 352


以LeNet-5的结构为例,在数个Conv-Pooling Block(CPB)后增加了最后的全连接后,得到一个最简单
的网络结构
计算过程完全一致,只不过,有没有什么办法可以将CNN的前向传播也写成FC一样的矩阵运算?
刚刚说到全连接层会损失位置相关性,指的是多个FC串联会造成这种结果。但是一个有趣同时似乎
也比较明显的结果的是:在CPB后直接跟上单层FC连接反而不会丢失像素间的位置关系并能够充分
利用位置信息输出位置敏感的数据,有兴趣的读者可以思考一下两种连接方式的差异和造成这样结
果的原因。在 5.2.6 中将要介绍的目标检测网络的回归分支就常选用全连接层放在最后用于输出检
测框。
至此,一个完整的CNN就搭建好了,假设这是一个已经训练好的网络,这时候我们只需要投入一张图
片,它就能够输出对应的分类了!可惜它还没训练好
下面让我们看一下CNN和标准的全连接神经网络的训练有什么差别。
5.2.5.4. CNN的反向传播
很多教程在这部分讲得非常模糊甚至根本没有提及这个过程,其实它的原理和普通神经网络是几乎一样
的,只不过由于我们更应该去打破这种简单的黑箱子。
CNN在前向传播的时候,计算似乎比全连接网络要复杂,然而其本质都是线性组合+非线性化,即用上一
层的输出向量通过分量乘以不同权重相加再投入一个激活函数(非线性过程)得到这一层的输出:
为了能够方便的计算出CNN的前向传播结果,充分利用矩阵运算加速库(如numpy等)并发挥GPU的威
力,我们只需要将CNN的forward Propogation转化成矩阵形式:
No. 92 / 352


只需要把上方的原图通过一点小trick转换成下方的矩阵,轻松的用矩阵相乘来得到输出了
图源深度之眼-Deepshare.net
这里我们只不过是把原来的一个正方形resize成一个一维向量,观察一下转换后feature map的下标很容
易找到resize的规律。这里也回答了前面提出的“为什么单层FC能够保留位置信息”的问题。在此之后,我
们也很容易写出反向传播的偏微分方程了。
当然,这里只是对一张单通道图片进行运算,那如果有多张图片,或者一个图片有多个通道该怎么做?
小问题,把这些通道或图片全部resize成一个如上图所示的matrix,随后在行的方向上,把它们堆叠起来
形成一个长长的矩阵(显然其列数还是保持不变,为卷积核的大小,故满足矩阵相乘的定义),再进行
相同的运算即可。
卷积搞定再来想想池化操作,有了上面了例子是不是感觉也简单了不少?平均池化其实就相当于卷积核
的参数固定为(1/卷积核大小)的卷积操作,这里不再展开说,直接上图看看最大池化吧:
No. 93 / 352


用同样的方式根据卷积核的大小,把原feature map resize成x hat,再取每一行的最大值
图源深度之眼-Deepshare.net
池化层只会提取前层最大的特征,显然没有被提取到的特征点在反向传播中不会得到训练,分配到的误
差为零,梯度为零:
至此,反向传播的基本介绍就到此结束。当然我们没有讲解使用batchnorm、layernorm的时候反向传
播应该如何进行,不过原理都是完全一样的!最简单的办法就是先按照规律resize成一维向量,随后就和
标准网络的FC层如出一辙!现在,你应该能够使用pytroch、caffe等网络框架搭建属于你自己的CNN图
像识别网络了,快动手试试吧,马上复现一个LeNet-5问题应该不大。
No. 94 / 352


目标定位方法,得到一个WxH到1x9向量的映射
1代表有目标在图像中,随后四个数代表中心坐标和目标框的长宽,最后四个数表示目标分类
5.2.6. 目标检测
利用CNN我们已经可以完成对图像的识别和分类。但是这样是远远不够的,为了能准确定位图像中
的物体,我们需要对图像中所有目标进行定位(找出框住目标的bounding box外接矩形框,即
[cx,cy,w,h]四个参数,分别表示目标中心在图像中的坐标和bbox的长宽)。此部分会介绍几个经典
的目标检测网络实现的原理和方法。
有同学可能会想,那我直接让网络在全连接层后输出一个向量而不是标量(分类),即多输出四个坐标
也就是8个值,分别代表图像中目标的四个角点不就行了吗?确实,对于只有一个目标的图像我们可以这
么做,可惜倘若图像中有多个目标的话,网络的输出就变成不确定的了(需要一次性输出不确定长度,
分别表示每个目标框中心、长宽和其对应的分类),我们是无法训练一个没有确定输出的网络的。
因为图像分类是对整张图像进行的,是一个WxH维到一个确定的标量或向量(如上面说的同时进行分类
和回归)的映射,标准的CNN方法显然无法完成一张图片中多个对象的分类和定位操作。不过机智的你
应该已经想到,我们可以把图片拆成很多的部分,然后将这些子图分别投入CNN从而得到它们对应的分
类,不就把里面的对象定位出来了么。
没错,这就是密集采样的定位思想,也是最早的目标检测方法。使用滑动窗口来检测目标位置的方法和
模板匹配有些类似。还有一种方法是将图像用不同的长度划分格点,在每个格点中进行图像分类(这其
实是一种特殊的滑动窗口方法,即设定滑动步长大小使其与窗口的大小相同)。
No. 95 / 352


采用不同大小的滑动窗口,裁取图像的一部分投入CNN得到分类(图中只使用了一种大小)
使用滑动窗口方法可能出现的问题,没有一个滑窗能够和汽车整体匹配
显然,这种方法的缺点也非常明显:超高的计算开销。步长设置的过大可能会发生漏检,而为了检测大
小不同的目标,我们又需要采用各种大小的窗口来运行CNN,同时还可能需要使用不同长宽比的窗口来
应对物体长宽比不同的情况(例如汽车在侧面看是细长的矩形,正面又比较接近正方形,如果仅使用一
种类型的窗口,则可能会出现定位不精确的问题,得到的定位框包含了大量的背景)。
仔细观察,我们就会发现,在目标检测问题中,我们面临的最大困难就是候选区域太多且目标大小的尺
度不一,问题的搜索空间过于庞大,我们难以用暴力搜索穷举全部解。于是two-stage方法的代表作R
CNN(region proposal-CNN)横空出世。不过,再介绍R-CNN之前,先让我们看看目标检测中常用的
术语和概念吧。
5.2.6.1. 目标检测中的常用术语
1. GT(ground truth)
GT可以理解为真实值、有效值、正确的答案、正确的标注信息。对于目标检测来说,标记的坐标点
和标记分类就被称作ground truth。
No. 96 / 352


训练标记的bbox区域(蓝色区域)就是ground truth
在目标检测网络的推理过程中,如何为预测框分配标签(训练的label即GT的属于的分类)会极大地
影响训练的效率和推理的准确度,这也是当前目标检测研究的热点和难点。
2. IOU(intersection of Union)
字面意思。两个bbox的交集和并集的比例,详见上方GT的插图,灰色区域为交集面积,除以两个
bbox的并集就得到交并比。交并比是衡量检测生成bbox的准确性的一个指标。现在也出现了一些
其他用于衡量预测框质量的指标。
3. 正/负样本(positive/negative sample)
在目标检测中,包含了目标的bbox就是正样本,而没有包含目标即bbox内是背景的将被作为负样
本。当然一些网络会对正负样本进行特殊定义。特别注意,我们是根据训练标注来生成正负样本
的,并不是说训练标注中地GT就是正样本,其他区域都是负样本。我们会依据某种准则如与GT的
IOU或置信度进而由得到正负样本。
No. 97 / 352


我们的检测目标是杯子,假设网络生成了4个检测框,认为这里面有杯子,那么根据特定的IOU阈值
如0.5,我们将生成的4个检测框分别作为正负样本对待;在这里红色框是GT,紫色和蓝色框因为和
GT的IOU超过50%被记为正样本,绿色框则被判定为负样本(背景)
在阅读了低下的几个经典网络后再回来看看,你应该会对正负样本有新的理解。并且目标检测中有
一个很大的、亟待解决的问题就是正负样本、难易样本的失衡。(在一张图片中,没有目标存在的
区域占了全图的绝大部分,负样本即背景类占了所有样本的大多数,大量负样本在训练时会淹没正
样本,导致神经网络失去鉴别能力对负样本产生过拟合)
正负样本的划分策略(用IOU来划分正负样本是一个合适的准则么?)和不平衡问题同样是目标检
测的难点。
4. ConvNet和特征图
ConvNet一般值的是神经网络的backbone(骨干网络),也就是负责提取图像特征的部分,即我
们在上面讲解CNN的时候由卷积层和池化层堆叠起来的block。特征图就是经过n个block之后产生
的“图片”,因为原图的特征被提取到这些小”图片“上所以我们称之为特征图。简言之经过卷积或池化
处理的图片或特征都可以被称作特征图,上面记录着原图的某些特征。
5. 感受野(receptive field)
从这个术语的名字应该可以略知一二。receptive field是高层feature map上的一个点的信息是由原
图或之前的feature map中多大的范围/面积的像素所贡献的度量。换个简单句:原图上的某个像素
或低层次的feature map是否和高层中的feature map上的某个点有关联/间接连接?再或者直接看
图:
No. 98 / 352


绿色区域是l2中左上角像素的感受野,而黄色区域是l3中间像素的感受野
有许多扩大感受野的方法如变步长卷积、空洞卷积、膨胀卷积、focus层等我们稍后介绍。其实这个
概念应该在CNN部分就进行介绍,但是由于图像分类对此并不敏感所以将介绍移到了此处。
6. NMS(非极大值抑制)
在生成检测框的时候,可能会出现下图这种情况,这时候我们要根据框的置信度或定位精度等信息
筛去一些重叠区域很大的目标框,防止出现重复检测的情况。这里给出了NMS的大致介绍和实现源
码:NMS 在目标检测中的应用。
7. 性能指标与mAP(mean average precision)
No. 99 / 352


混淆矩阵和对应的概念含义
找到曲线上的平衡点,即边际效用最大的点,当继续往曲线的右侧移动,查全率上升的速度将会低
于查准率下降的速度
mAP是目标检测中最常见的测试检测器性能的指标。在次之前先让我们看看混淆矩阵,这是机器学
习中所有分类器都要确定的一个参数:
由混淆矩阵中的数据,我们可以通过各种运算来得到千奇百怪的指标,常见的如正确率、真阳性
率、特异度、假阴性率(漏诊率)、Youden指数等。我们则选择mAP来指示一个目标检测算法的
性能,mAP的计算需要查准率(true positive rate,precision: TP/(TP+FP) )和查全率(又叫召
回率recall,计算通过TPR:true positive rate,TP/(TP+FP) )两个信息。这两个指标是一对矛盾
指标,当查准率高的时候,说明分类器很少把负类分为正类,只遴选那些最优把握、正类置信度高
的样本,显然分类为正类的样本TP+FP将会下降(置信度上升,分类器把更少的样本分为正类),
同时查全率必然跟着下降,很多正样本将会成为漏网之鱼而被分类为负样本。当降低阈值希望尽可
能多地把所有的正类的查出来,那么很多置信度较高的负类同样容易被分为正类,导致查准度下
降。
目标检测任务中判断预测框的正负样本一般以IOU是否>0.5为准则(这里的分类是说预测框里有没
有物体,而对预测框中物体进行的分类则是直接通过置信度进行)。那么如何绘制处pr曲线?只需
根据不同的分类阈值分别设定分类器并对样本进行分类即可得到PR空间上的一个点(一组测试对应
一个查全率和一个查准率,也就是PR空间中的两个坐标),改变阈值就能在pr图上画出一些列的
点,平滑地连接它们即可得到pr曲线。一般的测试方法是改变阈值每级递增0.05直到0.95,称为
IOU=[0.5:0.95]。AP就是PR曲线下方的面积(为什么用曲线下的面积,而不用平衡点处的分类器
性能作为指标?),称作average precision平均精度。而mAP是把每个类的AP曲线进行求平均,
即可算出多分类的平均精确度。
No. 100 / 352